{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/","title":"Azure, better UI and more","text":"<p>The 0.9.1 update introduces Azure support among other improvements.</p> <p>At <code>dstack</code>, our goal is to create a simple and unified interface for ML engineers to run dev environments, pipelines, and apps on any cloud. With the latest update, we take another significant step in this direction.</p> <p>We are thrilled to announce that the latest update introduces Azure support, among other things, making it incredibly easy to run dev environments, pipelines, and apps in Azure. Read on for more details.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#azure-support","title":"Azure support","text":"<p>Using Azure with <code>dstack</code> is very straightforward. All you need to do is create the corresponding project via the UI and  provide your Azure credentials.</p> <p></p> <p>NOTE:</p> <p>For detailed instructions on setting up <code>dstack</code> for Azure, refer to the documentation.</p> <p>Once the project is set up, you can define dev environments, pipelines, and apps as code, and easily run them with just a single command. <code>dstack</code> will automatically provision the infrastructure for you.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#logs-and-artifacts-in-ui","title":"Logs and artifacts in UI","text":"<p>Secondly, with the new update, you now have the ability to browse the logs and artifacts of any run through the user interface.</p> <p></p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#better-documentation","title":"Better documentation","text":"<p>Last but not least, with the update, we have reworked the documentation to provide a greater emphasis on specific use cases: dev environments,  tasks, and services.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#try-it-out","title":"Try it out","text":"<p>Please note that when installing <code>dstack</code> via <code>pip</code>, you now need to specify the exact list of cloud providers you intend to use:</p> <pre><code>$ pip install \"dstack[aws,gcp,azure]\" -U\n</code></pre> <p>This requirement applies only when you start the server locally. If you connect to a server hosted elsewhere,  you can use the shorter syntax:<code>pip install dstack</code>.</p> <p>Feedback</p> <p>If you have any feedback, including issues or questions, please share them in our Discord community or file it as a GitHub issue.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/","title":"GCP support just landed","text":"<p>The 0.2 update adds support for Google Cloud Platform (GCP).</p> <p>With the release of version 0.2 of <code>dstack</code>, it is now possible to configure GCP as a remote. All features that were previously available for AWS, except real-time artifacts, are now available for GCP as well.</p> <p>This means that you can define your ML workflows in code and easily run them locally or remotely in your GCP account.</p> <p><code>dstack</code> automatically creates and deletes cloud instances as needed, and assists in setting up the environment, including pipeline dependencies, and saving/loading artifacts. </p> <p>No code changes are required since ML workflows are described in YAML. You won't need to deal with Docker, Kubernetes, or stateful UI.</p> <p>This article will explain how to use <code>dstack</code> to run remote ML workflows on GCP.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the latest version of <code>dstack</code> before proceeding.</p> <pre><code>$ pip install dstack --upgrade\n</code></pre> <p>By default, workflows run locally. To run workflows remotely, e.g. on a GCP account), you must configure a remote using the <code>dstack config</code> command. Follow the steps below to do so.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#1-create-a-project","title":"1. Create a project","text":"<p>First you have to create a project in your GCP account, link a billing to it, and make sure that the required APIs and enabled for it.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#2-create-a-storage-bucket","title":"2. Create a storage bucket","text":"<p>Once the project is set up, you can proceed and create a storage bucket. This bucket will be used to store workflow artifacts and metadata.</p> <p>NOTE:</p> <p>Make sure to create the bucket in the sane location where you'd like to run your workflows.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#3-create-a-service-account","title":"3. Create a service account","text":"<p>The next step is to create a service account in the created project and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p> <p>Once the service account is set up, create a key for it and download the corresponding JSON file to your local machine (e.g. to <code>~/Downloads/my-awesome-project-d7735ca1dd53.json</code>).</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#4-configure-the-cli","title":"4. Configure the CLI","text":"<p>Once the service account key JSON file is on your machine, you can configure the CLI using the <code>dstack config</code> command.</p> <p>The command will ask you for a path to the key, GCP region and zone, and storage bucket name.</p> <pre><code>$ dstack config\n\n? Choose backend: gcp\n? Enter path to credentials file: ~/Downloads/dstack-d7735ca1dd53.json\n? Choose GCP geographic area: North America\n? Choose GCP region: us-west1\n? Choose GCP zone: us-west1-b\n? Choose storage bucket: dstack-dstack-us-west1\n? Choose VPC subnet: no preference\n</code></pre> <p>That's it! Now you can run remote workflows on GCP.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/","title":"An early preview of dstack server","text":"<p>The 0.7 update introduces the server with UI, team management, and more.</p> <p>Last October, we open-sourced the <code>dstack</code> CLI for defining ML workflows as code and running them easily on any cloud or locally. The tool abstracts ML engineers from vendor APIs and infrastructure, making it convenient to run scripts, development environments, and applications.</p> <p>Today, we are excited to announce a preview of <code>Hub</code>, a new way to use dstack for teams to manage their model development workflows effectively on any cloud platform.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#how-does-it-work","title":"How does it work?","text":"<p>Previously, the <code>dstack</code> CLI configured a cloud account as a remote to use local cloud credentials for direct requests to the cloud. Now, the CLI allows configuration of Hub as a remote, enabling requests to the cloud using user credentials stored in Hub.</p> <pre><code>sequenceDiagram\n  autonumber\n  participant CLI\n  participant Hub\n  participant Cloud\n  %  Note right of Cloud: AWS, GCP, etc\n  CLI-&gt;&gt;Hub: Run a workflow\n  activate Hub\n      Hub--&gt;&gt;Hub: User authentication\n      loop Workflow provider\n        Hub--&gt;&gt;Cloud: Submit workflow jobs\n      end\n  Hub--&gt;&gt;CLI: Return the workflow status\n  deactivate Hub\n  loop Workflow scheduler\n    Hub--&gt;&gt;Cloud: Re-submit workflow jobs\n  end</code></pre> <p>The Hub not only provides basic features such as authentication and credential storage, but it also has built-in workflow scheduling capabilities. For instance, it can monitor the availability of spot instances and automatically resubmit jobs.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#why-does-it-matter","title":"Why does it matter?","text":"<p>As you start developing models more regularly, you'll encounter the challenge of automating your ML workflows to reduce time spent on infrastructure and manual work.</p> <p>While many cloud vendors offer tools to automate ML workflows, they do so through opinionated UIs and APIs, leading to a suboptimal developer experience and vendor lock-in.</p> <p>In contrast, <code>dstack</code> aims to provide a non-opinionated and developer-friendly interface that can work across any  vendor.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#try-the-preview","title":"Try the preview","text":"<p>Here's a quick guide to get started with Hub:</p> <ol> <li>Start the Hub application</li> <li>Visit the URL provided in the output to log in as an administrator</li> <li>Create a project and configure its backend (AWS or GCP)</li> <li>Configure the CLI to use the project as a remote</li> </ol> <p>For more details, visit the Hub documentation. </p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#whats-next","title":"What's next?","text":"<p>Currently, the only way to run or manage workflows is through the <code>dstack</code> CLI. There are scenarios when you'd prefer to run workflows other ways, e.g. from Python code or programmatically via API. To support these scenarios, we plan to release soon Python SDK and REST API.</p> <p>The built-in scheduler currently monitors spot instance availability and automatically resubmits jobs. Our plan is to enhance this feature and include additional capabilities. Users will be able to track cloud compute usage, and manage quotes per team via the user interface.</p> <p>Lastly, and of utmost importance, we plan to extend support to other cloud platforms, not limiting ourselves to AWS, GCP, and Azure.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#contribution","title":"Contribution","text":"<p>You are encouraged to report any bugs, suggest new features, and provide feedback to improve Hub through GitHub issues.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/","title":"Lambda Cloud GA, and Docker support","text":"<p>The 0.10.5 release improves Lambda Cloud integration and adds support for Docker.</p> <p>In the previous update, we added initial integration with Lambda Cloud. With today's release, this integration has significantly improved and finally goes generally available. Additionally, the latest release adds support for custom Docker images.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#lambda-cloud","title":"Lambda Cloud","text":"<p>In this update, we've added a possibility to create Lambda Cloud projects via the user interface.</p> <p></p> <p>All you need to do is provide your Lambda Cloud API key, and specify an S3 bucket and AWS credentials  for storing state and artifacts.</p> <p>Learn more \u2192</p> <p>Once the project is configured, feel free to run dev environments and tasks in Lambda Cloud using the <code>dstack</code> CLI.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#custom-docker-images","title":"Custom Docker images","text":"<p>By default, <code>dstack</code> uses its own base Docker images to run  dev environments and tasks. These base images come pre-configured with Python, Conda, and essential CUDA drivers.  However, there may be times when you need additional dependencies that you don't want to install every time you run your dev environment or task.</p> <p>To address this, <code>dstack</code> now allows specifying custom Docker images. Here's an example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nenv:\n  - MODEL_ID=tiiuae/falcon-7b\n\nports:\n - 3000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 3000 --trust-remote-code\n</code></pre> Existing limitations <p>Dev environments require the Docker image to have <code>openssh-server</code> pre-installed. If you want to use a custom Docker image with a dev environment and it does not include <code>openssh-server</code>, you can install it using the following  method:</p> <pre><code>type: dev-environment\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nbuild:\n  - apt-get update\n  - DEBIAN_FRONTEND=noninteractive apt-get install -y openssh-server\n  - rm -rf /var/lib/apt/lists/*\n\nide: vscode\n</code></pre> <p>The documentation and examples are updated to reflect the changes in the release.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"blog/2023/08/22/multiple-clouds/","title":"Multi-cloud and multi-region GPU workloads","text":"<p>The v0.11 update now automatically finds the cheapest GPU across clouds and regions.</p> <p>The latest release of <code>dstack</code> enables the automatic discovery of the best GPU price and availability across multiple configured cloud providers and regions.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#multiple-backends-per-project","title":"Multiple backends per project","text":"<p>Now, <code>dstack</code> leverages price data from multiple configured cloud providers and regions to automatically suggest the most cost-effective options.</p> <pre><code>$ dstack run . -f llama-2/train.dstack.yml --gpu A100\n\n Configuration       llama-2/train.dstack.yml\n Min resources       2xCPUs, 8GB, 1xA100\n Max price           no\n Spot policy         auto\n Max duration        72h\n\n #  BACKEND  RESOURCES                      SPOT  PRICE\n 2  lambda   30xCPUs, 200GB, 1xA100 (80GB)  yes   $1.1\n 3  gcp      12xCPUs, 85GB, 1xA100 (40GB)   yes   $1.20582\n 1  azure    24xCPUs, 220GB, 1xA100 (80GB)  yes   $1.6469\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>The default behavior of <code>dstack</code> is to first attempt the most cost-effective options, provided they are available. You have the option to set a maximum price limit either through <code>max_price</code> in <code>.dstack/profiles.yml</code> or by using <code>--max-price</code> in the <code>dstack run</code> command.</p> <p>To implement this change, we have modified the way projects are configured. You can now configure multiple clouds and regions within a single project.</p> <p></p> <p>Why this matter?</p> <p>The ability to run LLM workloads across multiple cloud GPU providers allows for a significant reduction in costs and an increase in availability, while also remaining independent of any particular cloud vendor.</p> <p>We hope that the value of <code>dstack</code> will continue to grow as we expand our support for additional cloud GPU providers. If you're interested in a specific provider, please message us on Discord.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#custom-domains-and-https","title":"Custom domains and HTTPS","text":"<p>In other news, it is now possible to deploy services using HTTPS. All you need to do is configure a wildcard domain (e.g., <code>*.mydomain.com</code>), point it to the gateway IP address, and then pass the subdomain you want to use (e.g., <code>myservice.mydomain.com</code>) to the <code>gateway</code> property in YAML (instead of the gateway IP address).</p>"},{"location":"blog/2023/08/22/multiple-clouds/#other-changes","title":"Other changes","text":""},{"location":"blog/2023/08/22/multiple-clouds/#dstackprofilesyml","title":".dstack/profiles.yml","text":"<ul> <li>The <code>project</code> property is no longer supported.</li> <li>You can now use <code>max_price</code> to set the maximum price per hour in dollars.</li> </ul>"},{"location":"blog/2023/08/22/multiple-clouds/#dstack-run","title":"dstack run","text":"<p>Using the dstack run command, you are now able to utilize options such as <code>--gpu</code>, <code>--memory</code>, <code>--env</code>, <code>--max-price</code>, and several other arguments to override the profile settings.</p> <p>Lastly, the local backend is no longer supported. Now, you can run everything using only a cloud backend.</p> <p>The documentation is updated to reflect the changes in the release.</p> <p>Migration to 0.11</p> <p>The <code>dstack</code> version 0.11 update brings significant changes that break backward compatibility. If you used prior <code>dstack</code> versions, after updating to <code>dstack==0.11</code>, you'll need to log in to the UI and reconfigure clouds. </p> <p>We apologize for any inconvenience and aim to ensure future updates maintain backward compatibility.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Why managed notebooks are losing ground to cloud dev environments.</p> <p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/2023/08/07/services-preview/","title":"Introducing services to simplify deployment","text":"<p>The 0.10.7 update introduces services, a new configuration type for easier deployment.</p> <p>Until now, <code>dstack</code> has supported <code>dev-environment</code> and <code>task</code> as configuration types. Even though <code>task</code>  may be used for basic serving use cases, it lacks crucial serving features. With the new update, we introduce <code>service</code>, a dedicated configuration type for serving.</p> <p>Consider the following example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nports: \n  - 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>When running it, the <code>dstack</code> CLI forwards traffic to <code>127.0.0.1:8000</code>. This is convenient for development but unsuitable for production.</p> <p>In production, you need your endpoint available on the external network, preferably behind authentication  and a load balancer. </p> <p>This is why we introduce the <code>service</code> configuration type.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>As you see, there are two differences compared to <code>task</code>.</p> <ol> <li>The <code>gateway</code> property: the address of a special cloud instance that wraps the running service with a public    endpoint. Currently, you must specify it manually. In the future, <code>dstack</code> will assign it automatically.</li> <li>The <code>port</code> property: A service must always configure one port on which it's running.</li> </ol> <p>When running, <code>dstack</code> forwards the traffic to the gateway, providing you with a public endpoint that you can use to access the running service.</p> Existing limitations <ol> <li>Currently, you must create a gateway manually using the <code>dstack gateway</code> command  and specify its address via YAML (e.g. using secrets). In the future, <code>dstack</code> will assign it automatically.</li> <li>Gateways do not support HTTPS yet. When you run a service, its endpoint URL is <code>&lt;the address of the gateway&gt;:80</code>.  The port can be overridden via the port property: instead of <code>8000</code>, specify <code>&lt;gateway port&gt;:8000</code>.</li> <li>Gateways do not provide authorization and auto-scaling. In the future, <code>dstack</code> will support them as well.</li> </ol> <p>This initial support for services is the first step towards providing multi-cloud and cost-effective inference.</p> <p>Give it a try and share feedback</p> <p>Even though the current support is limited in many ways, we encourage you to give it a try and share your feedback with us!</p> <p>More details on how to use services can be found in a dedicated guide in our docs.  Questions and requests for help are very much welcome in our Discord server.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/","title":"Simplified cloud setup, and refined API","text":"<p>The v0.12.0 update makes it much easier to configure clouds and enhances the API.</p> <p>For the past six weeks, we've been diligently overhauling <code>dstack</code> with the aim of significantly simplifying the process of configuring clouds and enhancing the functionality of the API. Please take note of the breaking changes, as they necessitate careful migration.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#cloud-setup","title":"Cloud setup","text":"<p>Previously, the only way to configure clouds for a project was through the UI. Additionally, you had to specify not only the credentials but also set up a storage bucket for each cloud to store metadata.</p> <p>Now, you can configure clouds for a project via <code>~/.dstack/server/config.yml</code>. Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Regions and other settings are optional. Learn more on what credential types are supported  via Clouds.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#enhanced-api","title":"Enhanced API","text":"<p>The earlier introduced Python API is now greatly refined.</p> <p>Creating a <code>dstack</code> client is as easy as this: </p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError:\n    print(\"Can't connect to the server\")\n</code></pre> <p>Now, you can submit a task or a service:</p> <pre><code>from dstack.api import Task, Resources, GPU\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",\n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre> <p>The <code>dstack.api.Run</code> instance provides methods for various operations including attaching to the run,  forwarding ports to <code>localhost</code>, retrieving status, stopping, and accessing logs. For more details, refer to  the example and reference.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#other-changes","title":"Other changes","text":"<ul> <li>Because we've prioritized CLI and API UX over the UI, the UI is no longer bundled.  Please inform us if you experience any significant inconvenience related to this.</li> <li>Gateways should now be configured using the <code>dstack gateway</code> command, and their usage requires you to specify a domain.   Learn more about how to set up a gateway.</li> <li>The <code>dstack start</code> command is now <code>dstack server</code>.</li> <li>The Python API classes were moved from the <code>dstack</code> package to <code>dstack.api</code>.</li> </ul>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#migration","title":"Migration","text":"<p>Unfortunately, when upgrading to 0.12.0, there is no automatic migration for data. This means you'll need to delete <code>~/.dstack</code> and configure <code>dstack</code> from scratch.</p> <ol> <li><code>pip install \"dstack[all]==0.12.0\"</code></li> <li>Delete <code>~/.dstack</code></li> <li>Configure clouds via <code>~/.dstack/server/config.yml</code> (see the new guide)</li> <li>Run <code>dstack server</code></li> </ol> <p>The documentation and examples are updated.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n</code></pre> <p>Feedback and support</p> <p>Questions and requests for help are very much welcome in our  Discord server.</p>"},{"location":"blog/2023/10/31/tensordock/","title":"World's cheapest GPUs with TensorDock and dstack","text":"<p>With v0.12.2, you can now use cloud GPU at highly competitive pricing using TensorDock.</p> <p>At <code>dstack</code>, we remain committed to our mission of building the most convenient tool for orchestrating generative AI workloads in the cloud. In today's release, we have added support for TensorDock, making it easier for you to leverage cloud GPUs at highly competitive prices.</p> <p>Configuring your TensorDock account with <code>dstack</code> is very easy. Simply generate an authorization key in your TensorDock API settings and set it up in <code>~/.dstack/server/config.yml</code>:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>Now you can restart the server and proceed to using the CLI or API for running development environments, tasks, and services.</p> <pre><code>$ dstack run . -f .dstack.yml --gpu 40GB\n\n Min resources  1xGPU (40GB)\n Max price      -\n Max duration   6h\n Retry policy   no\n\n #  REGION        INSTANCE  RESOURCES                     SPOT  PRICE\n 1  unitedstates  ef483076  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6235\n 2  canada        0ca177e7  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n 3  canada        45d0cabd  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>TensorDock offers cloud GPUs on top of servers from dozens of independent hosts, providing some of the most affordable GPU pricing you can find on the internet.</p> <p>With <code>dstack</code>, you can now utilize TensorDock's GPUs through a highly convenient interface, which includes the developer-friendly CLI and API.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p>"},{"location":"blog/2023/11/21/vastai/","title":"Accessing the GPU marketplace with Vast.ai and dstack","text":"<p>With dstack 0.12.3, you can now use Vast.ai's GPU marketplace as a cloud provider.</p> <p><code>dstack</code> simplifies gen AI model development and deployment through its developer-friendly CLI and API.  It eliminates cloud infrastructure hassles while supporting top cloud providers (such as AWS, GCP, Azure, among others).</p> <p>While <code>dstack</code> streamlines infrastructure challenges, GPU costs can still hinder development. To address this,  we've integrated <code>dstack</code> with Vast.ai, a marketplace providing GPUs from independent hosts at  notably lower prices compared to other providers.</p> <p>With the <code>dstack</code> 0.12.3 release, it's now possible use Vast.ai alongside other cloud providers.</p> <pre><code>$ dstack run . --gpu 24GB --backend vastai --max-price 0.4\n\n #  REGION            INSTANCE  RESOURCES                       PRICE\n 1  pl-greaterpoland  6244171   16xCPU, 32GB, 1xRTX3090 (24GB)  $0.18478\n 2  ee-harjumaa       6648481   16xCPU, 64GB, 1xA5000 (24GB)    $0.29583\n 3  pl-greaterpoland  6244172   32xCPU, 64GB, 2xRTX3090 (24GB)  $0.36678\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>By default, it suggests GPU instances based on their quality score. If you want to, you can control the maximum price.</p> <p>You can use Vast.ai to fine-tune, and  deploy models or launch dev environments (which are highly convenient for interactive coding with your favorite IDE).</p> <p>Configuring Vast.ai for use with <code>dstack</code> is easy. Log into your Vast AI account, click <code>Account</code> in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>Now you can restart the server and proceed to using <code>dstack</code>'s CLI and API.</p> <p>If you want an easy way to  develop, train and deploy gen AI models using affordable cloud GPUs,  give <code>dstack</code> with Vast.ai a try.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p> <p>Lastly, take the time to check out the preview of the new fine-tuning and  text generation APIs.</p>"},{"location":"docs/","title":"Quickstart","text":"<p><code>dstack</code> is an open-source platform that simplifies  training, fine-tuning, and deploying generative AI models, leveraging the open-source  ecosystem.</p>"},{"location":"docs/#installation","title":"Installation","text":"<p>To use <code>dstack</code>, either set up the open-source server (and configure your own cloud accounts) or use the cloud version (which provides GPU out of the box).</p> Open-source <p>If you wish to use <code>dstack</code> with your own cloud accounts, you can do it  by using the open-source server.</p> dstack Cloud <p>If you want to use the cloud version of <code>dstack</code>,  sign up, and configure the client  with server address, user token, and project name using <code>dstack config</code>.</p> <pre><code>$ dstack config --server https://cloud.dstack.ai \\\n    --project &amp;lt;your project name&amp;gt; \\\n    --token &amp;lt;your user token&amp;gt;\n</code></pre> <p>The client configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>Once <code>dstack</code> is set up, you can use CLI or API.</p>"},{"location":"docs/#install-the-server","title":"Install the server","text":"<p>The easiest way to install the server, is via <code>pip</code>:</p> <pre><code>$ pip install \"dstack[all]\" -U\n</code></pre> <p>Another way to install the server is through Docker.</p>"},{"location":"docs/#configure-the-server","title":"Configure the server","text":"<p>If you have default AWS, GCP, or Azure credentials on your machine, the <code>dstack</code> server will pick them up automatically.</p> <p>Otherwise, you need to manually specify the cloud credentials in <code>~/.dstack/server/config.yml</code>. For further details, refer to server configuration.</p>"},{"location":"docs/#start-the-server","title":"Start the server","text":"<p>To start the server, use the <code>dstack server</code> command:</p> <pre><code>$ dstack server\n\nApplying configuration...\n---&gt; 100%\n\nThe server is running at http://127.0.0.1:3000/.\nThe admin token is bbae0f28-d3dd-4820-bf61-8f4bb40815da\n</code></pre>"},{"location":"docs/#client-configuration","title":"Client configuration","text":"<p>At startup, the server automatically configures CLI and API with the server address, user token, and  the default project name (<code>main</code>). </p> <p>To use CLI and API on different machines or projects, use <code>dstack config</code>.</p> <pre><code>$ dstack config --server &amp;lt;your server adddress&amp;gt; \\\n    --project &amp;lt;your project name&amp;gt; \\\n    --token &amp;lt;your user token&amp;gt;\n</code></pre> <p>The client configuration is stored via <code>~/.dstack/config.yml</code>.</p>"},{"location":"docs/#using-the-cli","title":"Using the CLI","text":"<p>The CLI allows you to define configurations (what you want to run) as YAML files and run them using the <code>dstack run</code> command.</p>"},{"location":"docs/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> Dev environmentTaskService <pre><code>type: dev-environment\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\nide: vscode\n</code></pre> <p>A dev environments is a perfect tool for interactive experimentation with your IDE. It allows to pre-configure the Python version or a Docker image, etc. Go to Dev environments to learn more.</p> <pre><code>type: task\n\nports:\n  - 7860\n\npython: \"3.11\" # (Optional) If not specified, your local version is used.\n\ncommands:\n  - pip install -r requirements.txt\n  - gradio app.py\n</code></pre> <p>A task may run training scripts, batch jobs, or web apps. It allows to specify the commands, ports,  and pre-configure the Python version or a Docker image, etc. Go to Tasks to learn more.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv: \n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ \n\nport: 80\n\ncommands:\n  - text-generation-launcher --hostname 0.0.0.0 --port 80 --trust-remote-code\n</code></pre> <p>A service makes it very easy to deploy models or web apps. It allows to specify the commands,  and the Python version or a Docker image, etc. Go to Services to learn more.</p>"},{"location":"docs/#run-configuration","title":"Run configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>No need to worry about copying code, setting up environment, IDE, etc. <code>dstack</code> handles it all  automatically.</p>"},{"location":"docs/configuration/server/","title":"Server configuration","text":"<p>If you're using the open-source server, you can configure your own  cloud accounts, allowing <code>dstack</code> to provision workloads there.</p> <p>For flexibility, the server allows you to have multiple projects and users.</p> <p>To configure a cloud account, specify its settings in <code>~/.dstack/server/config.yml</code> under the <code>backends</code> property  of the respective project.</p> <p>Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Default project</p> <p>The default project is <code>main</code>. The CLI and API are automatically configured to use it (through <code>~/.dstack/config.yml</code>).</p>"},{"location":"docs/configuration/server/#cloud-credentials","title":"Cloud credentials","text":""},{"location":"docs/configuration/server/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p>"},{"location":"docs/configuration/server/#access-key","title":"Access key","text":"<p>To create an access key, follow this guide. </p> <p>Once you've downloaded the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: KKAAUKLIZ5EHKICAOASV\n      secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre>"},{"location":"docs/configuration/server/#default-credentials","title":"Default credentials","text":"<p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/configuration/server/#azure","title":"Azure","text":"<p>Permission</p> <p>You must have the <code>Owner</code> permission for the Azure subscription.</p> <p>There are two ways to configure Azure: using a client secret or using the default credentials.</p>"},{"location":"docs/configuration/server/#client-secret","title":"Client secret","text":"<p>A client secret can be created using the Azure CLI: (1)</p> <ol> <li>If you don't know your <code>subscription_id</code>, run      <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre></li> </ol> <pre><code>$ SUBSCRIPTION_ID=...\n$ az ad sp create-for-rbac --name dstack-app --role Owner --scopes /subscriptions/$SUBSCRIPTION_ID --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <p>Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre>"},{"location":"docs/configuration/server/#default-credentials_1","title":"Default credentials","text":"<p>Another way to configure Azure is through default credentials.</p> <p>Obtain the <code>subscription_id</code> and <code>tenant_id</code> via the Azure CLI:</p> <pre><code>$ az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>Then proceed to configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/configuration/server/#gcp","title":"GCP","text":"Enable APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>. (1)</p> <ol> <li>If you don't know your GCP <code>project_id</code>, run      <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre></li> </ol> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com \n</code></pre> <p>There are two ways to configure GCP: using a service account or using the default credentials.</p>"},{"location":"docs/configuration/server/#service-account","title":"Service account","text":"<p>To create a service account, follow this guide. Make sure to grant it the <code>Service Account User</code>, and <code>Compute Admin</code> roles.</p> <p>After setting up the service account create a key for it  and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path. (1) </p> <ol> <li>If you don't know your GCP <code>project_id</code>, run      <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre></li> </ol> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: service_account\n      filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre>"},{"location":"docs/configuration/server/#default-credentials_2","title":"Default credentials","text":"<p>Example: (1) </p> <ol> <li>If you don't know your GCP <code>project_id</code>, run      <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre></li> </ol> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/configuration/server/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: lambda\n    creds:\n      type: api_key\n      api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/configuration/server/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock account, click API in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>NOTE:</p> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/configuration/server/#vast-ai","title":"Vast AI","text":"<p>Log into your Vast AI account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>NOTE:</p> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/configuration/server/#cloud-regions","title":"Cloud regions","text":"<p>In addition to credentials, each cloud (except TensorDock and Vast AI) optionally allows for region configuration.</p> <p>Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    regions: [us-west-2, eu-west-1]\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>If regions aren't specified, <code>dstack</code> will use all available regions.</p>"},{"location":"docs/guides/dev-environments/","title":"Dev environments","text":"<p>Before submitting a long-running task or deploying a model, you may want to experiment  interactively using your IDE, terminal, or Jupyter notebooks.</p> <p>With <code>dstack</code>, you can provision a dev environment with the required cloud resources,  code, and environment via a single command.</p>"},{"location":"docs/guides/dev-environments/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\nide: vscode\n</code></pre> <p>Configuration options</p> <p>You can specify your own Docker image, configure environment variables, etc. If no image is specified, <code>dstack</code> uses its own Docker image (pre-configured with Python, Conda, and essential CUDA drivers). For more details, refer to the Reference.</p>"},{"location":"docs/guides/dev-environments/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f .dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+fast-moth-1/workflow\n</code></pre> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p> <p>Once the dev environment is provisioned, click the link to open the environment in your desktop IDE.</p> <p></p> <p>Port forwarding</p> <p>When running a dev environment, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access.</p> <p>No need to worry about copying code, setting up environment, IDE, etc. <code>dstack</code> handles it all  automatically.</p> .gitignore <p>When running a dev environment, <code>dstack</code> uses the exact version of code from your project directory. </p> <p>If there are large files, consider creating a <code>.gitignore</code> file to exclude them for better performance.</p>"},{"location":"docs/guides/fine-tuning/","title":"Fine-tuning","text":"<p>For fine-tuning an LLM with <code>dstack</code>'s API, specify a model, dataset, training parameters, and required compute resources. The API takes care of everything else.</p> <p>The API currently supports only supervised fine-tuning (SFT). Support for DPO and RLHF is coming soon.</p>"},{"location":"docs/guides/fine-tuning/#how-does-it-work","title":"How does it work?","text":"<p><code>dstack</code> loads a base model and dataset from Hugging Face, schedules the fine-tuning task to run on a configured cloud, and reports metrics to a tracker of your choice.</p> <p></p> <p>Once the model is fine-tuned, it's pushed to Hugging Face and is ready for deployment.</p>"},{"location":"docs/guides/fine-tuning/#prepare-a-dataset","title":"Prepare a dataset","text":"<p>The dataset should contain a <code>\"text\"</code> column with completions following the prompt format of the corresponding model. Check the example (for fine-tuning Llama 2).</p> <p>Once the dataset is prepared, it must be uploaded to Hugging Face.</p> Uploading a dataset <p>Here's an example of how to upload a dataset programmatically:</p> <pre><code>import pandas as pd\nfrom datasets import Dataset\n\ndf = pd.read_json(\"samsum.jsonl\", lines=True)\ndataset = Dataset.from_pandas(df)\ndataset.push_to_hub(\"peterschmidt85/samsum\")\n</code></pre>"},{"location":"docs/guides/fine-tuning/#create-a-client","title":"Create a client","text":"<p>First, you connect to <code>dstack</code>:</p> <pre><code>from dstack.api import Client\n\nclient = Client.from_config()\n</code></pre>"},{"location":"docs/guides/fine-tuning/#create-a-task","title":"Create a task","text":"<p>Then, you create a fine-tuning task, specifying the model and dataset,  and various training parameters.</p> <pre><code>from dstack.api import FineTuningTask\n\ntask = FineTuningTask(\n    model_name=\"NousResearch/Llama-2-13b-hf\",\n    dataset_name=\"peterschmidt85/samsum\",\n    env={\n        \"HUGGING_FACE_HUB_TOKEN\": \"...\",\n    },\n    num_train_epochs=2,\n    max_seq_length=1024,\n    per_device_train_batch_size=2,\n)\n</code></pre>"},{"location":"docs/guides/fine-tuning/#run-the-task","title":"Run the task","text":"<p>When running a task, you can configure resources, and many other options.</p> <pre><code>from dstack.api import Resources, GPU\n\nrun = client.runs.submit(\n    run_name=\"Llama-2-13b-samsum\", # (Optional) If unset, its chosen randomly\n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre> <p>GPU memory</p> <p>The API defaults to using QLoRA based on the provided  training parameters. When specifying GPU memory, consider both the model size and the specified batch size. After a few attempts, you'll discover the best configuration.</p> <p>When the training is done, the API pushes the final model to the Hugging Face hub.</p> <p></p>"},{"location":"docs/guides/fine-tuning/#manage-runs","title":"Manage runs","text":"<p>You can manage runs using API, the CLI, or the user interface.</p>"},{"location":"docs/guides/fine-tuning/#track-metrics","title":"Track metrics","text":"<p>To track experiment metrics, specify <code>report_to</code> and related authentication environment variables.</p> <pre><code>task = FineTuningTask(\n    model_name=\"NousResearch/Llama-2-13b-hf\",\n    dataset_name=\"peterschmidt85/samsum\",\n    report_to=\"wandb\",\n    env={\n        \"HUGGING_FACE_HUB_TOKEN\": \"...\",\n        \"WANDB_API_KEY\": \"...\",\n    },\n    num_train_epochs=2\n)\n</code></pre> <p>Currently, the API supports <code>\"tensorboard\"</code> and <code>\"wandb\"</code>.</p> <p></p>"},{"location":"docs/guides/fine-tuning/#whats-next","title":"What's next?","text":"<ul> <li>Once the model is trained, proceed to deploy it as an endpoint.   The deployed endpoint can be used from your apps directly or via LangChain.</li> <li>The source code of the fine-tuning task is available   at GitHub.   If you prefer using a custom script, feel free to do so using dev environments and    tasks.</li> </ul>"},{"location":"docs/guides/services/","title":"Services","text":"<p>With <code>dstack</code>, you can use the CLI or API to deploy models or web apps. Provide the commands, port, and choose the Python version or a Docker image.</p> <p><code>dstack</code> handles the deployment on configured cloud GPU provider(s) with the necessary resources.</p> Prerequisites <p>If you're using the open-source server, you first have to set up a gateway.</p> <p>If you're using the cloud version of <code>dstack</code>, the gateway is set up for you.</p>"},{"location":"docs/guides/services/#set-up-a-gateway","title":"Set up a gateway","text":"<p>For example, if your domain is <code>example.com</code>, go ahead and run the  <code>dstack gateway create</code> command:</p> <pre><code>$ dstack gateway create --domain example.com --region eu-west-1 --backend aws\n\nCreating gateway...\n---&gt; 100%\n\n BACKEND  REGION     NAME          ADDRESS        DOMAIN       DEFAULT\n aws      eu-west-1  sour-fireant  52.148.254.14  example.com  \u2713\n</code></pre> <p>Afterward, in your domain's DNS settings, add an <code>A</code> DNS record for <code>*.example.com</code>  pointing to the IP address of the gateway.</p> <p>This way, if you run a service, <code>dstack</code> will make its endpoint available at  <code>https://&lt;run-name&gt;.example.com</code>.</p>"},{"location":"docs/guides/services/#using-the-cli","title":"Using the CLI","text":""},{"location":"docs/guides/services/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv: \n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ \n\nport: 80\n\ncommands:\n  - text-generation-launcher --hostname 0.0.0.0 --port 80 --trust-remote-code\n</code></pre> <p>By default, <code>dstack</code> uses its own Docker images to run dev environments,  which are pre-configured with Python, Conda, and essential CUDA drivers.</p> <p>Configuration options</p> <p>Configuration file allows you to specify a custom Docker image, environment variables, and many other  options. For more details, refer to the Reference.</p>"},{"location":"docs/guides/services/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f serve.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nServing HTTP on https://yellow-cat-1.example.com ...\n</code></pre> <p>Once the service is deployed, its endpoint will be available at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p>"},{"location":"docs/guides/tasks/","title":"Tasks","text":"<p>With <code>dstack</code>, you can use the CLI or API to run tasks like training scripts, batch jobs, or web apps.  Provide the commands, ports, and choose the Python version or a Docker image.</p> <p><code>dstack</code> handles the execution on configured cloud GPU provider(s) with the necessary resources.</p>"},{"location":"docs/guides/tasks/#using-the-cli","title":"Using the CLI","text":""},{"location":"docs/guides/tasks/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py\n</code></pre> <p>A task can configure ports:</p> <pre><code>type: task\n\nports:\n  - 7860\n\npython: \"3.11\" # (Optional) If not specified, your local version is used.\n\ncommands:\n  - pip install -r requirements.txt\n  - gradio app.py\n</code></pre> <p>When running a task, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access.</p> <p>By default, <code>dstack</code> uses its own Docker images to run dev environments,  which are pre-configured with Python, Conda, and essential CUDA drivers.</p> <p>Configuration options</p> <p>Configuration file allows you to specify a custom Docker image, ports, environment variables, and many other  options. For more details, refer to the Reference.</p>"},{"location":"docs/guides/tasks/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p> Port mapping <p>When running a task, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access. You can override local ports via <code>--port</code>:</p> <pre><code>$ dstack run . -f serve.dstack.yml --port 8080:7860\n</code></pre> <p>This will forward the task's port <code>7860</code> to <code>localhost:8080</code>.</p>"},{"location":"docs/guides/tasks/#parametrize-tasks","title":"Parametrize tasks","text":"<p>You can parameterize tasks with user arguments using <code>${{ run.args }}</code> in the configuration.</p> <p>Example:</p> <pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py ${{ run.args }}\n</code></pre> <p>Now, you can pass your arguments to the <code>dstack run</code> command:</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100 --train_batch_size=1 --num_train_epochs=100\n</code></pre> <p>The <code>dstack run</code> command will pass <code>--train_batch_size=1</code> and <code>--num_train_epochs=100</code> as arguments to <code>train.py</code>.</p>"},{"location":"docs/guides/tasks/#configure-retry-limit","title":"Configure retry limit","text":"<p>By default, if <code>dstack</code> is unable to find capacity, <code>dstack run</code> will fail. However, you may pass the <code>--retry-limit</code> option to <code>dstack run</code> to specify the timeframe in which <code>dstack</code> should search for capacity and automatically resubmit the run.</p> <p>Example:</p> <pre><code>$ dstack run . -f train.dstack.yml --retry-limit 3h\n</code></pre> <p>For more details on the <code>dstack run</code> command, refer to the Reference.</p>"},{"location":"docs/guides/text-generation/","title":"Text generation","text":"<p>The easiest way to deploy a text generation model is by using <code>dstack</code> API. You only need to specify a model, quantization parameters,  and required compute resources.</p> Prerequisites <p>If you're using the cloud version of <code>dstack</code>, no prerequisites are required.</p> <p>However, if you're using the open-source server, you need to  set up a gateway before running models as public endpoints.  Not required for private endpoints.</p>"},{"location":"docs/guides/text-generation/#create-a-client","title":"Create a client","text":"<p>First, you connect to <code>dstack</code>:</p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError:\n    print(\"Can't connect to the server\")\n</code></pre>"},{"location":"docs/guides/text-generation/#create-a-configuration","title":"Create a configuration","text":"<p><code>dstack</code> allows to run a model either as a public endpoint or as a private endpoint.</p> Public endpointPrivate endpoint <pre><code>from dstack.api import CompletionService\n\nconfiguration = CompletionService(\n    model_name=\"TheBloke/CodeLlama-34B-GPTQ\",\n    quantize=\"gptq\"\n)\n</code></pre> <p>When you run a model as a public endpoint, <code>dstack</code> makes it available at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <pre><code>from dstack.api import CompletionTask\n\nconfiguration = CompletionTask(\n    model_name=\"TheBloke/CodeLlama-34B-GPTQ\",\n    quantize=\"gptq\",\n    local_port=\"8080,\n)\n</code></pre> <p>When you run a model as a private endpoint, <code>dstack</code> makes it available only at <code>http://localhost:&lt;local_port&gt;</code> (even though the model itself is running in the cloud).  This is convenient if you intend to access the endpoint solely from your local machine.</p>"},{"location":"docs/guides/text-generation/#run-the-configuration","title":"Run the configuration","text":"<p>When running a service, you can configure resources, and many other options.</p> <pre><code>from dstack.api import Resources, GPU\n\nrun = client.runs.submit(\n    run_name=\"codellama-34b-gptq\", # (Optional) If unset, its chosen randomly\n    configuration=configuration,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre>"},{"location":"docs/guides/text-generation/#access-the-endpoint","title":"Access the endpoint","text":"Public endpointPrivate endpoint <pre><code>$ curl https://&amp;lt;run-name&amp;gt;.&amp;lt;domain-name&amp;gt;/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\": 20}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The OpenAPI documentation on the endpoint can be found at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;/docs</code>.</p> <pre><code>$ curl http://localhost:&amp;lt;local-port&amp;gt;/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\": 20}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The OpenAPI documentation on the endpoint can be found at <code>http://localhost:&lt;local-port&gt;/docs</code>.</p> <p>Both public and private endpoint support streaming, continuous batching, tensor parallelism, etc.</p>"},{"location":"docs/guides/text-generation/#manage-runs","title":"Manage runs","text":"<p>You can use the instance of <code>dstack.api.Client</code> to manage your runs,  including getting a list of runs, stopping a given run, etc.</p>"},{"location":"docs/installation/docker/","title":"Docker","text":"<p>Here's how to run the <code>dstack</code> server via Docker:</p> <pre><code>$ docker run --name dstack -p &amp;lt;port-on-host&amp;gt;:3000 \\ \n  -v ~/.dstack/server:/root/.dstack/server \\\n  dstackai/dstack\n</code></pre> <p>Configure clouds</p> <p>Upon startup, the server sets up the default project called <code>main</code>. Prior to using <code>dstack</code>, make sure to configure clouds.</p>"},{"location":"docs/installation/docker/#environment-variables","title":"Environment variables","text":"<p>Here's the list of environment variables which you can override:</p> <ul> <li><code>DSTACK_SERVER_ADMIN_TOKEN</code> \u2013 (Optional) The default token of the <code>admin</code> user. By default, it's generated randomly   at the first startup.</li> <li><code>DSTACK_SERVER_DIR</code> \u2013 (Optional) The path to the directory where the <code>dstack</code>server stores the state. Defaults to <code>~/.dstack/server</code>.</li> </ul>"},{"location":"docs/installation/docker/#persisting-state-in-a-cloud-storage","title":"Persisting state in a cloud storage","text":"<p>By default, <code>dstack</code> saves state in a local directory (<code>$DSTACK_SERVER_DIR/data</code>). If you want to persist state automatically to a cloud object storage, you can configure the following environment variables.</p> <ul> <li><code>LITESTREAM_REPLICA_URL</code> - The url of the cloud object storage.   Examples: <code>s3://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>gcs://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>abs://&lt;storage-account&gt;@&lt;container-name&gt;/&lt;path&gt;</code>, etc.</li> </ul> AWS S3 <p>To persist state into an AWS S3 bucket, provide the following environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> - The AWS access key ID</li> <li><code>AWS_SECRET_ACCESS_KEY</code> -  The AWS secret access key</li> </ul> GCP Storage <p>To persist state into an AWS S3 bucket, provide one of the following environment variables:</p> <ul> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> - The path to the GCP service account key JSON file</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> - The GCP service account key JSON</li> </ul> Azure Blob Storage <p>To persist state into an Azure blog storage, provide the following environment variable.</p> <ul> <li><code>LITESTREAM_AZURE_ACCOUNT_KEY</code> - The Azure storage account key</li> </ul> <p>More details on options for configuring persistent state.</p>"},{"location":"docs/installation/pip/","title":"pip","text":"<p>The easiest way to install <code>dstack</code>, is via <code>pip</code>:</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\"\n$ dstack start\n\nThe server is available at http://127.0.0.1:3000?token=b934d226-e24a-4eab-eb92b353b10f\n</code></pre> <p>Configure clouds</p> <p>Upon startup, the server sets up the default project called <code>main</code>. Prior to using <code>dstack</code>, make sure to configure clouds.</p>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Instead of configuring resources and other run options through<code>dstack run</code>,  you can do so via <code>.dstack/profiles.yml</code> in the root folder of the project. </p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: large\n\n    resources:\n      memory: 24GB  # (Optional) The minimum amount of RAM memory\n      gpu:\n        name: A100 # (Optional) The name of the GPU\n        memory: 40GB # (Optional) The minimum amount of GPU memory \n      shm_size: 8GB # (Optional) The size of shared memory\n\n    spot_policy: auto # (Optional) The spot policy. Supports `spot`, `on-demand, and `auto`.\n\n    max_price: 1.5 # (Optional) The maximum price per instance per hour\n\n    max_duration: 1d # (Optional) The maximum duration of the run.\n\n    retry:\n      retry-limit: 3h # (Optional) To wait for capacity\n\n    backends: [azure, lambda]  # (Optional) Use only listed backends \n\n    default: true # (Optional) Activate the profile by default\n</code></pre> <p>You can mark any profile as default or pass its name via <code>--profile</code> to <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/profiles.yml/#profile","title":"Profile","text":"Property Description Type Default value <code>name</code> The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code> <code>str</code> required <code>backends</code> The backends to consider for provisionig (e.g., \"[aws, gcp]\") <code>Optional[List[BackendType]]</code> <code>None</code> <code>resources</code> The minimum resources of the instance to be provisioned <code>ProfileResources</code> <code>cpu=2 memory=8192 gpu=None shm_size=None</code> <code>spot_policy</code> The policy for provisioning spot or on-demand instances: spot, on-demand, or auto <code>Optional[SpotPolicy]</code> <code>None</code> <code>retry_policy</code> The policy for re-submitting the run <code>Optional[ProfileRetryPolicy]</code> <code>None</code> <code>max_duration</code> The maximum duration of a run (e.g., 2h, 1d, etc). After it elapses, the run is forced to stop. <code>Union[Literal['off'],str,int,NoneType]</code> <code>None</code> <code>max_price</code> The maximum price per hour, in dollars <code>Optional[float]</code> <code>None</code> <code>default</code> If set to true, <code>dstack run</code> will use this profile by default. <code>bool</code> <code>False</code>"},{"location":"docs/reference/profiles.yml/#profileresources","title":"ProfileResources","text":"Property Description Type Default value <code>cpu</code> The minimum number of CPUs <code>Optional[int]</code> <code>2</code> <code>memory</code> The minimum size of RAM memory (e.g., \"16GB\") <code>Union[int,str,NoneType]</code> <code>8GB</code> <code>gpu</code> The minimum number of GPUs or a GPU spec <code>Union[int,ProfileGPU,NoneType]</code> <code>None</code> <code>shm_size</code> The size of shared memory (e.g., \"8GB\"). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this. <code>Union[int,str,NoneType]</code> <code>None</code>"},{"location":"docs/reference/profiles.yml/#profilegpu","title":"ProfileGPU","text":"Property Description Type Default value <code>name</code> The name of the GPU (e.g., \"A100\" or \"H100\") <code>Optional[str]</code> <code>None</code> <code>count</code> The minimum number of GPUs <code>int</code> <code>1</code> <code>memory</code> The minimum size of a single GPU memory (e.g., \"16GB\") <code>Union[int,str,NoneType]</code> <code>None</code> <code>total_memory</code> The minimum total size of all GPUs memory (e.g., \"32GB\") <code>Union[int,str,NoneType]</code> <code>None</code> <code>compute_capability</code> The minimum compute capability of the GPU (e.g., 7.5) <code>Union[float,str,Tuple,NoneType]</code> <code>None</code>"},{"location":"docs/reference/profiles.yml/#profileretrypolicy","title":"ProfileRetryPolicy","text":"Property Description Type Default value <code>retry</code> Whether to retry the run on failure or not <code>bool</code> <code>False</code> <code>limit</code> The maximum period of retrying the run, e.g., 4h or 1d <code>Union[int,str,NoneType]</code> <code>None</code>"},{"location":"docs/reference/api/python/","title":"API","text":"<p>The Python API allows for running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>repos</code> <code>RepoCollection</code> <p>operations with repos</p> <code>backends</code> <code>BackendCollection</code> <p>operations with backends</p> <code>runs</code> <code>RunCollection</code> <p>operations with runs</p> <code>client</code> <code>APIClient</code> <p>low-level server API client</p> <code>project</code> <code>str</code> <p>project name</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.__init__","title":"<code>__init__(api_client, project_name, ssh_identity_file=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>api_client</code> <code>APIClient</code> <p>low-level server API client</p> required <code>project_name</code> <code>str</code> <p>project name used for runs</p> required <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using global config <code>~/.dstack/config.yml</code></p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>dstack server url, e.g. <code>http://localhost:3000/</code></p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>dstack Client</p>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>ports</code> <code>List[PortMapping]</code> <p>Port numbers/mapping to expose</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private container</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p>"},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>port</code> <code>PortMapping</code> <p>The port, that application listens to or the mapping</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private container</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p>"},{"location":"docs/reference/api/python/#dstack.api.FineTuningTask","title":"<code>dstack.api.FineTuningTask</code>","text":"<p>This task configuration loads a given model from the Hugging Face hub and fine-tunes it on the provided dataset (also from Hugging Face hub), utilizing the SFT and QLoRA techniques. The final model is pushed to Hugging Face hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.</p> required <code>dataset_name</code> <code>str</code> <p>The instruction dataset to use.</p> required <code>new_model_name</code> <code>Optional[str]</code> <p>The name to use for pushing the fine-tuned model to the Hugging Face Hub. If unset, it defaults to the name of the run.</p> <code>None</code> <code>env</code> <code>Dict[str, str]</code> <p>The list of environment variables, e.g. <code>\"HUGGING_FACE_HUB_TOKEN\"</code>, <code>\"WANDB_API_KEY\"</code>, <code>\"WANDB_PROJECT\"</code>, etc.</p> <code>None</code> <code>report_to</code> <code>Optional[str]</code> <p>Supported integrations include <code>\"wandb\"</code> and <code>\"tensorboard\"</code>.</p> <code>None</code> <code>per_device_train_batch_size</code> <code>int</code> <p>Batch size per GPU for training.</p> <code>4</code> <code>per_device_eval_batch_size</code> <code>int</code> <p>Batch size per GPU for evaluation.</p> <code>4</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>Number of update steps to accumulate the gradients for.</p> <code>1</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate (AdamW optimizer).</p> <code>0.0002</code> <code>max_grad_norm</code> <code>float</code> <p>Maximum gradient normal (gradient clipping).</p> <code>0.3</code> <code>weight_decay</code> <code>float</code> <p>Weight decay to apply to all layers except bias/LayerNorm weights.</p> <code>0.001</code> <code>lora_alpha</code> <code>int</code> <p>Alpha parameter for LoRA scaling.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>Dropout probability for LoRA layers.</p> <code>0.1</code> <code>lora_r</code> <code>int</code> <p>LoRA attention dimension.</p> <code>64</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length to use.</p> <code>None</code> <code>use_4bit</code> <code>bool</code> <p>Activate 4bit precision base model loading.</p> <code>True</code> <code>use_nested_quant</code> <code>bool</code> <p>Activate nested quantization for 4bit base models.</p> <code>True</code> <code>bnb_4bit_compute_dtype</code> <code>str</code> <p>Compute dtype for 4bit base models.</p> <code>'float16'</code> <code>bnb_4bit_quant_type</code> <code>str</code> <p>Quantization type fp4 or nf4.</p> <code>'nf4'</code> <code>num_train_epochs</code> <code>float</code> <p>The number of training epochs for the reward model.</p> <code>1</code> <code>fp16</code> <code>bool</code> <p>Enables fp16 training.</p> <code>False</code> <code>bf16</code> <code>bool</code> <p>Enables bf16 training.</p> <code>False</code> <code>packing</code> <code>bool</code> <p>Use packing dataset creating.</p> <code>False</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Enables gradient checkpointing.</p> <code>True</code> <code>optim</code> <code>str</code> <p>The optimizer to use.</p> <code>'paged_adamw_32bit'</code> <code>lr_scheduler_type</code> <code>str</code> <p>Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis</p> <code>'constant'</code> <code>max_steps</code> <code>int</code> <p>How many optimizer update steps to take</p> <code>-1</code> <code>warmup_ratio</code> <code>float</code> <p>Fraction of steps to do a warmup for</p> <code>0.03</code> <code>group_by_length</code> <code>bool</code> <p>Group sequences into batches with same length. Saves memory and speeds up training considerably.</p> <code>True</code> <code>save_steps</code> <code>int</code> <p>Save checkpoint every X updates steps.</p> <code>0</code> <code>logging_steps</code> <code>int</code> <p>Log every X updates steps.</p> <code>25</code>"},{"location":"docs/reference/api/python/#dstack.api.CompletionService","title":"<code>dstack.api.CompletionService</code>","text":"<p>This service configuration loads a given model from the Hugging Face hub and deploys it as a public endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model that you want to deploy from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.</p> required <code>env</code> <code>Dict[str, str]</code> <p>The list of environment variables, e.g. <code>\"HUGGING_FACE_HUB_TOKEN\"</code>, <code>\"WANDB_API_KEY\"</code>, <code>\"WANDB_PROJECT\"</code>, etc.</p> <code>None</code> <code>quantize</code> <code>Optional[str]</code> <p>Whether you want the model to be quantized. Supported values: <code>\"awq\"</code>, <code>\"eetq\"</code>, <code>\"gptq\"</code>, and <code>\"bitsandbytes\"</code>.</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype to be forced upon the model. This option cannot be used with <code>quantize</code>.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.CompletionTask","title":"<code>dstack.api.CompletionTask</code>","text":"<p>This task configuration loads a specified model from the Hugging Face hub and runs it as a private endpoint while forwarding the port to the local machine.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model that you want to deploy from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.</p> required <code>env</code> <code>Dict[str, str]</code> <p>The list of environment variables, e.g. <code>\"HUGGING_FACE_HUB_TOKEN\"</code>, <code>\"WANDB_API_KEY\"</code>, <code>\"WANDB_PROJECT\"</code>, etc.</p> <code>None</code> <code>quantize</code> <code>Optional[str]</code> <p>Whether you want the model to be quantized. Supported values: <code>\"awq\"</code>, <code>\"eetq\"</code>, <code>\"gptq\"</code>, and <code>\"bitsandbytes\"</code>.</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype to be forced upon the model. This option cannot be used with <code>quantize</code>.</p> <code>None</code> <code>local_port</code> <code>int</code> <p>The local port to forward the traffic to.</p> <code>80</code>"},{"location":"docs/reference/api/python/##dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>JobStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.Client.runs</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.exec_plan","title":"<code>exec_plan(run_plan, repo, reserve_ports=True)</code>","text":"<p>Execute run plan</p> <p>Parameters:</p> Name Type Description Default <code>run_plan</code> <code>RunPlan</code> <p>result of <code>get_plan</code> call</p> required <code>repo</code> <code>Repo</code> <p>repo to use for the run</p> required <code>reserve_ports</code> <code>bool</code> <p>reserve local ports before submit</p> <code>True</code> <p>Returns:</p> Type Description <code>SubmittedRun</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get_plan","title":"<code>get_plan(configuration, repo, configuration_path=None, backends=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None)</code>","text":"<p>Get run plan. Same arguments as <code>submit</code></p> <p>Returns:</p> Type Description <code>RunPlan</code> <p>run plan</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>AnyRunConfiguration</code> <p>run configuration</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>run configuration path, relative to <code>repo_dir</code></p> <code>None</code> <code>repo</code> <code>Optional[Repo]</code> <p>repo to use for the run</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>list of allowed backend for provisioning</p> <code>None</code> <code>resources</code> <code>Optional[ProfileResources]</code> <p>minimal resources for provisioning</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>spot policy for provisioning</p> <code>None</code> <code>retry_policy</code> <code>Optional[ProfileRetryPolicy]</code> <p>retry policy for interrupted jobs</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>max instance running duration in seconds</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>max instance price in dollars per hour for provisioning</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>working directory relative to <code>repo_dir</code></p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>desired run_name. Must be unique in the project</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>reserve local ports before submit</p> <code>True</code> <p>Returns:</p> Type Description <code>SubmittedRun</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.Client.repos</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Upload credentials and initializes the repo in the project</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>repo to initialize</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>SSH private key to access the remote repo</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>GitHub OAuth token to access the remote repo</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.is_initialized","title":"<code>is_initialized(repo)</code>","text":"<p>Checks if the remote repo is initialized in the project</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>repo to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>repo is initialized</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.load","title":"<code>load(repo_dir, local=False, init=False, git_identity_file=None, oauth_token=None)</code>","text":"<p>Loads the repo from the local directory using global config</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>repo root directory</p> required <code>local</code> <code>bool</code> <p>do not try to load <code>RemoteRepo</code> first</p> <code>False</code> <code>init</code> <code>bool</code> <p>initialize the repo if it's not initialized</p> <code>False</code> <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>path to an SSH private key to access the remote repo</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>GitHub OAuth token to access the remote repo</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>if the repo is not initialized and <code>init</code> is <code>False</code></p> <p>Returns:</p> Name Type Description <code>repo</code> <code>Union[RemoteRepo, LocalRepo]</code> <p>initialized repo</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.backends","title":"<code>dstack.api.Client.backends</code>","text":"<p>Operations with backends</p>"},{"location":"docs/reference/api/python/#dstack.api.BackendCollection.list","title":"<code>list()</code>","text":"<p>List available backends in the project</p> <p>Returns:</p> Type Description <code>List[Backend]</code> <p>backends</p>"},{"location":"docs/reference/api/python/_artifacts_archived/","title":"Python API","text":""},{"location":"docs/reference/api/python/_artifacts_archived/#artifacts","title":"Artifacts","text":"<p>This API allows you to save and load data and models from the artifact storage.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#dstackartifactsupload","title":"dstack.artifacts.upload","text":"<pre><code>def upload(local_path: str, artifact_path: Optional[str] = None,\n           tag: Optional[str] = None)\n</code></pre> <p>Uploads the files located under the <code>local_path</code> folder as artifacts.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#argument-reference","title":"Argument reference","text":"<ul> <li><code>tag</code> \u2013 (Optional) If <code>tag</code> is not specified, artifacts are automatically attached to the current run.     If <code>tag is specified</code>, artifacts are attached to the given tag name.</li> <li><code>local_path</code> \u2013 (Optional) The path to a local folder with the files to upload.</li> <li><code>artifact_path</code> \u2013 (Optional) The path under which the files will be stored.</li> </ul>"},{"location":"docs/reference/api/python/_artifacts_archived/#usage-example","title":"Usage example:","text":"<pre><code>from dstack import artifacts\n\n# Uploads files under \"datasets/dataset1\" as artifacts and attach them to the current run \nartifacts.upload(\"datasets/dataset1\")\n\n# Uploads files under \"datasets/dataset1\" as artifacts and creates a tag \"my_tag\"\nartifacts.upload(\"datasets/dataset1\", tag=\"my_tag\")\n</code></pre>"},{"location":"docs/reference/api/python/_artifacts_archived/#dstackartifactsdownload","title":"dstack.artifacts.download","text":"<pre><code>def download(run: Optional[str] = None, tag: Optional[str] = None,\n             artifact_path: Optional[str] = None, local_path: Optional[str] = None)\n</code></pre> <p>Downloads artifact files of a given run or a tag.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#argument-reference_1","title":"Argument reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>run</code> \u2013 The run to download the artifacts from</li> <li><code>tag</code> \u2013 The tag to download the artifacts from</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>artifact_path</code> \u2013 (Optional) The path to the artifact files to download.      If not specified, all artifact files are downloaded.</li> <li><code>local_path</code> \u2013 (Optional) The local path to save the files to.     If not specified, files are downloaded to the current directory.</li> </ul>"},{"location":"docs/reference/api/python/_artifacts_archived/#usage-example_1","title":"Usage example","text":"<pre><code>from dstack import artifacts\n\n# Downloads all artifact files of a run to the current directory\nartifacts.download(run=\"sharp-shrimp-1\")\n\n# Downloads all artifact files of the \"my_tag\" tag and saves them to \"my_model\"\nartifacts.download(tag=\"my_tag\", local_path=\"my_model\")\n</code></pre> <p>NOTE:</p> <p>Currently, the Python API can only be used from dev environments and tasks.</p>"},{"location":"docs/reference/api/python/server/","title":"Low-level API client","text":""},{"location":"docs/reference/api/python/server/#dstack.api.server.APIClient","title":"<code>dstack.api.server.APIClient</code>","text":"<p>Low-level API client for interacting with dstack server. Implements all API endpoints</p> <p>Attributes:</p> Name Type Description <code>users</code> <code>UsersAPIClient</code> <p>operations with users</p> <code>projects</code> <code>ProjectsAPIClient</code> <p>operations with projects</p> <code>backends</code> <code>BackendsAPIClient</code> <p>operations with backends</p> <code>repos</code> <code>ReposAPIClient</code> <p>operations with repos</p> <code>runs</code> <code>RunsAPIClient</code> <p>operations with runs</p> <code>logs</code> <code>LogsAPIClient</code> <p>operations with logs</p> <code>gateways</code> <code>GatewaysAPIClient</code> <p>operations with gateways</p>"},{"location":"docs/reference/api/python/server/#dstack.api.server.APIClient.__init__","title":"<code>__init__(base_url, token)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>API endpoints prefix, e.g. <code>http://127.0.0.1:3000/</code></p> required <code>token</code> <code>str</code> <p>API token</p> required"},{"location":"docs/reference/backends/aws/","title":"AWS","text":"<p>The <code>AWS</code> backend type allows provisioning infrastructure and storing artifacts in an AWS account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/aws/#create-an-s3-bucket","title":"Create an S3 bucket","text":"<p>First, you need to create an S3 bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/aws/#create-an-iam-user","title":"Create an IAM user","text":"<p>The next step is to create an IAM user and  grant this user permissions to perform actions on the <code>s3</code>, <code>logs</code>, <code>secretsmanager</code>, <code>ec2</code>, and <code>iam</code> services.</p> IAM policy template <p>If you'd like to limit the permissions to the most narrow scope, feel free to use the IAM policy template below.</p> <p>Replace <code>{bucket_name}</code> and <code>{bucket_name_under_score}</code> variables in the template below with the values that correspond to your S3 bucket.</p> <p>For <code>{bucket_name}</code>, use the name of the S3 bucket.  For <code>{bucket_name_under_score}</code>, use the same but with dash characters replaced to underscores  (e.g. if <code>{bucket_name}</code> is <code>my-awesome-project</code>, then  <code>{bucket_name_under_score}</code>  must be <code>my_awesome_project</code>.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\",\n        \"s3:GetLifecycleConfiguration\",\n        \"s3:PutLifecycleConfiguration\",\n        \"s3:PutObjectTagging\",\n        \"s3:GetObjectTagging\",\n        \"s3:DeleteObjectTagging\",\n        \"s3:GetBucketAcl\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::{bucket_name}\",\n        \"arn:aws:s3:::{bucket_name}/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:DescribeLogGroups\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:*:*:log-group:*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:FilterLogEvents\",\n        \"logs:TagLogGroup\",\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:*:*:log-group:/dstack/jobs/{bucket_name}*:*\",\n        \"arn:aws:logs:*:*:log-group:/dstack/runners/{bucket_name}*:*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:UpdateSecret\",\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:CreateSecret\",\n        \"secretsmanager:PutSecretValue\",\n        \"secretsmanager:PutResourcePolicy\",\n        \"secretsmanager:TagResource\",\n        \"secretsmanager:DeleteSecret\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/credentials/*\",\n        \"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/secrets/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeSpotPriceHistory\",\n        \"ec2:RunInstances\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:AuthorizeSecurityGroupEgress\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CancelSpotInstanceRequests\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:ResourceTag/dstack_bucket\": \"{bucket_name}\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetRole\",\n        \"iam:CreateRole\",\n        \"iam:AttachRolePolicy\",\n        \"iam:TagRole\"\n      ],\n      \"Resource\": \"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:CreatePolicy\",\n        \"iam:TagPolicy\"\n      ],\n      \"Resource\": \"arn:aws:iam::*:policy/dstack_policy_{bucket_name_under_score}*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetInstanceProfile\",\n        \"iam:CreateInstanceProfile\",\n        \"iam:AddRoleToInstanceProfile\",\n        \"iam:TagInstanceProfile\",\n        \"iam:PassRole\"\n      ],\n      \"Resource\": [\n        \"arn:aws:iam::*:instance-profile/dstack_role_{bucket_name_under_score}*\",\n        \"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/reference/backends/aws/#set-up-aws-credentials","title":"Set up AWS credentials","text":"<p><code>dstack</code> support two methods to authenticate with AWS: Default credentials and Access key.</p>"},{"location":"docs/reference/backends/aws/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up AWS credentials set up on your machine (e.g. credentials stored as AWS profiles or environment variables). You can use default credentials if you don't want to enter and store AWS credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/aws/#access-key","title":"Access key","text":"<p><code>dstack</code> also support authentication using an access key. To create an access key, follow this guide. Once the access key is created, make sure to download the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>.</p>"},{"location":"docs/reference/backends/aws/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>AWS</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/aws/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Region</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The S3 bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Access key ID</code> - (Optional) The Access key ID to authenticate <code>dstack</code> </li> <li><code>Secret access key</code> - (Optional) The Secret access key to authenticate <code>dstack</code></li> <li><code>Subnet</code> - (Optional) The EC2 subnet is required to provision infrastructure using a non-default VPC and subnet. If   not specified, dstack will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/azure/","title":"Azure","text":"<p>The <code>Azure</code> backend type allows provisioning infrastructure and storing artifacts in an Azure account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/azure/#create-a-resource-group","title":"Create a resource group","text":"<p>First, create a new Azure resource group. All resource created by <code>dstack</code> will belong to this group.</p>"},{"location":"docs/reference/backends/azure/#create-a-storage-account","title":"Create a storage account","text":"<p>Next, create an Azure storage account in the newly created resource group. <code>dstack</code> will use this storage account to store metadata and artifacts.</p> <p>NOTE:</p> <p>Make sure that the storage account is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/azure/#set-up-azure-credentials","title":"Set up Azure credentials","text":"<p><code>dstack</code> support two methods to authenticate with Azure: Default credentials and Client secret.</p>"},{"location":"docs/reference/backends/azure/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up Azure default credentials set up on your machine. You can use default credentials if you don't want to enter and store Azure credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/azure/#client-secret","title":"Client secret","text":"<p><code>dstack</code> also supports an Azure Active Directory app credentials to authenticate with your Azure account. If the app is <code>Owner</code> of the subscription, <code>dstack</code> will automatically set up all the resources required to run workflows. It will also create a separate managed identity with fine-grained permissions to authenticate with your Azure account when running workflows. </p> <p>To create new application credentials using the Azure CLI, run:</p> <pre><code>az ad sp create-for-rbac --name dstack-app --role Owner --scopes /subscriptions/$SUBSCRIPTION_ID --query \"{ client_id: appId, client_secret: password, tenant_id: tenant }\"\n</code></pre>"},{"location":"docs/reference/backends/azure/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>Azure</code> in the <code>Type</code> field.</p> <p>It may take up to a minute to set up Azure resource after saving the backend settings.</p>"},{"location":"docs/reference/backends/azure/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Tenant ID</code> - (Required) The Azure Tenant ID</li> <li><code>Subscription ID</code> - (Required) The Azure Subscription ID</li> <li><code>Location</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Storage account</code> - (Required) The Storage account to store state and artifacts (must be in the same region)</li> </ul> <p>The following fields are optional:</p> <ul> <li><code>Client ID</code> - (Optional) The Client ID to authenticate <code>dstack</code></li> <li><code>Client Secret</code> - (Optional) The Client secret to authenticate <code>dstack</code></li> </ul>"},{"location":"docs/reference/backends/gcp/","title":"GCP","text":"<p>The GCP backend type allows provisioning infrastructure and storing artifacts in a GCP account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/gcp/#enable-apis","title":"Enable APIs","text":"<p>First, ensure that the required APIs are enabled in your GCP project.</p> Required APIs <p>Here's the list of APIs that have to be enabled for the project.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"docs/reference/backends/gcp/#create-a-storage-bucket","title":"Create a storage bucket","text":"<p>Once the APIs are enabled, proceed and create a storage bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/gcp/#set-up-gcp-credentials","title":"Set up GCP credentials","text":"<p><code>dstack</code> support two methods to authenticate with GCP: Default credentials and Service account key.</p>"},{"location":"docs/reference/backends/gcp/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up GCP default credentials set up on your machine. You can use default credentials if you don't want to enter and store GCP credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/gcp/#service-account-key","title":"Service account key","text":"<p><code>dstack</code> also support authentication using a service account key. Follow this guide to create a service account and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p> <p>Once the service account is set up, create a key for it and download the corresponding JSON file.</p>"},{"location":"docs/reference/backends/gcp/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>GCP</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/gcp/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Location</code> - (Required) The location where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Region</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Zone</code> - (Required) The zone where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The storage bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Service account</code> - (Optional) The JSON file of the service account key to authenticate <code>dstack</code> </li> <li><code>Subnet</code> - (Optional) The VPC subnet where <code>dstack</code> will provision infrastructure. If   not specified, <code>dstack</code> will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/lambda/","title":"Lambda Cloud","text":"<p>The <code>Lambda</code> backend allows provisioning infrastructure in Lambda Cloud while storing  artifacts in an S3 bucket.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/lambda/#set-up-storage","title":"Set up storage","text":"<p>As Lambda Cloud doesn't have its own object storage, <code>dstack</code> requires you to specify an S3 bucket,  along with AWS credentials, for storing state and artifacts.</p>"},{"location":"docs/reference/backends/lambda/#create-an-s3-bucket","title":"Create an S3 bucket","text":"<p>First, you need to create an S3 bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p>"},{"location":"docs/reference/backends/lambda/#create-an-iam-user","title":"Create an IAM user","text":"<p>The next step is to create an IAM user and grant this user permissions to perform actions on the <code>s3</code> service.</p> Logs and secrets <p>If you want <code>dstack</code> to also store logs and secrets, you can optionally grant permissions  to the <code>logs</code> and <code>secretsmanager</code> services.</p>"},{"location":"docs/reference/backends/lambda/#create-an-access-key","title":"Create an access key","text":"<p>To create an access key, follow this guide. Once the access key is created, make sure to download the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>.</p>"},{"location":"docs/reference/backends/lambda/#set-up-api-key","title":"Set up API key","text":"<p>Then, you'll need a Lambda Cloud API key. Log into your Lambda Cloud account, click <code>API keys</code> in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p></p>"},{"location":"docs/reference/backends/lambda/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>Lambda</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/lambda/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>API key</code> - (Required) The [API key] to authenticate <code>dstack</code> with Lambda Cloud</li> <li><code>Regions</code> - (Required) The list of regions where <code>dstack</code> may provision infrastructure. It is recommended to select as many regions as possible to maximize availability.</li> <li><code>Storage</code> - (Required) The storage provider that <code>dstack</code> will use to store the state and artifacts. Currently, only <code>AWS</code> is supported.</li> <li><code>Access key ID</code> - (Required) The Access key ID to authenticate <code>dstack</code> with AWS</li> <li><code>Secret access key</code> - (Required) The Secret access key to authenticate <code>dstack</code> with AWS</li> <li><code>Bucket</code> - (Required) The S3 bucket to store state and artifacts</li> </ul>"},{"location":"docs/reference/cli/","title":"CLI","text":""},{"location":"docs/reference/cli/#commands","title":"Commands","text":""},{"location":"docs/reference/cli/#dstack-server","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p> <pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL]\n                     [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to WARNING.\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/#dstack-init","title":"dstack init","text":"<p>This command initializes the current folder as a repo.</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        A path to the private SSH key file for non-public\n                        repositories\n  --ssh-identity SSH_PRIVATE_KEY\n                        A path to the private SSH key file for SSH tunneling\n  --local               Do not use git\n</code></pre> Git credentials <p>If the current folder is a Git repo, the command authorizes <code>dstack</code> to access it. By default, the command uses the default Git credentials configured for the repo.  You can override these credentials via <code>--token</code> (OAuth token) or <code>--git-identity</code>.</p> Custom SSH key <p>By default, this command generates an SSH key that will be used for port forwarding and SSH access to running workloads.  You can override this key via <code>--ssh-identity</code>.</p>"},{"location":"docs/reference/cli/#dstack-run","title":"dstack run","text":"<p>This command runs a given configuration.</p> <pre><code>$ dstack run . --help\nUsage: dstack run [--project NAME] [-h [TYPE]] [-f FILE] [-n RUN_NAME] [-d]\n                  [-y] [--max-offers MAX_OFFERS] [--profile NAME] [--gpu SPEC]\n                  [--max-price PRICE] [--max-duration DURATION] [-b NAME]\n                  [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                  [--retry | --no-retry | --retry-limit DURATION]\n                  [-e KEY=VALUE]\n                  working_dir\n\nPositional Arguments:\n  working_dir\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit. TYPE is one of task,\n                        dev-environment, service\n  -f, --file FILE       The path to the run configuration file. Defaults to\n                        WORKING_DIR/.dstack.yml\n  -n, --name RUN_NAME   The name of the run. If not specified, a random name\n                        is assigned\n  -d, --detach          Do not poll logs and run status\n  -y, --yes             Do not ask for plan confirmation\n  --max-offers MAX_OFFERS\n                        Number of offers to show in the run plan\n  -e, --env KEY=VALUE   Environment variables\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --gpu SPEC            Request a GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --max-price PRICE     The maximum price per hour, in dollars\n  --max-duration DURATION\n  -b, --backend NAME    The backends that will be tried for provisioning\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-limit DURATION\n</code></pre> .gitignore <p>When running anything via CLI, <code>dstack</code> uses the exact version of code from your project directory.</p> <pre><code>If there are large files, consider creating a `.gitignore` file to exclude them for better performance.\n</code></pre>"},{"location":"docs/reference/cli/#dstack-ps","title":"dstack ps","text":"<p>This command shows the status of runs.</p> <pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-stop","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p> <pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/#dstack-logs","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p> <pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose\n  -a, --attach          Set up an SSH tunnel, and print logs as they follow.\n  --ssh-identity SSH_PRIVATE_KEY\n                        A path to the private SSH key file for SSH tunneling\n</code></pre>"},{"location":"docs/reference/cli/#dstack-config","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p> <pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN]\n                     [--default] [--remove] [--no-default]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  --default          Set the project as default. It will be used when\n                     --project is omitted in commands.\n  --remove           Delete project configuration\n  --no-default       Do not prompt to set the project as default.\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway","title":"dstack gateway","text":"<p>A gateway is required for running services.</p>"},{"location":"docs/reference/cli/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p> <pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-create","title":"dstack gateway create","text":"<p>The <code>dstack gateway create</code> command creates a new gateway instance in the project.</p> <pre><code>$ dstack gateway create --help\nUsage: dstack gateway create [-h] --backend {aws,gcp,azure} --region REGION\n                             [--set-default] [--name NAME] --domain DOMAIN\n\nOptions:\n  -h, --help            show this help message and exit\n  --backend {aws,gcp,azure}\n  --region REGION\n  --set-default         Set as default gateway for the project\n  --name NAME           Set a custom name for the gateway\n  --domain DOMAIN       Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p> <pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-update","title":"dstack gateway update","text":"<p>The <code>dstack gateway update</code> command updates the specified gateway.</p> <pre><code>$ dstack gateway update --help\nUsage: dstack gateway update [-h] [--set-default] [--domain DOMAIN] name\n\nPositional Arguments:\n  name             The name of the gateway\n\nOptions:\n  -h, --help       show this help message and exit\n  --set-default    Set it the default gateway for the project\n  --domain DOMAIN  Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#environment-variables","title":"Environment variables","text":"Name Description Default <code>DSTACK_CLI_LOG_LEVEL</code> Configures CLI logging level <code>CRITICAL</code> <code>DSTACK_PROFILE</code> Has the same effect as <code>--profile</code> <code>None</code> <code>DSTACK_PROJECT</code> Has the same effect as <code>--project</code> <code>None</code> <code>DSTACK_DEFAULT_CREDS_DISABLED</code> Disables default credentials detection if set <code>None</code> <code>DSTACK_LOCAL_BACKEND_ENABLED</code> Enables local backend for debug if set <code>None</code> <code>DSTACK_RUNNER_VERSION</code> Sets exact runner version for debug <code>latest</code> <code>DSTACK_SERVER_ADMIN_TOKEN</code> Has the same effect as <code>--token</code> <code>None</code> <code>DSTACK_SERVER_DIR</code> Sets path to store data and server configs <code>~/.dstack/server</code> <code>DSTACK_SERVER_HOST</code> Has the same effect as <code>--host</code> <code>127.0.0.1</code> <code>DSTACK_SERVER_LOG_LEVEL</code> Has the same effect as <code>--log-level</code> <code>WARNING</code> <code>DSTACK_SERVER_PORT</code> Has the same effect as <code>--port</code> <code>3000</code> <code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> Sets root logger log level <code>ERROR</code> <code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> Sets uvicorn logger log level <code>ERROR</code>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<p>Configurations are YAML files that describe what you want to run with <code>dstack</code>. Configurations can be of three types: <code>dev-environment</code>, <code>task</code>, and <code>service</code>.</p> <p>Filename</p> <p>The configuration file must be named with the suffix <code>.dstack.yml</code>. For example, you can name the configuration file <code>.dstack.yml</code> or <code>app.dstack.yml</code>. You can define these configurations anywhere within your project. </p> <p>Each folder may have one default configuration file named <code>.dstack.yml</code>.</p> <p>Read more about configuration types:</p> <ul> <li><code>type: dev-environment</code></li> <li><code>type: task</code></li> <li><code>type: service</code></li> </ul>"},{"location":"docs/reference/dstack.yml/dev-environment/","title":".dstack.yml (dev-environment)","text":"<p>A dev environment is a cloud instance pre-configured with an IDE.</p> <p>The configuration file name must end with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#example","title":"Example","text":"<pre><code>type: dev-environment\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\nide: vscode\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#devenvironmentconfiguration","title":"DevEnvironmentConfiguration","text":"Property Description Type Default value <code>type</code> <code>Literal['dev-environment']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private container <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>ide</code> The IDE to run <code>Literal['vscode']</code> required <code>version</code> The version of the IDE <code>Optional[str]</code> <code>None</code> <code>init</code> The bash commands to run <code>List[str]</code> <code>[]</code>"},{"location":"docs/reference/dstack.yml/service/","title":".dstack.yml (service)","text":"<p>A service is a web app accessible from the Internet.</p> <p>The configuration file name must end with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>service.dstack.yml</code> are both acceptable).</p>"},{"location":"docs/reference/dstack.yml/service/#example","title":"Example","text":"<pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv: \n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ \n\nport: 80\n\ncommands:\n  - text-generation-launcher --hostname 0.0.0.0 --port 80 --trust-remote-code\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/dstack.yml/service/#serviceconfiguration","title":"ServiceConfiguration","text":"Property Description Type Default value <code>type</code> <code>Literal['service']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private container <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>commands</code> The bash commands to run <code>List[str]</code> required <code>port</code> The port, that application listens to or the mapping <code>Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]</code> required"},{"location":"docs/reference/dstack.yml/task/","title":".dstack.yml (task)","text":"<p>A task can be a batch job or a web app.</p> <p>The configuration file name must end with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p>"},{"location":"docs/reference/dstack.yml/task/#example","title":"Example","text":"<pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/dstack.yml/task/#taskconfiguration","title":"TaskConfiguration","text":"Property Description Type Default value <code>type</code> <code>Literal['task']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private container <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>commands</code> The bash commands to run <code>List[str]</code> required"},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":""},{"location":"docs/reference/server/config.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/server/config.yml/#serverconfig","title":"ServerConfig","text":"Property Description Type Default value <code>projects</code> <code>List[ProjectConfig]</code> required"},{"location":"docs/reference/server/config.yml/#projectconfig","title":"ProjectConfig","text":"Property Description Type Default value <code>name</code> <code>str</code> required <code>backends</code> <code>Union[AWSConfigInfoWithCreds, AzureConfigInfoWithCreds, GCPConfigInfoWithCreds, LambdaConfigInfoWithCreds, TensorDockConfigInfoWithCreds, VastAIConfigInfoWithCreds]</code> required"},{"location":"docs/reference/server/config.yml/#awsconfig","title":"AWSConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['aws']</code> <code>aws</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[AWSAccessKeyCreds,AWSDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#awsdefaultcreds","title":"AWSDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#awsaccesskeycreds","title":"AWSAccessKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['access_key']</code> <code>access_key</code> <code>access_key</code> <code>str</code> required <code>secret_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#azureconfig","title":"AzureConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['azure']</code> <code>azure</code> <code>tenant_id</code> <code>str</code> required <code>subscription_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[AzureClientCreds,AzureDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#azuredefaultcreds","title":"AzureDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#azureclientcreds","title":"AzureClientCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['client']</code> <code>client</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required <code>tenant_id</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#datacrunchconfig","title":"DataCrunchConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['datacrunch']</code> <code>datacrunch</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>DataCrunchAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#datacrunchapikeycreds","title":"DataCrunchAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#gcpconfig","title":"GCPConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['gcp']</code> <code>gcp</code> <code>project_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[GCPServiceAccountCreds,GCPDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#gcpdefaultcreds","title":"GCPDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#gcpserviceaccountcreds","title":"GCPServiceAccountCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['service_account']</code> <code>service_account</code> <code>filename</code> <code>str</code> required <code>data</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#lambdaconfig","title":"LambdaConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['lambda']</code> <code>lambda</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>LambdaAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#lambdaapikeycreds","title":"LambdaAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#tensordockconfig","title":"TensorDockConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['tensordock']</code> <code>tensordock</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>TensorDockAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#tensordockapikeycreds","title":"TensorDockAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required <code>api_token</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#vastaiconfig","title":"VastAIConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['vastai']</code> <code>vastai</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>VastAIAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#vastaiapikeycreds","title":"VastAIAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"learn/deploy-python/","title":"Deploying LLMs using Python API","text":"<p>The Python API of <code>dstack</code> can be used to run tasks and services programmatically.</p> <p>To demonstrate how it works, we've created a simple Streamlit app that uses <code>dstack</code>'s API to deploy a quantized  version of Llama 2 to your cloud with a click of a button.</p> <p></p>"},{"location":"learn/deploy-python/#prerequisites","title":"Prerequisites","text":"<p>Before you can use <code>dstack</code> Python API, ensure you have set up the server.</p>"},{"location":"learn/deploy-python/#how-does-it-work","title":"How does it work?","text":""},{"location":"learn/deploy-python/#create-a-client","title":"Create a client","text":"<p>If you're familiar with Docker's Python SDK, you'll find <code>dstack</code>'s Python API  quite similar, except that it runs your workload in the cloud.</p> <p>To get started, create an instance of <code>dstack.Client</code> and use its methods to submit and manage runs.</p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError as e:\n    print(e)\n</code></pre>"},{"location":"learn/deploy-python/#create-a-task","title":"Create a task","text":"<p>NOTE:</p> <p>With <code>dstack.Client</code>, you can run tasks and services. Running a task allows you to programmatically access its ports and forward traffic to your local machine. For example, if you run an LLM as a task, you can access it on <code>localhost</code>. Services on the other hand allow deploying applications as public endpoints.</p> <p>In our example, we'll deploy an LLM as a task. To do this, we'll create a <code>dstack.Task</code> instance that configures how the LLM should be run.</p> <pre><code>from dstack.api import Task\n\nconfiguration = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": model_id},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"8080:80\"],  # LLM runs on port 80, forwarded to localhost:8080\n)\n</code></pre>"},{"location":"learn/deploy-python/#create-resources","title":"Create resources","text":"<p>Then, we'll need to specify the resources our LLM will require. To do this, we'll create a <code>dstack.Resources</code> instance:</p> <pre><code>from dstack.api import Resources, GPU\n\nif model_id == \"TheBloke/Llama-2-13B-chat-GPTQ\":\n    gpu_memory = \"20GB\"\nelif model_id == \"TheBloke/Llama-2-70B-chat-GPTQ\":\n    gpu_memory = \"40GB\"\n\nresources = Resources(gpu=GPU(memory=gpu_memory))\n</code></pre>"},{"location":"learn/deploy-python/#submit-the-run","title":"Submit the run","text":"<p>To deploy the LLM, we submit the task using <code>runs.submit()</code> in <code>dstack.Client</code>.</p> <pre><code>run_name = \"deploy-python\"\n\nrun = client.runs.submit(configuration=configuration, run_name=run_name, resources=resources)\n</code></pre>"},{"location":"learn/deploy-python/#attach-to-the-run","title":"Attach to the run","text":"<p>Then, we use the <code>attach()</code> method on <code>dstack.Run</code>. This method waits for the task to start,  and forwards the configured ports to <code>localhost</code>.</p> <pre><code>run.attach()\n</code></pre>"},{"location":"learn/deploy-python/#wait-for-the-endpoint-to-start","title":"Wait for the endpoint to start","text":"<p>Finally, we wait until <code>http://localhost:8080/health</code> returns <code>200</code>, which indicates that the LLM is deployed and ready to handle requests.</p> <pre><code>import time\nimport requests\n\nwhile True:\n    time.sleep(0.5)\n    try:\n        r = requests.get(\"http://localhost:8080/health\")\n        if r.status_code == 200:\n            break\n    except Exception:\n        pass\n</code></pre>"},{"location":"learn/deploy-python/#stop-the-run","title":"Stop the run","text":"<p>To undeploy the model, we can use the <code>stop()</code> method on <code>dstack.Run</code>.</p> <pre><code>run.stop()\n</code></pre>"},{"location":"learn/deploy-python/#retrieve-the-status-of-a-run","title":"Retrieve the status of a run","text":"<p>Note: If you'd like to retrieve the <code>dstack.Run</code> instance by the name of the run, you can use the <code>runs.get()</code> method on <code>dstack.Client</code>.</p> <pre><code>run = client.runs.get(run_name)\n</code></pre> <p>The <code>status</code> property on <code>dstack.Run</code> provides the status of the run.</p> <pre><code>if run:\n    print(run.status)\n</code></pre> <p>To get the latest state of the run, you can use the <code>run.refresh()</code> method:</p> <pre><code>run.refresh()\n</code></pre>"},{"location":"learn/deploy-python/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p> <pre><code>git clone https://github.com/dstackai/dstack-examples\ncd dstack-examples\n</code></pre> <p>Once the repository is cloned, feel free to install the requirements and run the app:</p> <pre><code>pip install -r deploy-python/requirements.txt\nstreamlit run deploy-python/app.py\n</code></pre>"},{"location":"learn/finetuning-llama-2/","title":"Fine-tuning LLMs with QLoRA","text":"<p>NOTE:</p> <p>This example demonstrates how to fine-tune <code>llama-2-7b-chat-hf</code>, with QLoRA and your own script, using Tasks.</p> <p>If you'd like to fine-tune an LLM via a simple API, consider using the Fine-tuning API. It's a lot simpler and  doesn't need your own script.</p>"},{"location":"learn/finetuning-llama-2/#prepare-a-dataset","title":"Prepare a dataset","text":"<p>When selecting a dataset, make sure that it is pre-processed to match the prompt format of Llama 2:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\n</code></pre> <p>In our example, we'll use the <code>mlabonne/guanaco-llama2-1k</code> dataset. It is a 1K sample from the <code>timdettmers/openassistant-guanaco</code> dataset converted to Llama 2's format.</p>"},{"location":"learn/finetuning-llama-2/#define-the-training-script","title":"Define the training script","text":""},{"location":"learn/finetuning-llama-2/#requirements","title":"Requirements","text":"<p>The most notable libraries that we'll use are <code>peft</code> (required for using the QLoRA technique), <code>bitsandbytes</code> (required for using the quantization technique), and <code>trl</code> (required for supervised fine-tuning).</p> <pre><code>accelerate==0.21.0\npeft==0.4.0\nbitsandbytes==0.40.2\ntransformers==4.31.0\ntrl==0.4.7\nscipy\ntensorboard\nsentencepiece\nhf-transfer\n</code></pre>"},{"location":"learn/finetuning-llama-2/#load-the-base-model","title":"Load the base model","text":"<p>In the first part of our script, we prepare the <code>bitsandbytes</code> config and load the base model along with its tokenizer, based on the script arguments.</p> <pre><code>from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\ndef create_and_prepare_model(args):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=args.use_4bit,\n        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n        bnb_4bit_use_double_quant=args.use_nested_quant,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n\n    model.config.use_cache = False\n    model.config.pretraining_tp\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    return model, tokenizer\n</code></pre>"},{"location":"learn/finetuning-llama-2/#create-a-trainer-instance","title":"Create a trainer instance","text":"<p>In the second part of our script, we prepare the <code>peft</code> config and create the trainer based on the script arguments.</p> <pre><code>from peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ndef create_and_prepare_trainer(model, tokenizer, dataset, args):\n    training_arguments = TrainingArguments(\n        output_dir=args.output_dir,\n        num_train_epochs=args.num_train_epochs,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        optim=args.optim,\n        save_steps=args.save_steps,\n        logging_steps=args.logging_steps,\n        learning_rate=args.learning_rate,\n        weight_decay=args.weight_decay,\n        fp16=args.fp16,\n        bf16=args.bf16,\n        max_grad_norm=args.max_grad_norm,\n        max_steps=args.max_steps,\n        warmup_ratio=args.warmup_ratio,\n        group_by_length=args.group_by_length,\n        lr_scheduler_type=args.lr_scheduler_type,\n        report_to=\"tensorboard\",\n    )\n\n    peft_config = LoraConfig(\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        r=args.lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=args.max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=args.packing,\n    )\n\n    return trainer\n</code></pre>"},{"location":"learn/finetuning-llama-2/#publish-the-fine-tined-model","title":"Publish the fine-tined model","text":"<p>In the third part of the script, we merge the base model with the fine-tuned model and push it to the Hugging Face Hub.</p> <pre><code>from peft import PeftModel\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer\n)\n\ndef merge_and_push(args):\n    # Reload model in FP16 and merge it with LoRA weights\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, args.new_model_name)\n    model = model.merge_and_unload()\n\n    # Reload the new tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name, trust_remote_code=True\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    # Publish the new model to Hugging Face Hub\n    model.push_to_hub(args.new_model_name, use_temp_dir=False)\n    tokenizer.push_to_hub(args.new_model_name, use_temp_dir=False)\n</code></pre>"},{"location":"learn/finetuning-llama-2/#put-it-all-together","title":"Put it all together","text":"<p>Finally, in the main part of the script, we put it all together.</p> <pre><code>from dataclasses import dataclass\nfrom datasets import load_dataset\nfrom transformers import HfArgumentParser\n\n@dataclass\nclass ScriptArguments:\n    # ...\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n\n    dataset = load_dataset(args.dataset_name, split=\"train\")\n\n    model, tokenizer = create_and_prepare_model(args)\n\n    trainer = create_and_prepare_trainer(model, tokenizer, dataset, args)\n\n    trainer.train()\n    trainer.model.save_pretrained(args.new_model_name)\n\n    if args.merge_and_push:\n        merge_and_push(args)\n</code></pre>"},{"location":"learn/finetuning-llama-2/#define-the-configuration","title":"Define the configuration","text":"<p>Here's the configuration that runs the training task via <code>dstack</code>:</p> <pre><code>type: task\n\n# (Optional) When not specified, your local Python version is used\npython: \"3.11\"\n\nenv: \n  - HF_HUB_ENABLE_HF_TRANSFER=1\n  # (Required) Specify your Hugging Face token to publish the fine-tuned model\n  - HUGGING_FACE_HUB_TOKEN=\n\nports:\n  - 6006\n\ncommands:\n  - echo \"Installing requirements...\"\n  - pip -q install -r llama-2/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python llama-2/train.py --merge_and_push ${{ run.args }}\n</code></pre>"},{"location":"learn/finetuning-llama-2/#run-the-configuration","title":"Run the configuration","text":"<p>Here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f llama-2/train.dstack.yml --gpu 16GB --num_train_epochs 10\n\nInstalling requirements...\nTensorBoard 2.14.0 at http://127.0.0.1:6006/ (Press CTRL+C to quit)\n{'loss': 1.3491, 'learning_rate': 0.0002, 'epoch': 0.1}\n{'loss': 1.6299, 'learning_rate': 0.0002, 'epoch': 0.2}\n{'loss': 1.2071, 'learning_rate': 0.0002, 'epoch': 0.3}\n</code></pre> <p><code>dstack</code> will provision the cloud instance corresponding to the configured project and profile, run the training, and tear down the cloud instance once the training is complete.</p> Tensorboard <p>Since we've executed <code>tensorboard</code> within our task and configured its port using <code>ports</code>, you can access it using the URL provided in the output. <code>dstack</code> automatically forwards the configured port to your local machine.</p> <p></p> <p>Source code</p> <p>The complete and ready-to-run code for the example is available in our GitHub repo.</p>"},{"location":"learn/llama-index-weaviate/","title":"RAG with Llama Index and Weaviate","text":"<p>RAG, or retrieval-augmented generation, empowers LLMs by providing them with access to your data.</p> <p>Here's an example of how to apply this technique using the Llama Index framework  and Weaviate vector database.</p>"},{"location":"learn/llama-index-weaviate/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Llama Index loads data from local files, structures it into chunks, and ingests it into Weaviate (an open-source vector database).    We set up Llama Index to use local embeddings through the SentenceTransformers library.</li> <li><code>dstack</code> allows us to deploy LLMs to any cloud provider, e.g.    via the text generation API. Alternatively, you can also deploy using TGI's     docker image (as a task or a service).</li> <li>Llama Index allows us to prompt the LLM automatically incorporating the context from Weaviate. </li> </ol>"},{"location":"learn/llama-index-weaviate/#requirements","title":"Requirements","text":"<p>Here's the list of Python libraries that we'll use:</p> <pre><code>weaviate-client\nllama-index\nsentence-transformers\ntext_generation\n</code></pre>"},{"location":"learn/llama-index-weaviate/#load-data-to-weaviate","title":"Load data to Weaviate","text":"<p>The first thing we do is load the data from local files and ingest it into Weaviate.</p> <p>NOTE:</p> <p>To use Weaviate, you need to either install  it on-premises or sign up for their managed service.</p> <p>Since we're going to load data into or from Weaviate, we'll need a <code>weaviate.Client</code>:</p> <pre><code>import os\n\nimport weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_TOKEN\"))\n\nclient = weaviate.Client(url=os.getenv(\"WEAVIATE_URL\"), auth_client_secret=auth_config)\n\nclient.schema.delete_class(\"DstackExample\")\n</code></pre> <p>Next, prepare the Llama Index classes: <code>llama_index.ServiceContext</code> (for indexing and querying) and <code>llama_index.StorageContext</code> (for loading and storing). </p> <p>Embeddings</p> <p>Note that we're using <code>langchain.embeddings.huggingface.HuggingFaceEmbeddings</code> for local embeddings instead of OpenAI.</p> <pre><code>from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index import (\n    LangchainEmbedding,\n    ServiceContext,\n    StorageContext,\n)\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n</code></pre> <p>Once the utility classes are configured, we can load the data from local files and pass it to <code>llama_index.VectorStoreIndex</code>. Using its <code>from_documents</code> method will then store the data in the vector database.</p> <pre><code>from pathlib import Path\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\n\ndocuments = SimpleDirectoryReader(Path(__file__).parent / \"data\").load_data()\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    service_context=service_context,\n    storage_context=storage_context,\n)\n</code></pre> <p>The data is in the vector database! Now we can proceed with the part where we invoke an LLM using this data as context.</p>"},{"location":"learn/llama-index-weaviate/#deploy-an-llm","title":"Deploy an LLM","text":"<p>This example assumes we're using an LLM deployed via the text generation API, or using TGI.</p> <p>Once you deployed the model, make sure to set the <code>TGI_ENDPOINT_URL</code> environment variable  to its URL, e.g. <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (or <code>http://localhost:&lt;port&gt;</code> if it's deployed  as a task). We'll use this environment variable below.</p> <pre><code>$ curl -X POST --location $TGI_ENDPOINT_URL/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n          \"inputs\": \"What is Deep Learning?\",\n          \"parameters\": {\n            \"max_new_tokens\": 20\n          }\n        }'\n</code></pre>"},{"location":"learn/llama-index-weaviate/#generate-response","title":"Generate response","text":"<p>Once the LLM endpoint is up, we can prompt it through Llama Index to automatically incorporate context from Weaviate.</p> <p>Since we'll invoke the actual LLM, when configuring <code>llama_index.ServiceContext</code>, we must include the LLM configuration.</p> <pre><code>import os\n\nfrom llama_index import (\n    LangchainEmbedding,\n    PromptHelper,\n    ServiceContext,\n    VectorStoreIndex,\n)\n\nfrom langchain import HuggingFaceTextGenInference\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index.llm_predictor import LLMPredictor\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nllm_predictor = LLMPredictor(\n    llm=HuggingFaceTextGenInference(\n        inference_server_url=os.getenv(\"TGI_ENDPOINT_URL\"),\n        max_new_tokens=512,\n        streaming=True,\n    ),\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model,\n    llm_predictor=llm_predictor,\n    prompt_helper=PromptHelper(context_window=1024),\n)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nindex = VectorStoreIndex.from_vector_store(\n    vector_store, service_context=service_context\n)\n</code></pre> <p>Once <code>llama_index.VectorStoreIndex</code> is ready, we can proceed with querying it.</p> <p>Prompt format</p> <p>If we're deploying Llama 2, we have to ensure that the prompt format is correct.</p> <pre><code>from llama_index import (QuestionAnswerPrompt, RefinePrompt)\n\ntext_qa_template = QuestionAnswerPrompt(\n        \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nWe have provided context information below. \n\n{context_str}\n\nGiven this information, please answer the question.\n&lt;&lt;/SYS&gt;&gt;\n\n{query_str} [/INST]\"\"\"\n    )\n\nrefine_template = RefinePrompt(\n    \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nThe original query is as follows: \n\n{query_str}\n\nWe have provided an existing answer:\n\n{existing_answer}\n\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\n\n{context_msg}\n&lt;&lt;/SYS&gt;&gt;\n\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. [/INST]\"\"\"\n)\n\nquery_engine = index.as_query_engine(\n    text_qa_template=text_qa_template,\n    refine_template=refine_template,\n    streaming=True,\n)\n\nresponse = query_engine.query(\"Make a bullet-point timeline of the authors biography?\")\nresponse.print_response_stream()\n</code></pre> <p>That's it! This basic example shows how straightforward it is to use Llama Index and Weaviate with the LLMs deployed using <code>dstack</code>. For more in-depth information, we encourage you to explore the documentation for each tool.</p>"},{"location":"learn/llama-index-weaviate/#source-code","title":"Source code","text":"<p>Source code</p> <p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"learn/llmchat/","title":"LLM as Chatbot","text":"<p>This example is built by Chansung Park. It can run any open-source LLM either as a Gradio chat app or as a Discord bot. With <code>dstack</code>, you can run this Gradio chat app or Discord bot in any cloud with a single command. To try this example with <code>dstack</code>, follow the instructions below.</p>"},{"location":"learn/llmchat/#define-a-profile","title":"Define a profile","text":"<p>Each LLM model requires specific resources. To inform <code>dstack</code> about the required resources, you need to  define a profile via the <code>.dstack/profiles.yaml</code> file within your project.</p> <p>Below is a profile that will provision a cloud instance with <code>24GB</code> of memory and a <code>T4</code> GPU in the <code>gcp</code> project.</p> <pre><code>profiles:\n  - name: t4-serve\n\n    resources:\n      memory: 24GB\n      gpu:\n        name: T4\n\n    spot_policy: auto # (Optional) Use spot instances if available\n\n    default: true\n</code></pre>"},{"location":"learn/llmchat/#run-the-app","title":"Run the app","text":"<p>Here's the configuration that runs the Gradio app:</p> <pre><code>type: task\n\nenv:\n  # (Optional) Specify your Hugging Face token\n  - HUGGING_FACE_HUB_TOKEN=\n  # (Optional) Specify your Serper API Key\n  - LLMCHAT_SERPER_API_KEY=\n\nports:\n  - 6006\n\ncommands:\n  - pip install -r requirements.txt\n  - LLMCHAT_APP_MODE=GRADIO python entry_point.py\n</code></pre> <p>Here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f gradio.dstack.yml\n</code></pre> <p><code>dstack</code> will provision the cloud instance, run the task, and forward the defined ports to your local machine for secure and convenient access.</p> <p></p>"},{"location":"learn/llmchat/#run-the-discord-bot","title":"Run the Discord bot","text":"<p>Here's the configuration that runs the Gradio app:</p> <pre><code>type: task\n\nenv:\n  # (Required) Specify your Discord bot token.\n  - DISCORD_BOT_TOKEN=\n  # (Required) Specify the name of the model. See `README.md`` for supported models.\n  - DISCORD_BOT_MODEL_NAME=alpaca-lora-7b\n  # (Optional) Specify your Hugging Face token\n  - HUGGING_FACE_HUB_TOKEN=\n  # (Optional) Specify your Serper API Key to enable Internet search support.\n  - LLMCHAT_SERPER_API_KEY=\n\ncommands:\n  - pip install -r requirements.txt --progress-bar off\n  - LLMCHAT_APP_MODE=DISCORD python entry_point.py\n</code></pre> How to acquire a Discord bot token <p>Before running, ensure you have specified your Discord bot token, which you can obtain from the Discord Developer Portal. If you haven't set up a Discord Bot on the portal yet,  follow the How to Create a Discord Bot Account  section of the tutorial from freeCodeCamp.</p> <p>Finally, here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f discord.dstack.yml\n</code></pre> <p>Once you confirm, <code>dstack</code> will provision the cloud instance and run the task. Once it's up, you can freely send messages to your bot via Discord.</p> <p></p> <p>For advanced commands supported by the bot, check the README file.</p> <p>Source code</p>"},{"location":"learn/stable-diffusion-xl/","title":"Deploying SDXL with FastAPI","text":"<p>Stable Diffusion XL (SDXL) 1.0 is the latest version of the open-source model that is capable  of generating high-quality images from text.</p> <p>The example below demonstrates how to use <code>dstack</code> to serve SDXL as a REST endpoint in a cloud of your choice for image generation and refinement.</p>"},{"location":"learn/stable-diffusion-xl/#define-endpoints","title":"Define endpoints","text":""},{"location":"learn/stable-diffusion-xl/#requirements","title":"Requirements","text":"<p>Here's the list of libraries that our example will require:</p> <pre><code>transformers\naccelerate\nsafetensors\ndiffusers\ninvisible-watermark&gt;=0.2.0\nopencv-python-headless\nfastapi\nuvicorn\n</code></pre> <p>Let's walk through the code of the example.</p>"},{"location":"learn/stable-diffusion-xl/#load-the-model","title":"Load the model","text":"<p>First of all, let's load the base SDXL model using the <code>diffusers</code> library.</p> <pre><code>from diffusers import StableDiffusionXLPipeline\nimport torch\n\n\nbase = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n)\nbase.to(\"cuda\")\n</code></pre>"},{"location":"learn/stable-diffusion-xl/#define-the-generate-endpoint","title":"Define the generate endpoint","text":"<p>Now that the model is loaded, let's define the FastAPI app and the <code>/generate</code> REST endpoint that will accept a prompt and generate an image.</p> <pre><code>import uuid\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = None\n    width: Optional[int] = None\n    height: Optional[str] = None\n\n\nclass ImageResponse(BaseModel):\n    id: str\n\n\nimages_dir = Path(\"images\")\nimages_dir.mkdir(exist_ok=True)\n\n\n@app.post(\"/generate\")\nasync def generate(request: GenerateRequest):\n    image = base(\n        prompt=request.prompt,\n        negative_prompt=request.negative_prompt,\n        width=request.width,\n        height=request.height,\n    ).images[0]\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre>"},{"location":"learn/stable-diffusion-xl/#define-the-download-endpoint","title":"Define the download endpoint","text":"<p>Notice that the endpoint only returns the ID of the image. To download images by ID, we'll define another endpoint:</p> <pre><code>from fastapi.responses import FileResponse\n\n\n@app.get(\"/download/{id}\")\ndef download(id: str):\n    filename = f\"{id}.png\"\n    return FileResponse(\n        images_dir / filename, media_type=\"image/png\", filename=filename\n    )\n</code></pre> <p>That's it. Once we run the application, we can already utilize the <code>/generate</code> and <code>/download</code> endpoints.</p>"},{"location":"learn/stable-diffusion-xl/#define-the-refine-endpoint","title":"Define the refine endpoint","text":"<p>Since SDXL allows refining images, let's define the refine endpoint to accept the image ID and the refinement prompt.</p> <pre><code>import asyncio\n\nimport PIL\n\n\nclass RefineRequest(BaseModel):\n    id: str\n    prompt: str\n\n\nrefiner = None\nrefiner_lock = asyncio.Lock()\n\n\n@app.post(\"/refine\")\nasync def refine(request: RefineRequest):\n    await refiner_lock.acquire()\n    global refiner\n    if refiner is None:\n        refiner = DiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n            text_encoder_2=base.text_encoder_2,\n            vae=base.vae,\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n            variant=\"fp16\",\n        )\n        refiner.to(\"cuda\")\n    refiner_lock.release()\n\n    image = refiner(\n        prompt=request.prompt,\n        image=PIL.Image.open(images_dir / f\"{request.id}.png\"),\n    ).images[0]\n\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre> <p>The code for the endpoints is ready. Now, let's explore how to use <code>dstack</code> to serve it on a cloud account of your choice.</p>"},{"location":"learn/stable-diffusion-xl/#define-the-configuration","title":"Define the configuration","text":"Tasks <p>If you want to serve an application for development purposes only, you can use  tasks.  In this scenario, while the application runs in the cloud,  it is accessible from your local machine only.</p> <p>For production purposes, the optimal approach to serve an application is by using  services. In this case, the application can be accessed through a public endpoint.</p> <p>Here's the configuration that uses services:</p> <pre><code>type: service\n\n# (Optional) If not specified, it will use your local version\npython: \"3.11\"\n\nport: 8000\n\ncommands: \n  - apt-get update \n  - apt-get install libgl1 -y\n  - pip install -r stable-diffusion-xl/requirements.txt\n  - uvicorn stable-diffusion-xl.main:app --port 8000\n</code></pre>"},{"location":"learn/stable-diffusion-xl/#run-the-configuration","title":"Run the configuration","text":"<p>NOTE:</p> <p>Before running a service, ensure that you have configured a gateway.</p> <p>After the gateway is configured, go ahead run the service.</p> <pre><code>$ dstack run . -f stable-diffusion-xl/api.dstack.yml\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query the endpoint:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{ \"prompt\": \"A cat in a hat\" }'\n</code></pre> <p>Source code</p> <p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"learn/text-generation-inference/","title":"Deploying LLMs with TGI","text":"<p>NOTE:</p> <p>This example demonstrates how to deploy an LLM using Services and TGI, an open-source framework by Hugging Face.</p> <p>If you'd like to deploy an LLM via a simple API, consider using the Text generation API. It's a lot simpler.</p>"},{"location":"learn/text-generation-inference/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using TGI, you have to define the following configuration file:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=NousResearch/Llama-2-7b-hf\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre>"},{"location":"learn/text-generation-inference/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway.</p> <pre><code>$ dstack run . -f text-generation-inference/serve.dstack.yml --gpu 24GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query it:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n          \"inputs\": \"What is Deep Learning?\",\n          \"parameters\": {\n            \"max_new_tokens\": 20\n          }\n        }'\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f text-generation-inference/serve.dstack.yml --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; --gpu 24GB\n</code></pre> </p>"},{"location":"learn/text-generation-inference/#quantization","title":"Quantization","text":"<p>An LLM typically requires twice the GPU memory compared to its parameter count. For instance, a model with <code>13B</code> parameters needs around <code>26GB</code> of GPU memory. To decrease memory usage and fit the model on a smaller GPU, consider using quantization, which TGI offers as <code>bitsandbytes</code> and <code>gptq</code> methods. </p> <p>Here's an example of the Llama 2 13B model tailored for a <code>24GB</code> GPU (A10 or L4):</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-GPTQ\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code --quantize gptq\n</code></pre> <p>A similar approach allows running the Llama 2 70B model on an <code>40GB</code> GPU (A100).</p> <p>To calculate the exact GPU memory required for a specific model with different quantization methods, you can use the hf-accelerate/memory-model-usage Space.</p> <p>Source code</p> <p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"learn/vllm/","title":"Deploying LLMs with vLLM","text":"<p>NOTE:</p> <p>This example demonstrates how to deploy an LLM using Services and vLLM, an open-source library.</p> <p>If you'd like to deploy an LLM via a simple API, consider using the Text generation API. It's a lot simpler.</p>"},{"location":"learn/vllm/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using vLLM, you have to define the following configuration file:</p> <pre><code>type: service\n\npython: \"3.11\"\n\nenv:\n  - MODEL=NousResearch/Llama-2-7b-hf\n\nport: 8000\n\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\n</code></pre>"},{"location":"learn/vllm/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway.</p> <pre><code>$ dstack run . -f vllm/serve.dstack.yml --gpu 24GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query it:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"model\": \"NousResearch/Llama-2-7b-hf\",\n          \"prompt\": \"San Francisco is a\",\n          \"max_tokens\": 7,\n          \"temperature\": 0\n        }'\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f vllm/serve.dstack.yml --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; --gpu 24GB\n</code></pre> </p> <p>Source code</p> <p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/releases/","title":"Releases","text":""}]}