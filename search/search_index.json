{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"terms/","title":"Terms of service","text":""},{"location":"terms/#agreement-to-terms","title":"Agreement to terms","text":"<p>We are dstack GmbH (\"Company,\" \"we,\" \"us,\" \"our\"), a company registered in Germany at Franz-Joseph-Stra\u00dfe, 11, Munich, Bayern 80801.</p> <p>These Legal Terms constitute a legally binding agreement made between you, whether personally or on behalf of an entity (\"you\"), and dstack GmbH, concerning your access to and use of the Services. You agree that by accessing the Services, you have read, understood, and agreed to be bound by all of these Legal Terms. IF YOU DO NOT AGREE WITH ALL OF THESE LEGAL TERMS, THEN YOU ARE EXPRESSLY PROHIBITED FROM USING THE SERVICES AND YOU MUST DISCONTINUE USE IMMEDIATELY.</p> <p>Supplemental terms and conditions or documents that may be posted on the Services from time to time are hereby expressly incorporated herein by reference. We reserve the right, in our sole discretion, to make changes or modifications to these Legal Terms from time to time. We will alert you about any changes by updating the \"Last updated\" date of these Legal Terms, and you waive any right to receive specific notice of each such change. It is your responsibility to periodically review these Legal Terms to stay informed of updates. You will be subject to, and will be deemed to have been made aware of and to have accepted, the changes in any revised Legal Terms by your continued use of the Services after the date such revised Legal Terms are posted.</p>"},{"location":"terms/#1-our-services","title":"1. Our services","text":"<p>The information provided when using the Services is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country. Accordingly, those persons who choose to access the Services from other locations do so on their own initiative and are solely responsible for compliance with local laws, if and to the extent local laws are applicable.</p> <p>The Services are not tailored to comply with industry-specific regulations (Health Insurance Portability and Accountability Act (HIPAA), Federal Information Security Management Act (FISMA), etc.), so if your interactions would be subjected to such laws, you may not use the Services. You may not use the Services in a way that would violate the Gramm-Leach-Bliley Act (GLBA).</p>"},{"location":"terms/#2-intelliectual-property-rights","title":"2. Intelliectual property rights","text":"<p>Our intellectual property</p> <p>We are the owner or the licensee of all intellectual property rights in our Services, including all source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics in the Services ( collectively, the \"Content\"), as well as the trademarks, service marks, and logos contained therein (the \"Marks\").</p> <p>Our Content and Marks are protected by copyright and trademark laws (and various other intellectual property rights and unfair competition laws) and treaties in the United States and around the world.</p> <p>The Content and Marks are provided in or through the Services \"AS IS\" for your personal, non-commercial use or internal business purpose only.</p> <p>Your use of our Services</p> <p>Subject to your compliance with these Legal Terms, including the \"Prohibited activities\" section below, we grant you a non-exclusive, non-transferable, revocable license to:</p> <ul> <li>access the Services; and</li> <li>download or print a copy of any portion of the Content to which you have properly gained access. solely for your personal, non-commercial use or internal business purpose.</li> </ul> <p>Except as set out in this section or elsewhere in our Legal Terms, no part of the Services and no Content or Marks may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>If you wish to make any use of the Services, Content, or Marks other than as set out in this section or elsewhere in our Legal Terms, please address your request to: hello@dstack.ai. If we ever grant you the permission to post, reproduce, or publicly display any part of our Services or Content, you must identify us as the owners or licensors of the Services, Content, or Marks and ensure that any copyright or proprietary notice appears or is visible on posting, reproducing, or displaying our Content.</p> <p>We reserve all rights not expressly granted to you in and to the Services, Content, and Marks.</p> <p>Any breach of these Intellectual Property Rights will constitute a material breach of our Legal Terms and your right to use our Services will terminate immediately.</p> <p>Your submissions</p> <p>Please review this section and the \"Prohibited activities\" section carefully prior to using our Services to understand the (a) rights you give us and (b) obligations you have when you post or upload any content through the Services.</p> <p>Submissions: By directly sending us any question, comment, suggestion, idea, feedback, or other information about the Services (\"Submissions\"), you agree to assign to us all intellectual property rights in such Submission. You agree that we shall own this Submission and be entitled to its unrestricted use and dissemination for any lawful purpose, commercial or otherwise, without acknowledgment or compensation to you.</p> <p>You are responsible for what you post or upload: By sending us Submissions through any part of the Services you: * confirm that you have read and agree with our \"Prohibited activities\" and will not post, send, publish, upload, or * transmit through the Services any Submission that is illegal, harassing, hateful, harmful, defamatory, obscene, * bullying, abusive, discriminatory, threatening to any person or group, sexually explicit, false, inaccurate, deceitful,   or misleading; * to the extent permissible by applicable law, waive any and all moral rights to any such Submission; * warrant that any such Submission are original to you or that you have the necessary rights and licenses to submit such    Submissions and that you have full authority to grant us the above-mentioned rights in relation to your Submissions; and * warrant and represent that your Submissions do not constitute confidential information.</p> <p>You are solely responsible for your Submissions and you expressly agree to reimburse us for any and all losses that we may suffer because of your breach of (a) this section, (b) any third party\u2019s intellectual property rights, or (c) applicable law.</p>"},{"location":"terms/#3-user-representations","title":"3. User representations","text":"<p>By using the Services, you represent and warrant that: (1) all registration information you submit will be true, accurate, current, and complete; (2) you will maintain the accuracy of such information and promptly update such registration information as necessary; (3) you have the legal capacity and you agree to comply with these Legal Terms; ( 4) you are not a minor in the jurisdiction in which you reside; (5) you will not access the Services through automated or non-human means, whether through a bot, script or otherwise; (6) you will not use the Services for any illegal or unauthorized purpose; and (7) your use of the Services will not violate any applicable law or regulation.</p> <p>If you provide any information that is untrue, inaccurate, not current, or incomplete, we have the right to suspend or terminate your account and refuse any and all current or future use of the Services (or any portion thereof).</p>"},{"location":"terms/#4-user-registration","title":"4. User registration","text":"<p>You may be required to register to use the Services. You agree to keep your password confidential and will be responsible for all use of your account and password. We reserve the right to remove, reclaim, or change a username you select if we determine, in our sole discretion, that such username is inappropriate, obscene, or otherwise objectionable.</p>"},{"location":"terms/#5-purchases-and-payment","title":"5. Purchases and payment","text":"<p>We accept the following forms of payment:</p> <ul> <li>Visa</li> <li>Mastercard</li> </ul> <p>You agree to provide current, complete, and accurate purchase and account information for all purchases made via the Services. You further agree to promptly update account and payment information, including email address, payment method, and payment card expiration date, so that we can complete your transactions and contact you as needed. Sales tax will be added to the price of purchases as deemed required by us. We may change prices at any time. All payments shall be in US dollars.</p> <p>You agree to pay all charges at the prices then in effect for your purchases and any applicable shipping fees, and you authorize us to charge your chosen payment provider for any such amounts upon placing your order. We reserve the right to correct any errors or mistakes in pricing, even if we have already requested or received payment.</p> <p>We reserve the right to refuse any order placed through the Services. We may, in our sole discretion, limit or cancel quantities purchased per person, per household, or per order. These restrictions may include orders placed by or under the same customer account, the same payment method, and/or orders that use the same billing or shipping address. We reserve the right to limit or prohibit orders that, in our sole judgment, appear to be placed by dealers, resellers, or distributors.</p>"},{"location":"terms/#6-subscriptions","title":"6. Subscriptions","text":"<p>Billing and Renewal</p> <p>e.g. by topping up their balance manually using their credit card.</p> <p>Cancellation</p> <p>You can cancel your subscription at any time by contacting us using the contact information provided below. Your cancellation will take effect at the end of the current paid term. If you have any questions or are unsatisfied with our Services, please email us at hello@dstack.ai .</p> <p>Fee Changes</p> <p>We may, from time to time, make changes to the subscription fee and will communicate any price changes to you in accordance with applicable law.</p>"},{"location":"terms/#7-software","title":"7. Software","text":"<p>We may include software for use in connection with our Services. If such software is accompanied by an end user license agreement (\"EULA\"), the terms of the EULA will govern your use of the software. If such software is not accompanied by a EULA, then we grant to you a non-exclusive, revocable, personal, and non-transferable license to use such software solely in connection with our services and in accordance with these Legal Terms. Any software and any related documentation is provided \"AS IS\" without warranty of any kind, either express or implied, including, without limitation, the implied warranties of merchantability, fitness for a particular purpose, or non-infringement. You accept any and all risk arising out of use or performance of any software. You may not reproduce or redistribute any software except in accordance with the EULA or these Legal Terms.</p>"},{"location":"terms/#8-prohibited-activities","title":"8. Prohibited activities","text":"<p>You may not access or use the Services for any purpose other than that for which we make the Services available. The Services may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us.</p> <p>As a user of the Services, you agree not to:</p> <ul> <li>Systematically retrieve data or other content from the Services to create or compile, directly or indirectly, a   collection, compilation, database, or directory without written permission from us.</li> <li>Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such   as user passwords.</li> <li>Circumvent, disable, or otherwise interfere with security-related features of the Services, including features that   prevent or restrict the use or copying of any Content or enforce limitations on the use of the Services and/or the   Content contained therein.</li> <li>Disparage, tarnish, or otherwise harm, in our opinion, us and/or the Services.</li> <li>Use any information obtained from the Services in order to harass, abuse, or harm another person.</li> <li>Make improper use of our support services or submit false reports of abuse or misconduct.</li> <li>Use the Services in a manner inconsistent with any applicable laws or regulations.</li> <li>Engage in unauthorized framing of or linking to the Services.</li> <li>Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material, including   excessive use of capital letters and spamming (continuous posting of repetitive text), that interferes with any   party\u2019s uninterrupted use and enjoyment of the Services or modifies, impairs, disrupts, alters, or interferes with the   use, features, functions, operation, or maintenance of the Services.</li> <li>Engage in any automated use of the system, such as using scripts to send comments or messages, or using any data   mining, robots, or similar data gathering and extraction tools.</li> <li>Delete the copyright or other proprietary rights notice from any Content.</li> <li>Attempt to impersonate another user or person or use the username of another user.</li> <li>Upload or transmit (or attempt to upload or to transmit) any material that acts as a passive or active information   collection or transmission mechanism, including without limitation, clear graphics interchange formats (\"gifs\"), 1\u00d71   pixels, web bugs, cookies, or other similar devices (sometimes referred to as \"spyware\" or \"passive collection   mechanisms\" or \"pcms\").</li> <li>Interfere with, disrupt, or create an undue burden on the Services or the networks or services connected to the   Services.</li> <li>Harass, annoy, intimidate, or threaten any of our employees or agents engaged in providing any portion of the Services   to you.</li> <li>Attempt to bypass any measures of the Services designed to prevent or restrict access to the Services, or any portion   of the Services.</li> <li>Copy or adapt the Services' software, including but not limited to Flash, PHP, HTML, JavaScript, or other code.</li> <li>Except as permitted by applicable law, decipher, decompile, disassemble, or reverse engineer any of the software   comprising or in any way making up a part of the Services.</li> <li>Except as may be the result of standard search engine or Internet browser usage, use, launch, develop, or distribute   any automated system, including without limitation, any spider, robot, cheat utility, scraper, or offline reader that   accesses the Services, or use or launch any unauthorized script or other software.</li> <li>Use a buying agent or purchasing agent to make purchases on the Services.</li> <li>Make any unauthorized use of the Services, including collecting usernames and/or email addresses of users by   electronic or other means for the purpose of sending unsolicited email, or creating user accounts by automated means   or under false pretenses.</li> <li>Use the Services as part of any effort to compete with us or otherwise use the Services and/or the Content for any   revenue-generating endeavor or commercial enterprise.</li> </ul>"},{"location":"terms/#9-user-generated-contributions","title":"9. User generated contributions","text":"<p>The Services does not offer users to submit or post content.</p>"},{"location":"terms/#10-contribution-license","title":"10. Contribution license","text":"<p>You and Services agree that we may access, store, process, and use any information and personal data that you provide following the terms of the Privacy Policy and your choices (including settings).</p> <p>By submitting suggestions or other feedback regarding the Services, you agree that we can use and share such feedback for any purpose without compensation to you.</p>"},{"location":"terms/#11-social-media","title":"11. Social media","text":"<p>As part of the functionality of the Services, you may link your account with online accounts you have with third-party service providers (each such account, a \"Third-Party Account\") by either: (1) providing your Third-Party Account login information through the Services; or (2) allowing us to access your Third-Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third-Party Account. You represent and warrant that you are entitled to disclose your Third-Party Account login information to us and/or grant us access to your Third-Party Account, without breach by you of any of the terms and conditions that govern your use of the applicable Third-Party Account, and without obligating us to pay any fees or making us subject to any usage limitations imposed by the third-party service provider of the Third-Party Account. By granting us access to any Third-Party Accounts, you understand that (1) we may access, make available, and store (if applicable) any content that you have provided to and stored in your Third-Party Account (the \"Social Network Content\") so that it is available on and through the Services via your account, including without limitation any friend lists and (2) we may submit to and receive from your Third-Party Account additional information to the extent you are notified when you link your account with the Third-Party Account. Depending on the Third-Party Accounts you choose and subject to the privacy settings that you have set in such Third-Party Accounts, personally identifiable information that you post to your Third-Party Accounts may be available on and through your account on the Services. Please note that if a Third-Party Account or associated service becomes unavailable or our access to such Third-Party Account is terminated by the third-party service provider, then Social Network Content may no longer be available on and through the Services. You will have the ability to disable the connection between your account on the Services and your Third-Party Accounts at any time. PLEASE NOTE THAT YOUR RELATIONSHIP WITH THE THIRD-PARTY SERVICE PROVIDERS ASSOCIATED WITH YOUR THIRD-PARTY ACCOUNTS IS GOVERNED SOLELY BY YOUR AGREEMENT(S) WITH SUCH THIRD-PARTY SERVICE PROVIDERS. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality, or non-infringement, and we are not responsible for any Social Network Content. You acknowledge and agree that we may access your email address book associated with a Third-Party Account and your contacts list stored on your mobile device or tablet computer solely for purposes of identifying and informing you of those contacts who have also registered to use the Services. You can deactivate the connection between the Services and your Third-Party Account by contacting us using the contact information below or through your account settings (if applicable). We will attempt to delete any information stored on our servers that was obtained through such Third-Party Account, except the username and profile picture that become associated with your account.</p>"},{"location":"terms/#12-third-party-websites-and-content","title":"12. Third-party websites and content","text":"<p>The Services may contain (or you may be sent via the Site) links to other websites (\"Third-Party Websites\") as well as articles, photographs, text, graphics, pictures, designs, music, sound, video, information, applications, software, and other content or items belonging to or originating from third parties (\"Third-Party Content\"). Such Third-Party Websites and Third-Party Content are not investigated, monitored, or checked for accuracy, appropriateness, or completeness by us, and we are not responsible for any Third-Party Websites accessed through the Services or any Third-Party Content posted on, available through, or installed from the Services, including the content, accuracy, offensiveness, opinions, reliability, privacy practices, or other policies of or contained in the Third-Party Websites or the Third-Party Content. Inclusion of, linking to, or permitting the use or installation of any Third-Party Websites or any Third-Party Content does not imply approval or endorsement thereof by us. If you decide to leave the Services and access the Third-Party Websites or to use or install any Third-Party Content, you do so at your own risk, and you should be aware these Legal Terms no longer govern. You should review the applicable terms and policies, including privacy and data gathering practices, of any website to which you navigate from the Services or relating to any applications you use or install from the Services. Any purchases you make through Third-Party Websites will be through other websites and from other companies, and we take no responsibility whatsoever in relation to such purchases which are exclusively between you and the applicable third party. You agree and acknowledge that we do not endorse the products or services offered on Third-Party Websites and you shall hold us blameless from any harm caused by your purchase of such products or services. Additionally, you shall hold us blameless from any losses sustained by you or harm caused to you relating to or resulting in any way from any Third-Party Content or any contact with Third-Party Websites.</p>"},{"location":"terms/#13-services-management","title":"13. Services management","text":"<p>We reserve the right, but not the obligation, to: (1) monitor the Services for violations of these Legal Terms; (2) take appropriate legal action against anyone who, in our sole discretion, violates the law or these Legal Terms, including without limitation, reporting such user to law enforcement authorities; (3) in our sole discretion and without limitation, refuse, restrict access to, limit the availability of, or disable (to the extent technologically feasible) any of your Contributions or any portion thereof; (4) in our sole discretion and without limitation, notice, or liability, to remove from the Services or otherwise disable all files and content that are excessive in size or are in any way burdensome to our systems; and (5) otherwise manage the Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Services.</p>"},{"location":"terms/#14-privacy-policy","title":"14. Privacy policy","text":"<p>We care about data privacy and security. Please review our Privacy Policy. By using the Services, you agree to be bound by our Privacy Policy, which is incorporated into these Legal Terms. Please be advised the Services are hosted in Germany and United States. If you access the Services from any other region of the world with laws or other requirements governing personal data collection, use, or disclosure that differ from applicable laws in Germany and United States, then through your continued use of the Services, you are transferring your data to Germany and United States, and you expressly consent to have your data transferred to and processed in Germany and United States.</p>"},{"location":"terms/#15-term-and-termination","title":"15. Term and termination","text":"<p>These Legal Terms shall remain in full force and effect while you use the Services. WITHOUT LIMITING ANY OTHER PROVISION OF THESE LEGAL TERMS, WE RESERVE THE RIGHT TO, IN OUR SOLE DISCRETION AND WITHOUT NOTICE OR LIABILITY, DENY ACCESS TO AND USE OF THE SERVICES (INCLUDING BLOCKING CERTAIN IP ADDRESSES), TO ANY PERSON FOR ANY REASON OR FOR NO REASON, INCLUDING WITHOUT LIMITATION FOR BREACH OF ANY REPRESENTATION, WARRANTY, OR COVENANT CONTAINED IN THESE LEGAL TERMS OR OF ANY APPLICABLE LAW OR REGULATION. WE MAY TERMINATE YOUR USE OR PARTICIPATION IN THE SERVICES OR DELETE YOUR ACCOUNT AND ANY CONTENT OR INFORMATION THAT YOU POSTED AT ANY TIME, WITHOUT WARNING, IN OUR SOLE DISCRETION.</p> <p>If we terminate or suspend your account for any reason, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p>"},{"location":"terms/#16-modifications-and-interruptions","title":"16. Modifications and interruptions","text":"<p>We reserve the right to change, modify, or remove the contents of the Services at any time or for any reason at our sole discretion without notice. However, we have no obligation to update any information on our Services. We will not be liable to you or any third party for any modification, price change, suspension, or discontinuance of the Services.</p> <p>We cannot guarantee the Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Services, resulting in interruptions, delays, or errors. We reserve the right to change, revise, update, suspend, discontinue, or otherwise modify the Services at any time or for any reason without notice to you. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Services during any downtime or discontinuance of the Services. Nothing in these Legal Terms will be construed to obligate us to maintain and support the Services or to supply any corrections, updates, or releases in connection therewith.</p>"},{"location":"terms/#17-governing-law","title":"17. Governing law","text":"<p>These Legal Terms are governed by and interpreted following the laws of Germany, and the use of the United Nations Convention of Contracts for the International Sales of Goods is expressly excluded. If your habitual residence is in the EU, and you are a consumer, you additionally possess the protection provided to you by obligatory provisions of the law in your country to residence. dstack GmbH and yourself both agree to submit to the non-exclusive jurisdiction of the courts of Bayern, which means that you may make a claim to defend your consumer protection rights in regards to these Legal Terms in Germany, or in the EU country in which you reside.</p>"},{"location":"terms/#18-dispute-resolution","title":"18. Dispute resolution","text":"<p>Informal Negotiations</p> <p>To expedite resolution and control the cost of any dispute, controversy, or claim related to these Legal Terms (each a \" Dispute\" and collectively, the \"Disputes\") brought by either you or us (individually, a \"Party\" and collectively, the \" Parties\"), the Parties agree to first attempt to negotiate any Dispute (except those Disputes expressly provided below) informally for at least thirty (30) days before initiating arbitration. Such informal negotiations commence upon written notice from one Party to the other Party.</p> <p>Binding Arbitration</p> <p>Any dispute arising from the relationships between the Parties to these Legal Terms shall be determined by one arbitrator who will be chosen in accordance with the Arbitration and Internal Rules of the European Court of Arbitration being part of the European Centre of Arbitration having its seat in Strasbourg, and which are in force at the time the application for arbitration is filed, and of which adoption of this clause constitutes acceptance. The seat of arbitration shall be Munich , Germany . The language of the proceedings shall be German . Applicable rules of substantive law shall be the law of Germany .</p> <p>Restrictions</p> <p>The Parties agree that any arbitration shall be limited to the Dispute between the Parties individually. To the full extent permitted by law, (a) no arbitration shall be joined with any other proceeding; (b) there is no right or authority for any Dispute to be arbitrated on a class-action basis or to utilize class action procedures; and (c) there is no right or authority for any Dispute to be brought in a purported representative capacity on behalf of the general public or any other persons.</p> <p>Exceptions to Informal Negotiations and Arbitration</p> <p>The Parties agree that the following Disputes are not subject to the above provisions concerning informal negotiations binding arbitration: (a) any Disputes seeking to enforce or protect, or concerning the validity of, any of the intellectual property rights of a Party; (b) any Dispute related to, or arising from, allegations of theft, piracy, invasion of privacy, or unauthorized use; and (c) any claim for injunctive relief. If this provision is found to be illegal or unenforceable, then neither Party will elect to arbitrate any Dispute falling within that portion of this provision found to be illegal or unenforceable and such Dispute shall be decided by a court of competent jurisdiction within the courts listed for jurisdiction above, and the Parties agree to submit to the personal jurisdiction of that court.</p>"},{"location":"terms/#19-corrections","title":"19. Corrections","text":"<p>There may be information on the Services that contains typographical errors, inaccuracies, or omissions, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information on the Services at any time, without prior notice.</p>"},{"location":"terms/#20-disclaimer","title":"20. Disclaimer","text":"<p>THE SERVICES ARE PROVIDED ON AN AS-IS AND AS-AVAILABLE BASIS. YOU AGREE THAT YOUR USE OF THE SERVICES WILL BE AT YOUR SOLE RISK. TO THE FULLEST EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED, IN CONNECTION WITH THE SERVICES AND YOUR USE THEREOF, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. WE MAKE NO WARRANTIES OR REPRESENTATIONS ABOUT THE ACCURACY OR COMPLETENESS OF THE SERVICES' CONTENT OR THE CONTENT OF ANY WEBSITES OR MOBILE APPLICATIONS LINKED TO THE SERVICES AND WE WILL ASSUME NO LIABILITY OR RESPONSIBILITY FOR ANY (1) ERRORS, MISTAKES, OR INACCURACIES OF CONTENT AND MATERIALS, (2) PERSONAL INJURY OR PROPERTY DAMAGE, OF ANY NATURE WHATSOEVER, RESULTING FROM YOUR ACCESS TO AND USE OF THE SERVICES, (3) ANY UNAUTHORIZED ACCESS TO OR USE OF OUR SECURE SERVERS AND/OR ANY AND ALL PERSONAL INFORMATION AND/OR FINANCIAL INFORMATION STORED THEREIN, (4) ANY INTERRUPTION OR CESSATION OF TRANSMISSION TO OR FROM THE SERVICES, (5) ANY BUGS, VIRUSES, TROJAN HORSES, OR THE LIKE WHICH MAY BE TRANSMITTED TO OR THROUGH THE SERVICES BY ANY THIRD PARTY, AND/OR (6) ANY ERRORS OR OMISSIONS IN ANY CONTENT AND MATERIALS OR FOR ANY LOSS OR DAMAGE OF ANY KIND INCURRED AS A RESULT OF THE USE OF ANY CONTENT POSTED, TRANSMITTED, OR OTHERWISE MADE AVAILABLE VIA THE SERVICES. WE DO NOT WARRANT, ENDORSE, GUARANTEE, OR ASSUME RESPONSIBILITY FOR ANY PRODUCT OR SERVICE ADVERTISED OR OFFERED BY A THIRD PARTY THROUGH THE SERVICES, ANY HYPERLINKED WEBSITE, OR ANY WEBSITE OR MOBILE APPLICATION FEATURED IN ANY BANNER OR OTHER ADVERTISING, AND WE WILL NOT BE A PARTY TO OR IN ANY WAY BE RESPONSIBLE FOR MONITORING ANY TRANSACTION BETWEEN YOU AND ANY THIRD-PARTY PROVIDERS OF PRODUCTS OR SERVICES. AS WITH THE PURCHASE OF A PRODUCT OR SERVICE THROUGH ANY MEDIUM OR IN ANY ENVIRONMENT, YOU SHOULD USE YOUR BEST JUDGMENT AND EXERCISE CAUTION WHERE APPROPRIATE.</p>"},{"location":"terms/#21-limitations-of-liability","title":"21. Limitations of liability","text":"<p>IN NO EVENT WILL WE OR OUR DIRECTORS, EMPLOYEES, OR AGENTS BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY DIRECT, INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL, OR PUNITIVE DAMAGES, INCLUDING LOST PROFIT, LOST REVENUE, LOSS OF DATA, OR OTHER DAMAGES ARISING FROM YOUR USE OF THE SERVICES, EVEN IF WE HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, OUR LIABILITY TO YOU FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM OF THE ACTION, WILL AT ALL TIMES BE LIMITED TO THE LESSER OF THE AMOUNT PAID, IF ANY, BY YOU TO US DURING THE zero (0) MONTH PERIOD PRIOR TO ANY CAUSE OF ACTION ARISING OR $0.00 USD. CERTAIN US STATE LAWS AND INTERNATIONAL LAWS DO NOT ALLOW LIMITATIONS ON IMPLIED WARRANTIES OR THE EXCLUSION OR LIMITATION OF CERTAIN DAMAGES. IF THESE LAWS APPLY TO YOU, SOME OR ALL OF THE ABOVE DISCLAIMERS OR LIMITATIONS MAY NOT APPLY TO YOU, AND YOU MAY HAVE ADDITIONAL RIGHTS.</p>"},{"location":"terms/#22-indemnification","title":"22. Indemnification","text":"<p>You agree to defend, indemnify, and hold us harmless, including our subsidiaries, affiliates, and all of our respective officers, agents, partners, and employees, from and against any loss, damage, liability, claim, or demand, including reasonable attorneys\u2019 fees and expenses, made by any third party due to or arising out of: (1) use of the Services; (2) breach of these Legal Terms; (3) any breach of your representations and warranties set forth in these Legal Terms; (4) your violation of the rights of a third party, including but not limited to intellectual property rights; or (5) any overt harmful act toward any other user of the Services with whom you connected via the Services. Notwithstanding the foregoing, we reserve the right, at your expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate, at your expense, with our defense of such claims. We will use reasonable efforts to notify you of any such claim, action, or proceeding which is subject to this indemnification upon becoming aware of it.</p>"},{"location":"terms/#23-user-data","title":"23. User data","text":"<p>We will maintain certain data that you transmit to the Services for the purpose of managing the performance of the Services, as well as data relating to your use of the Services. Although we perform regular routine backups of data, you are solely responsible for all data that you transmit or that relates to any activity you have undertaken using the Services. You agree that we shall have no liability to you for any loss or corruption of any such data, and you hereby waive any right of action against us arising from any such loss or corruption of such data.</p>"},{"location":"terms/#24-electronic-communications-transactions-and-signatures","title":"24. Electronic communications, transactions, and signatures","text":"<p>Visiting the Services, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications, and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Services, satisfy any legal requirement that such communication be in writing. YOU HEREBY AGREE TO THE USE OF ELECTRONIC SIGNATURES, CONTRACTS, ORDERS, AND OTHER RECORDS, AND TO ELECTRONIC DELIVERY OF NOTICES, POLICIES, AND RECORDS OF TRANSACTIONS INITIATED OR COMPLETED BY US OR VIA THE SERVICES. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances, or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by any means other than electronic means.</p>"},{"location":"terms/#25-california-users-and-residents","title":"25. California users and residents","text":"<p>If any complaint with us is not satisfactorily resolved, you can contact the Complaint Assistance Unit of the Division of Consumer Services of the California Department of Consumer Affairs in writing at 1625 North Market Blvd., Suite N 112, Sacramento, California 95834 or by telephone at (800) 952-5210 or (916) 445-1254.</p>"},{"location":"terms/#26-miscellaneous","title":"26. Miscellaneous","text":"<p>These Legal Terms and any policies or operating rules posted by us on the Services or in respect to the Services constitute the entire agreement and understanding between you and us. Our failure to exercise or enforce any right or provision of these Legal Terms shall not operate as a waiver of such right or provision. These Legal Terms operate to the fullest extent permissible by law. We may assign any or all of our rights and obligations to others at any time. We shall not be responsible or liable for any loss, damage, delay, or failure to act caused by any cause beyond our reasonable control. If any provision or part of a provision of these Legal Terms is determined to be unlawful, void, or unenforceable, that provision or part of the provision is deemed severable from these Legal Terms and does not affect the validity and enforceability of any remaining provisions. There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Legal Terms or use of the Services. You agree that these Legal Terms will not be construed against us by virtue of having drafted them. You hereby waive any and all defenses you may have based on the electronic form of these Legal Terms and the lack of signing by the parties hereto to execute these Legal Terms.</p>"},{"location":"terms/#27-contact-us","title":"27. Contact us","text":"<p>In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us at hello@dstack.ai.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/archive/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/dstack-sky/","title":"Introducing dstack Sky","text":"<p>Today we're previewing <code>dstack Sky</code>, a service built on top of  <code>dstack</code> that enables you to get GPUs at competitive rates from a wide pool of providers.</p> <p></p>"},{"location":"blog/dstack-sky/#tldr","title":"TL;DR","text":"<ul> <li>GPUs at competitive rates from multiple providers</li> <li>No need for your own cloud accounts</li> <li>Compatible with <code>dstack</code>'s CLI and API</li> <li>A pre-configured gateway for deploying services</li> </ul>"},{"location":"blog/dstack-sky/#introduction","title":"Introduction","text":"<p><code>dstack</code> is an open-source tool designed for managing AI infrastructure across various cloud platforms. It's lighter and more specifically geared towards AI tasks compared to Kubernetes.</p> <p>Due to its support for multiple cloud providers, <code>dstack</code> is frequently used to access on-demand and spot GPUs  across multiple clouds.  From our users, we've learned that managing various cloud accounts, quotas, and billing can be cumbersome.</p> <p>To streamline this process, we introduce <code>dstack Sky</code>, a managed service that enables users to access GPUs from multiple providers through <code>dstack</code> \u2013 without needing an account in each cloud provider. </p>"},{"location":"blog/dstack-sky/#what-is-dstack-sky","title":"What is dstack Sky?","text":"<p>Instead of running <code>dstack server</code> yourself, you point <code>dstack config</code> to a project set up with <code>dstack Sky</code>.</p> <pre><code>$ dstack config --url https://sky.dstack.ai \\\n    --project my-awesome-project \\\n    --token ca1ee60b-7b3f-8943-9a25-6974c50efa75\n</code></pre> <p>Now, you can use <code>dstack</code>'s CLI or API \u2013 just like you would with your own cloud accounts.</p> <pre><code>$ dstack run . -b tensordock -b vastai\n\n #  BACKEND     REGION  RESOURCES                    SPOT  PRICE \n 1  vastai      canada  16xCPU/64GB/1xRTX4090/1TB    no    $0.35\n 2  vastai      canada  16xCPU/64GB/1xRTX4090/400GB  no    $0.34\n 3  tensordock  us      8xCPU/48GB/1xRTX4090/480GB   no    $0.74\n    ...\n Shown 3 of 50 offers, $0.7424 max\n\nContinue? [y/n]:\n</code></pre> <p>Backends</p> <p><code>dstack Sky</code> supports the same backends as the open-source version, except that you don't need to set them up. By default, it uses all supported backends.</p> <p>You can use both on-demand and spot instances without needing to manage quotas, as they are automatically handled for you.</p> <p>With <code>dstack Sky</code> you can use all of <code>dstack</code>'s features, incl. dev environments,  tasks, services, and  pools.</p> <p>To use services, the open-source version requires setting up a gateway with your own domain.  <code>dstack Sky</code> comes with a pre-configured gateway.</p> <pre><code>$ dstack gateway list\n BACKEND  REGION     NAME    ADDRESS       DOMAIN                            DEFAULT\n aws      eu-west-1  dstack  3.252.79.143  my-awesome-project.sky.dstack.ai  \u2713\n</code></pre> <p>If you run it with <code>dstack Sky</code>, the service's endpoint will be available at <code>https://&lt;run name&gt;.&lt;project name&gt;.sky.dstack.ai</code>.</p> <p>Let's say we define a service:</p> <pre><code>type: service\n# Deploys Mixtral 8x7B with Ollama\n\n# Serve model using Ollama's Docker image\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull mixtral\n  - fg\nport: 11434\n\n# Configure hardware requirements\nresources:\n  gpu: 48GB..80GB\n\n# Enable OpenAI compatible endpoint\nmodel:\n  type: chat\n  name: mixtral\n  format: openai\n</code></pre> <p>If it has a <code>model</code> mapping, the model will be accessible at <code>https://gateway.&lt;project name&gt;.sky.dstack.ai</code> via the OpenAI compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.&lt;project name&gt;.sky.dstack.ai\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mixtral\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Now, you can choose \u2014 either use <code>dstack</code> via the open-source version or via <code>dstack Sky</code>,  or even use them side by side.</p> <p>Credits</p> <p>Are you an active contributor to the AI community? Request free <code>dstack Sky</code> credits.</p> <p><code>dstack Sky</code>  is live on Product Hunt. Support it by giving it your vote!</p> <p>Join Discord</p>"},{"location":"changelog/","title":"Blog","text":""},{"location":"docs/","title":"What is dstack?","text":"<p><code>dstack</code> is an open-source container orchestration engine designed for AI workloads across any cloud or data center.</p> <p>The supported cloud providers include AWS, GCP, Azure, Lambda, TensorDock, Vast.ai, CUDO, and RunPod. You can also use <code>dstack</code> ro run workloads on on-prem servers.</p> <p>Think of <code>dstack</code> as a lightweight alternative to Kubernetes or Slurm that provides a straightforward interface for AI model development, training, and deployment.</p> <p><code>dstack</code> supports dev environements, running tasks on clusters, and deployment with auto-scaling and built-in authorization, all right out of the box.</p> <p><code>dstack</code> is vendor-agnostic, allowing you to utilize any open-source libraries, frameworks, or tools from your container.</p>"},{"location":"docs/#why-use-dstack","title":"Why use dstack?","text":"<ol> <li>Lightweight and easy-to-use compared to Kubernetes and Slurm</li> <li>Supports all major GPU cloud providers</li> <li>Scalable and reliable for production environments</li> <li>Enables management of AI infrastructure across multiple clouds without vendor lock-in</li> <li>Fully open-source</li> </ol>"},{"location":"docs/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Install the open-source version of <code>dstack</code> and configure your own cloud accounts, or sign up with dstack Sky </li> <li>Define configurations such as dev environments, tasks,     and services.</li> <li>Run configurations via <code>dstack</code>'s CLI or API.</li> <li>Use pools to manage instances and on-prem servers.</li> </ol>"},{"location":"docs/#where-do-i-start","title":"Where do I start?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples </li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/protips/","title":"Protips","text":"<p>Below are tips and tricks to use <code>dstack</code> more efficiently.</p>"},{"location":"docs/protips/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p> <p>Once the commands work, go ahead and run them as a task or a service.</p> Jupyter <p>VS Code</p> <p>When you access a dev environment using your desktop VS Code, it allows you to work with Jupyter notebooks via its pre-configured and easy-to-use extension.</p> <p>JupyterLab</p> <p>If you prefer to use JupyterLab, you can run it as a task:</p> <pre><code>type: task\n\ncommands:\n    - pip install jupyterlab\n    - jupyter lab --allow-root\n\nports:\n    - 8888\n</code></pre>"},{"location":"docs/protips/#tasks","title":"Tasks","text":"<p>Tasks can be used not only for batch jobs but also for web applications.</p> <pre><code>type: task\n\npython: \"3.11\"\n\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\n\nports: \n  - 8501\n</code></pre> <p>While you run a task, <code>dstack</code> forwards the remote ports to <code>localhost</code>.</p> <pre><code>$ dstack run . -f app.dstack.yml\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: http://localhost:8501\n</code></pre> <p>This allows you to access the remote <code>8501</code> port on <code>localhost:8501</code> while the CLI is attached.</p> Port mapping <p>If you want to override the local port, use the <code>--port</code> option:</p> <pre><code>$ dstack run . -f app.dstack.yml --port 3000:8501\n</code></pre> <p>This will forward the remote <code>8501</code> port to <code>localhost:3000</code>.</p> <p>If the task works, go ahead and run it as a service.</p>"},{"location":"docs/protips/#environment-variables","title":"Environment variables","text":"<p>If a configuration requires an environment variable that you don't want to hardcode in the YAML, you can define it without assigning a value:</p> <pre><code>type: dev-environment\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n\npython: \"3.11\"\nide: vscode\n</code></pre> <p>Then, you can pass the environment variable either via the shell:</p> <pre><code>HUGGING_FACE_HUB_TOKEN=... dstack run . -f .dstack.yml\n</code></pre> <p>Or via the <code>-e</code> option of the <code>dstack run</code> command:</p> <pre><code>dstack run . -f .dstack.yml -e HUGGING_FACE_HUB_TOKEN=...\n</code></pre> <p>.env</p> <p>A better way to configure environment variables not hardcoded in YAML is by specifying them in a <code>.env</code> file:</p> <pre><code>HUGGING_FACE_HUB_TOKEN=...\n</code></pre> <p>If you install <code>direnv</code> , it will automatically pass the environment variables from the <code>.env</code> file to the <code>dstack run</code> command.</p> <p>Remember to add <code>.env</code> to <code>.gitignore</code> to avoid pushing it to the repo.    </p>"},{"location":"docs/protips/#idle-instances","title":"Idle instances","text":"<p>By default, the <code>dstack</code> run command reuses an idle instance from the pool. If no instance matches the requirements, it creates a new one.</p> <p>When the run finishes, the instance remains idle for the configured time (by default, <code>5m</code>) before it gets destroyed.</p> <p>You can change the default idle duration by using <code>--idle-duration DURATION</code> with <code>dstack run</code>, or set <code>termination_idle_duration</code> in the configuration or profile.</p> <p>An idle instance can be destroyed at any time via <code>dstack pool remove INSTANCE_NAME</code>.</p>"},{"location":"docs/protips/#profiles","title":"Profiles","text":"<p>If you don't want to specify the same parameters for each configuration, you can define them once via profiles and reuse them across configurations.</p> <p>This can be handy, for example, for configuring parameters such as <code>max_duration</code>, <code>max_price</code>, <code>termination_idle_duration</code>, <code>regions</code>, etc.</p> <p>Set <code>default</code> to <code>true</code> in your profile, and it will be applied automatically to any run.</p>"},{"location":"docs/protips/#attached-mode","title":"Attached mode","text":"<p>By default, <code>dstack run</code> runs in attached mode. This means it streams the logs as they come in and, in the case of a task, forwards its ports to <code>localhost</code>.</p> <p>If you detach the CLI, you can re-attach it using <code>dstack logs -a RUN_NAME</code>.</p> <p>To run in detached mode, use <code>-d</code> with <code>dstack run</code>.</p>"},{"location":"docs/protips/#gpu","title":"GPU","text":"<p>The <code>gpu</code> property withing <code>resources</code> (or the <code>--gpu</code> option with <code>dstack run</code>) allows specifying not only memory size but also GPU names, their memory, and quantity.</p> <p>Examples:</p> <ul> <li><code>1</code> (any GPU)</li> <li><code>A100</code> (A100)</li> <li><code>24GB..</code> (any GPU starting from 24GB)</li> <li><code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB)</li> <li><code>A10G,A100</code> (either A10G or A100)</li> <li><code>A100:80GB</code> (one A100 of 80GB)</li> <li><code>A100:2</code> (two A100)</li> <li><code>A100:40GB:2</code> (two A100 40GB)</li> </ul>"},{"location":"docs/protips/#service-quotas","title":"Service quotas","text":"<p>If you're using your own AWS, GCP, or Azure accounts, before you can use GPUs or spot instances, you have to request the corresponding service quotas for each type of instance in each region.</p> AWS <p>Check this guide   on EC2 service quotas. The relevant service quotas include:</p> <ul> <li><code>Running On-Demand P instances</code> (on-demand V100, A100 80GB x8)</li> <li><code>All P4, P3 and P2 Spot Instance Requests</code> (spot V100, A100 80GB x8)</li> <li><code>Running On-Demand G and VT instances</code> (on-demand T4, A10G, L4)</li> <li><code>All G and VT Spot Instance Requests</code> (spot T4, A10G, L4)</li> <li><code>Running Dedicated p5 Hosts</code> (on-demand H100)</li> <li><code>All P5 Spot Instance Requests</code> (spot H100)</li> </ul> GCP <p>Check this guide   on Compute Engine service quotas. The relevant service quotas include:</p> <ul> <li><code>NVIDIA V100 GPUs</code> (on-demand V100)</li> <li><code>Preemtible V100 GPUs</code> (spot V100)</li> <li><code>NVIDIA T4 GPUs</code> (on-demand T4)</li> <li><code>Preemtible T4 GPUs</code> (spot T4)</li> <li><code>NVIDIA L4 GPUs</code> (on-demand L4)</li> <li><code>Preemtible L4 GPUs</code> (spot L4)</li> <li><code>NVIDIA A100 GPUs</code> (on-demand A100)</li> <li><code>Preemtible A100 GPUs</code> (spot A100)</li> <li><code>NVIDIA A100 80GB GPUs</code> (on-demand A100 80GB)</li> <li><code>Preemtible A100 80GB GPUs</code> (spot A100 80GB)</li> <li><code>NVIDIA H100 GPUs</code> (on-demand H100)</li> <li><code>Preemtible H100 GPUs</code> (spot H100)</li> </ul> Azure <p>Check this guide   on Compute Engine service quotas. The relevant service quotas include:</p> <ul> <li><code>Total Regional Spot vCPUs</code> (any spot instances)</li> <li><code>Standard NCASv3_T4 Family vCPUs</code> (on-demand T4)</li> <li><code>Standard NVADSA10v5 Family vCPUs</code> (on-demand A10)</li> <li><code>Standard NCADS_A100_v4 Family vCPUs</code> (on-demand A100 80GB)</li> <li><code>Standard NDASv4_A100 Family vCPUs</code> (on-demand A100 40GB x8)</li> <li><code>Standard NDAMSv4_A100Family vCPUs</code> (on-demand A100 80GB x8)</li> <li><code>Standard NCadsH100v5 Family vCPUs</code> (on-demand H100)</li> <li><code>Standard NDSH100v5 Family vCPUs</code> (on-demand H100 x8)</li> </ul> <p>Note, for AWS, GCP, and Azure, service quota values are measured with the number of CPUs rather than GPUs.</p>"},{"location":"docs/protips/#data-and-models","title":"Data and models","text":"<p>For loading and saving data, it's best to use object storage like S3 or HuggingFace Datasets.</p> <p>For models, it's best to use services like HuggingFace Hub.</p>"},{"location":"docs/quickstart/","title":"Quickstart","text":"Prerequisites <p>To use the open-source version, make sure to install the server and configure backends.</p> <p>If you're using dstack Sky , install the CLI and run the <code>dstack config</code> command:</p> <p></p> <p>Once the CLI is set up, follow the quickstart.</p>"},{"location":"docs/quickstart/#initialize-a-repo","title":"Initialize a repo","text":"<p>To use <code>dstack</code>'s CLI in a folder, first run <code>dstack init</code> within that folder.</p> <pre><code>$ mkdir quickstart &amp;&amp; cd quickstart\n$ dstack init\n</code></pre> <p>Your folder can be a regular local folder or a Git repo.</p>"},{"location":"docs/quickstart/#define-a-configuration","title":"Define a configuration","text":"<p>Define what you want to run as a YAML file. The filename must end with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> Dev environmentTaskService <p>Dev environments allow you to quickly provision a machine with a pre-configured environment, resources, IDE, code, etc.</p> <p> <pre><code>type: dev-environment\n\n# Use either `python` or `image` to configure environment\npython: \"3.11\"\n# image: ghcr.io/huggingface/text-generation-inference:latest\n\nide: vscode\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>Tasks make it very easy to run any scripts, be it for training, data processing, or web apps. They allow you to pre-configure the environment, resources, code, etc.</p> <p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>Ensure <code>requirements.txt</code> and <code>train.py</code> are in your folder. You can take them from <code>dstack-examples</code>.</p> <p>Services make it easy to deploy models and apps cost-effectively as public endpoints, allowing you to use any frameworks.</p> <p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HUGGING_FACE_HUB_TOKEN # required to run gated models\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code\nport: 8000\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre>"},{"location":"docs/quickstart/#run-configuration","title":"Run configuration","text":"<p>Run a configuration using the <code>dstack run</code> command, followed by the working directory path (e.g., <code>.</code>), the path to the configuration file, and run options (e.g., configuring hardware resources, spot policy, etc.)</p> <pre><code>$ dstack run . -f train.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>The <code>dstack run</code> command automatically uploads your code, including any local uncommitted changes.  To exclude any files from uploading, use <code>.gitignore</code>.</p>"},{"location":"docs/quickstart/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks,      services, and pools </li> <li>Browse examples </li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/concepts/dev-environments/","title":"Dev environments","text":"<p>Before scheduling a task or deploying a model, you may want to run code interactively. Dev environments allow you to provision a remote machine set up with your code and favorite IDE with just one command.</p>"},{"location":"docs/concepts/dev-environments/#configuration","title":"Configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n\n# Specify the Python version, or your Docker image\npython: \"3.11\"\n\n# This pre-configures the IDE with required extensions\nide: vscode\n\n# Specify GPU, disk, and other resource requirements\nresources:\n  gpu: 80GB\n</code></pre> <p>See the .dstack.yml reference for more details.</p> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p>"},{"location":"docs/concepts/dev-environments/#environment-variables","title":"Environment variables","text":"<p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <pre><code>type: dev-environment\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\npython: \"3.11\"\nide: vscode\n\nresources:\n  gpu: 80GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p> <p>Profiles</p> <p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration, max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/dev-environments/#running","title":"Running","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and other options.</p> <pre><code>$ dstack run . -f .dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning `fast-moth-1`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+fast-moth-1/workflow\n</code></pre> <p>When <code>dstack</code> provisions the dev environment, it uses the current folder contents.</p> <p>.gitignore</p> <p>If there are large files or folders you'd like to avoid uploading,  you can list them in <code>.gitignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/dev-environments/#vs-code","title":"VS Code","text":"<p>To open the dev environment in your desktop IDE, use the link from the output  (such as <code>vscode://vscode-remote/ssh-remote+fast-moth-1/workflow</code>).</p> <p></p>"},{"location":"docs/concepts/dev-environments/#ssh","title":"SSH","text":"<p>Alternatively, while the CLI is attached to the run, you can connect to the dev environment via SSH:</p> <pre><code>$ ssh fast-moth-1\n</code></pre>"},{"location":"docs/concepts/dev-environments/#managing-runs","title":"Managing runs","text":"<p>Stopping runs</p> <p>Once the run exceeds the max duration, or when you use <code>dstack stop</code>,  the dev environment and its cloud resources are deleted.</p> <p>Listing runs</p> <p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/dev-environments/#whats-next","title":"What's next?","text":"<ol> <li>Check the <code>.dstack.yml</code> reference for more details and examples</li> </ol>"},{"location":"docs/concepts/pools/","title":"Pools","text":"<p>Pools enable the efficient reuse of cloud instances and on-premises servers across runs, simplifying their management.</p>"},{"location":"docs/concepts/pools/#adding-instances","title":"Adding instances","text":""},{"location":"docs/concepts/pools/#dstack-run","title":"<code>dstack run</code>","text":"<p>By default, when using the <code>dstack run</code> command, it tries to reuse an instance from a pool. If no idle instance meets the requirements, <code>dstack</code> automatically provisions a new cloud instance and adds it to the pool.</p> Reuse policy <p>To avoid provisioning new cloud instances with <code>dstack run</code>, use <code>--reuse</code>. Your run will be assigned to an idle instance in the pool.</p> Idle duration <p>By default, <code>dstack run</code> sets the idle duration of a newly provisioned instance to <code>5m</code>. This means that if the run is finished and the instance remains idle for longer than five minutes, it is automatically removed from the pool. To override the default idle duration, use  <code>--idle-duration DURATION</code> with <code>dstack run</code>.</p>"},{"location":"docs/concepts/pools/#dstack-pool-add","title":"<code>dstack pool add</code>","text":"<p>To manually provision a cloud instance and add it to a pool, use <code>dstack pool add</code>:</p> <pre><code>$ dstack pool add --gpu 80GB\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n</code></pre> <p>The <code>dstack pool add</code> command allows specifying resource requirements, along with the spot policy, idle duration, max price, retry policy, and other policies.</p> Idle duration <p>The default idle duration if you're using <code>dstack pool add</code> is <code>72h</code>. To override it, use the <code>--idle-duration DURATION</code> argument.</p> <p>You can also specify the policies via <code>.dstack/profiles.yml</code> instead of passing them as arguments. For more details on policies and their defaults, refer to <code>.dstack/profiles.yml</code>.</p> Limitations <p>The <code>dstack pool add</code> command is not supported for Kubernetes, VastAI, and RunPod backends yet.</p>"},{"location":"docs/concepts/pools/#dstack-pool-add-ssh","title":"<code>dstack pool add-ssh</code>","text":"<p>Any on-prem server that can be accessed via SSH can be added to a pool and used to run workloads.</p> <p>To add on-prem servers to the pool, use the <code>dstack pool add-ssh</code> command and pass the hostname of your server along with the SSH key.</p> <pre><code>$ dstack pool add-ssh ubuntu@54.73.155.119 -i ~/.ssh/id_rsa\n</code></pre> <p>The command accepts the same arguments as the standard <code>ssh</code> command.</p> <p>Requirements</p> <p>The on-prem server should be pre-installed with CUDA 12.1 and NVIDIA Docker.</p> <p>Once the instance is provisioned, you'll see it in the pool and will be able to run workloads on it.</p>"},{"location":"docs/concepts/pools/#removing-instances","title":"Removing instances","text":"<p>If the instance remains idle for the configured idle duration, <code>dstack</code> removes it and deletes all cloud resources.</p>"},{"location":"docs/concepts/pools/#dstack-pool-remove","title":"<code>dstack pool remove</code>","text":"<p>To remove an instance from the pool manually, use the <code>dstack pool remove</code> command. </p> <pre><code>$ dstack pool remove &amp;lt;instance name&amp;gt;\n</code></pre>"},{"location":"docs/concepts/pools/#list-instances","title":"List instances","text":""},{"location":"docs/concepts/pools/#dstack-pool-ps","title":"<code>dstack pool ps</code>","text":"<p>The <code>dstack pool ps</code> command lists active instances and their status (<code>busy</code> or <code>idle</code>).</p>"},{"location":"docs/concepts/services/","title":"Services","text":"<p>Services make it very easy to deploy any kind of model or application as public, secure, and scalable endpoints.</p> Gateway <p>If you're using the open-source server, you first have to set up a gateway.</p>"},{"location":"docs/concepts/services/#gateway","title":"Gateway","text":"<p>For example, if your domain is <code>example.com</code>, go ahead and run the  <code>dstack gateway create</code> command:</p> <pre><code>$ dstack gateway create --domain example.com --region eu-west-1 --backend aws\n\nCreating gateway...\n---&gt; 100%\n\n BACKEND  REGION     NAME          ADDRESS        DOMAIN       DEFAULT\n aws      eu-west-1  sour-fireant  52.148.254.14  example.com  \u2713\n</code></pre> <p>Afterward, in your domain's DNS settings, add an <code>A</code> DNS record for <code>*.example.com</code>  pointing to the IP address of the gateway.</p> <p>Now, if you run a service, <code>dstack</code> will make its endpoint available at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>In case your service has the model mapping configured, <code>dstack</code> will  automatically make your model available at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <p>If you're using dstack Sky , the gateway is set up for you.</p>"},{"location":"docs/concepts/services/#configuration","title":"Configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\n\n# Specify the Docker image\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\n# Specify environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN # required to run gated models\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\n\n# The commands to run on start of the service\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code\n\n# Specify the port of the service\nport: 8000\n\n# Specify GPU, disk, and other resource requirements\nresources:\n  gpu: 80GB\n</code></pre> <p>See the .dstack.yml reference for more details.</p> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p>"},{"location":"docs/concepts/services/#environment-variables","title":"Environment variables","text":"<p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN # required to run gated models\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\n\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code\n\nport: 8000\n\nresources:\n  gpu: 80GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/concepts/services/#model-mapping","title":"Model mapping","text":"<p>By default, if you run a service, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>If you run a model, you can optionally configure the mapping to make it accessible via the  OpenAI-compatible interface.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HUGGING_FACE_HUB_TOKEN # required to run gated models\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code\nport: 8000\n\nresources:\n  gpu: 80GB\n\n# Enable the OpenAI-compatible endpoint   \nmodel:\n  type: chat\n  name: mistralai/Mistral-7B-Instruct-v0.1\n  format: tgi\n</code></pre> <p>In this case, with such a configuration, once the service is up, you'll be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <p>The <code>format</code> supports only <code>tgi</code> (Text Generation Inference)  and <code>openai</code> (if you are using Text Generation Inference or vLLM with OpenAI-compatible mode).</p> Chat template <p>By default, <code>dstack</code> loads the chat template  from the model's repository. If it is not present there, manual configuration is required.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code --quantize gptq\nport: 8000\n\nresources:\n  gpu: 80GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Llama-2-13B-chat-GPTQ\n  format: tgi\n  chat_template: \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\\n' + system_message + '\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '&lt;s&gt;[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' &lt;/s&gt;' }}{% endif %}{% endfor %}\"\n  eos_token: \"&lt;/s&gt;\"\n</code></pre>"},{"location":"docs/concepts/services/#limitations","title":"Limitations","text":"<p>Please note that model mapping is an experimental feature with the following limitations:</p> <ol> <li>Doesn't work if your <code>chat_template</code> uses <code>bos_token</code>. As a workaround, replace <code>bos_token</code> inside <code>chat_template</code> with the token content itself.</li> <li>Doesn't work if <code>eos_token</code> is defined in the model repository as a dictionary. As a workaround, set <code>eos_token</code> manually, as shown in the example above (see Chat template).</li> </ol> <p>If you encounter any other issues, please make sure to file a GitHub issue.</p>"},{"location":"docs/concepts/services/#replicas-and-scaling","title":"Replicas and scaling","text":"<p>By default, <code>dstack</code> runs a single replica of the service. You can configure the number of replicas as well as the auto-scaling policy.</p> <pre><code>type: service\n\npython: \"3.11\"\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nreplicas: 1..4\nscaling:\n  metric: rps\n  target: 10\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre> <p>If you specify the minimum number of replicas as <code>0</code>, the service will scale down to zero when there are no requests.</p> Profiles <p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration, max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/services/#running","title":"Running","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options.</p> <pre><code>$ dstack run . -f serve.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nService is published at https://yellow-cat-1.example.com\n</code></pre> <p>When <code>dstack</code> submits the task, it uses the current folder contents.</p> .gitignore <p>If there are large files or folders you'd like to avoid uploading,  you can list them in <code>.gitignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/services/#service-endpoint","title":"Service endpoint","text":"<p>One the service is up, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>Model endpoint</p> <p>In case the service has the model mapping configured, you will also be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p>"},{"location":"docs/concepts/services/#authorization","title":"Authorization","text":"<p>By default, the service endpoint requires the <code>Authorization</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>. </p> <pre><code>$ curl https://yellow-cat-1.example.com/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"&amp;lt;s&amp;gt;[INST] What is your favourite condiment?[/INST]\"}' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: \"Bearer &amp;lt;dstack token&amp;gt;\"'\n</code></pre> <p>Authorization can be disabled by setting <code>auth</code> to <code>false</code> in the service configuration file.</p>"},{"location":"docs/concepts/services/#managing-runs","title":"Managing runs","text":"<p>Stopping runs</p> <p>When you use <code>dstack stop</code>, the service and its cloud resources are deleted.</p> <p>Listing runs</p> <p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/services/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference  and vLLM  examples</li> <li>Check the <code>.dstack.yml</code> reference for more details and examples</li> <li>Browse examples </li> </ol>"},{"location":"docs/concepts/tasks/","title":"Tasks","text":"<p>Tasks allow for convenient scheduling of various batch jobs, such as training, fine-tuning, or data processing, as well as running web applications.</p> <p>You can run tasks on a single machine or on a cluster of nodes.</p>"},{"location":"docs/concepts/tasks/#configuration","title":"Configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n\n# Specify the Python version, or your Docker image\npython: \"3.11\"\n\n# Specify environment variables\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\n# The commands to run on start of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# Specify GPU, disk, and other resource requirements\nresources:\n  gpu: 80GB\n</code></pre> <p>See the .dstack.yml reference for more details.</p> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p>"},{"location":"docs/concepts/tasks/#environment-variables","title":"Environment variables","text":"<p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nresources:\n  gpu: 80GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/concepts/tasks/#ports","title":"Ports","text":"<p>A task can configure ports. In this case, if the task is running an application on a port, <code>dstack run</code>  will securely allow you to access this port from your local machine through port forwarding.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python fine-tuning/qlora/train.py\nports:\n  - 6000\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>When running it, <code>dstack run</code> forwards <code>6000</code> port to <code>localhost:6000</code>, enabling secure access. </p> Port mapping <p>By default, <code>dstack</code> uses the same ports on your local machine for port forwarding. However, you can override local ports using <code>--port</code>:</p> <pre><code>$ dstack run . -f train.dstack.yml --port 6000:6001\n</code></pre> <p>This will forward the task's port <code>6000</code> to <code>localhost:6001</code>.</p>"},{"location":"docs/concepts/tasks/#nodes","title":"Nodes","text":"<p>By default, the task runs on a single node. However, you can run it on a cluster of nodes.</p> <pre><code>type: task\n\n# The size of the cluster\nnodes: 2\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r requirements.txt\n  - torchrun\n    --nproc_per_node=$DSTACK_GPUS_PER_NODE\n    --node_rank=$DSTACK_NODE_RANK\n    --nnodes=$DSTACK_NODES_NUM\n    --master_addr=$DSTACK_MASTER_NODE_IP\n    --master_port=8008 resnet_ddp.py\n    --num_epochs 20\n\nresources:\n  gpu: 24GB\n</code></pre> <p>If you run the task, <code>dstack</code> first provisions the master node and then runs the other nodes of the cluster. All nodes are provisioned in the same region.</p> Backends <p>Running on multiple nodes is supported only with AWS, GCP, and Azure.</p>"},{"location":"docs/concepts/tasks/#args","title":"Args","text":"<p>You can parameterize tasks with user arguments using <code>${{ run.args }}</code> in the configuration.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py ${{ run.args }}\n\nresources:\n  gpu: 80GB\n</code></pre> <p>Now, you can pass your arguments to the <code>dstack run</code> command:</p> <pre><code>$ dstack run . -f train.dstack.yml --train_batch_size=1 --num_train_epochs=100\n</code></pre> <p>The <code>dstack run</code> command will pass <code>--train_batch_size=1</code> and <code>--num_train_epochs=100</code> as arguments to <code>train.py</code>.</p> Profiles <p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration, max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/tasks/#running","title":"Running","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and other options.</p> <pre><code>$ dstack run . -f train.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>When <code>dstack</code> submits the task, it uses the current folder contents.</p> <p>.gitignore</p> <p>If there are large files or folders you'd like to avoid uploading,  you can list them in <code>.gitignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/tasks/#managing-runs","title":"Managing runs","text":"<p>Stoping runs</p> <p>Once the run exceeds the max duration, or when you use <code>dstack stop</code>,  the task and its cloud resources are deleted.</p> <p>Listing runs</p> <p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/tasks/#whats-next","title":"What's next?","text":"<ol> <li>Check the QLoRA  example</li> <li>Check the <code>.dstack.yml</code> reference for more details and examples</li> <li>Browse all examples </li> </ol>"},{"location":"docs/installation/","title":"Installation","text":"<p>To install <code>dstack</code>, use <code>pip</code>:</p> <pre><code>$ pip install \"dstack[all]\" -U\n</code></pre> <p>To use the open-source version of <code>dstack</code>, you have to configure  your cloud accounts via <code>~/.dstack/server/config.yml</code> and start the <code>dstack</code> server.</p>"},{"location":"docs/installation/#configure-backends","title":"Configure backends","text":"<p>To configure cloud accounts, create the   <code>~/.dstack/server/config.yml</code> file, and configure each cloud under the <code>backends</code> property.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Refer below for examples on how to configure a specific cloud provider.</p> Projects <p>For flexibility, <code>dstack</code> server permits you to configure backends for multiple projects.  If you intend to use only one project, name it <code>main</code>.</p>"},{"location":"docs/installation/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p> Access keyDefault credentials <p>Create an access key by following the this guide . Once you've downloaded the <code>.csv</code> file with your IAM user's Access key ID and Secret access key, proceed to  configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: KKAAUKLIZ5EHKICAOASV\n      secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> <p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: default\n</code></pre> VPC <p>By default, <code>dstack</code> uses the default VPC. It's possible to customize it:</p> vpc_namevpc_ids <pre><code>projects:\n- name: main\n  backends:\n    - type: aws\n      creds:\n        type: default\n\n      vpc_name: my-vpc\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n    - type: aws\n      creds:\n        type: default\n\n      vpc_ids:\n        us-east-1: vpc-0a2b3c4d5e6f7g8h\n        us-east-2: vpc-9i8h7g6f5e4d3c2b\n        us-west-1: vpc-4d3c2b1a0f9e8d7\n</code></pre> <p>Note, the VPCs are required to have a public subnet.</p> Required AWS permissions <p>The following AWS policy permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"servicequotas:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetRole\",\n                \"iam:CreateRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:TagRole\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreatePolicy\",\n                \"iam:TagPolicy\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetInstanceProfile\",\n                \"iam:CreateInstanceProfile\",\n                \"iam:AddRoleToInstanceProfile\",\n                \"iam:TagInstanceProfile\",\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"docs/installation/#azure","title":"Azure","text":"<p>There are two ways to configure Azure: using a client secret or using the default credentials.</p> Client secretDefault credentials <p>A client secret can be created using the Azure CLI :</p> <pre><code>SUBSCRIPTION_ID=...\naz ad sp create-for-rbac \n    --name dstack-app \\\n    --role $DSTACK_ROLE \\ \n    --scopes /subscriptions/$SUBSCRIPTION_ID \\ \n    --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <p>Obtain the <code>subscription_id</code> and <code>tenant_id</code> via the Azure CLI :</p> <pre><code>az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>Then proceed to configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your <code>subscription_id</code>, run</p> <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre> Required Azure permissions <p>The following Azure permissions are sufficient for <code>dstack</code> to work: <pre><code>{\n    \"properties\": {\n        \"roleName\": \"dstack-role\",\n        \"description\": \"Minimal reqired permissions for using Azure with dstack\",\n        \"assignableScopes\": [\n            \"/subscriptions/${YOUR_SUBSCRIPTION_ID}\"\n        ],\n        \"permissions\": [\n            {\n              \"actions\": [\n                  \"Microsoft.Authorization/*/read\",\n                  \"Microsoft.Compute/availabilitySets/*\",\n                  \"Microsoft.Compute/locations/*\",\n                  \"Microsoft.Compute/virtualMachines/*\",\n                  \"Microsoft.Compute/virtualMachineScaleSets/*\",\n                  \"Microsoft.Compute/cloudServices/*\",\n                  \"Microsoft.Compute/disks/write\",\n                  \"Microsoft.Compute/disks/read\",\n                  \"Microsoft.Compute/disks/delete\",\n                  \"Microsoft.Network/networkSecurityGroups/*\",\n                  \"Microsoft.Network/locations/*\",\n                  \"Microsoft.Network/virtualNetworks/*\",\n                  \"Microsoft.Network/networkInterfaces/*\",\n                  \"Microsoft.Network/publicIPAddresses/*\",\n                  \"Microsoft.Resources/subscriptions/resourceGroups/read\",\n                  \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n                  \"Microsoft.Resources/subscriptions/read\"\n              ],\n              \"notActions\": [],\n              \"dataActions\": [],\n              \"notDataActions\": []\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"docs/installation/#gcp","title":"GCP","text":"Enable APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>.</p> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com \n</code></pre> <p>There are two ways to configure GCP: using a service account or using the default credentials.</p> Service accountDefault credentials <p>To create a service account, follow this guide . Make sure to grant it the <code>Service Account User</code> and <code>Compute Admin</code> roles.</p> <p>After setting up the service account create a key  for it and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: service_account\n      filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your GCP project ID, run </p> <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre> Required GCP permissions <p>The following GCP permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>compute.disks.create\ncompute.firewalls.create\ncompute.images.useReadOnly\ncompute.instances.create\ncompute.instances.delete\ncompute.instances.get\ncompute.instances.setLabels\ncompute.instances.setMetadata\ncompute.instances.setTags\ncompute.networks.updatePolicy\ncompute.regions.list\ncompute.subnetworks.use\ncompute.subnetworks.useExternalIp\ncompute.zoneOperations.get\n</code></pre>"},{"location":"docs/installation/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud  account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: lambda\n    creds:\n      type: api_key\n      api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/installation/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock  account, click API in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>NOTE:</p> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#vastai","title":"Vast.ai","text":"<p>Log into your Vast.ai  account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>NOTE:</p> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#cudo","title":"CUDO","text":"<p>Log into your CUDO Compute  account, click API keys in the sidebar, and click the <code>Create an API key</code> button.</p> <p>Ensure you've created a project with CUDO Compute, then proceed to configuring the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: cudo\n    project_id: my-cudo-project\n    creds:\n      type: api_key\n      api_key: 7487240a466624b48de22865589\n</code></pre>"},{"location":"docs/installation/#runpod","title":"RunPod","text":"<p>Log into your RunPod  console, click Settings in the sidebar, expand the <code>API Keys</code> section, and click the button to create a key.</p> <p>Then proceed to configuring the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: runpod\n    creds:\n      type: api_key\n      api_key: US9XTPDIV8AR42MMINY8TCKRB8S4E7LNRQ6CAUQ9\n</code></pre> <p>NOTE:</p> <p>If you're using a custom Docker image, its entrypoint cannot be anything other than <code>/bin/bash</code> or <code>/bin/sh</code>.  See the issue  for more details.</p> <p>NOTE:</p> <p>The <code>runpod</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#datacrunch","title":"DataCrunch","text":"<p>Log into your DataCrunch  account, click Account Settings in the sidebar, find <code>REST API Credentials</code> area and then click the <code>Generate Credentials</code> button.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: datacrunch\n    creds:\n      type: api_key\n      client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n      client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/installation/#kubernetes","title":"Kubernetes","text":"<p><code>dstack</code> supports both self-managed, and managed Kubernetes clusters.</p> Prerequisite <p>To use GPUs with Kubernetes, the cluster must be installed with the  NVIDIA GPU Operator .</p> <p>To configure a Kubernetes backend, specify the path to the kubeconfig file,  and the port that <code>dstack</code> can use for proxying SSH traffic. In case of a self-managed cluster, also specify the IP address of any node in the cluster.</p> Self-managedManaged <p>Here's how to configure the backend to use a self-managed cluster.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_host: localhost # The external IP address of any node\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using Kind, make sure to add it via <code>extraPortMappings</code>:</p> <p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32000 # Must be same as `ssh_port`\n    hostPort: 32000 # Must be same as `ssh_port`\n</code></pre> <p>Here's how to configure the backend to use a managed cluster (AWS, GCP, Azure).</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using EKS, make sure to add it via an ingress rule of the corresponding security group:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;cluster-security-group-id&gt; --protocol tcp --port 32000 --cidr 0.0.0.0/0\n</code></pre>"},{"location":"docs/installation/#start-the-server","title":"Start the server","text":"<p>Once the <code>~/.dstack/server/config.yml</code> file is configured, proceed to start the server:</p> pipDocker <pre><code>$ dstack server\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <pre><code>$ docker run -p 3000:3000 \\\n    -v $HOME/.dstack/server/:/root/.dstack/server \\\n    dstackai/dstack\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <p>For more details on how to deploy <code>dstack</code> using Docker, check its Docker repo.</p>"},{"location":"docs/installation/#configure-the-cli","title":"Configure the CLI","text":"<p>To point the CLI to the <code>dstack</code> server, you need to configure <code>~/.dstack/config.yml</code>  with the server address, user token and project name.</p> <pre><code>$ dstack config --url http://127.0.0.1:3000 \\\n    --project main \\\n    --token bbae0f28-d3dd-4820-bf61-8f4bb40815da\n\nConfiguration is updated at ~/.dstack/config.yml\n</code></pre>"},{"location":"docs/installation/#whats-next","title":"What's next?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples </li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<ul> <li><code>dev-environment</code></li> <li><code>task</code></li> <li><code>service</code></li> </ul>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Sometimes, you may want to reuse the same parameters across different <code>.dstack.yml</code> configurations.</p> <p>This can be achieved by defining those parameters in a profile.</p> <p>Profiles can be defined on the repository level (via the <code>.dstack/profiles.yml</code> file in the root directory of the repository) or on the global level (via the <code>~/.dstack/profiles.yml</code> file).</p> <p>Any profile can be marked as default so that it will be applied automatically for any run. Otherwise, you can refer to a specific profile via <code>--profile NAME</code> in <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: my-profile\n\n    # The spot pololicy can be \"spot\", \"on-demand\", or \"auto\"\n    spot_policy: auto\n\n    # Limit the maximum price of the instance per hour\n    max_price: 1.5\n\n    # Stop any run if it runs longer that this duration\n    max_duration: 1d\n\n    # Use only these backends\n    backends: [azure, lambda]\n\n    # If set to true, this profile will be applied automatically\n    default: true\n</code></pre> <p>The profile configuration supports many properties. See below.</p>"},{"location":"docs/reference/profiles.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/profiles.yml/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisionig (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/profiles.yml/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisionig (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/profiles.yml/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisionig (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/profiles.yml/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/profiles.yml/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for re-submitting the run.","text":""},{"location":"docs/reference/profiles.yml/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/profiles.yml/#max_price","title":"<code>max_price</code> - (Optional) The maximum price per hour, in dollars.","text":""},{"location":"docs/reference/profiles.yml/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/profiles.yml/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/profiles.yml/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/profiles.yml/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for termination instances. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/profiles.yml/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/profiles.yml/#name","title":"<code>name</code> -  The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code>.","text":""},{"location":"docs/reference/profiles.yml/#default","title":"<code>default</code> - (Optional) If set to true, <code>dstack run</code> will use this profile by default..","text":""},{"location":"docs/reference/profiles.yml/#retry_policy","title":"<code>retry_policy</code>","text":""},{"location":"docs/reference/profiles.yml/#retry","title":"<code>retry</code> - (Optional) Whether to retry the run on failure or not.","text":""},{"location":"docs/reference/profiles.yml/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/api/python/","title":"Python API","text":"<p>The Python API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and displaying its logs.</p> <pre><code>import sys\n\nfrom dstack.api import Task, GPU, Client, Resources\n\nclient = Client.from_config()\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",  # If not specified, a random name is assigned \n    configuration=task,\n    repo=None, # Specify to mount additional files\n)\n\nrun.attach()\n\ntry:\n    for log in run.logs():\n        sys.stdout.buffer.write(log)\n        sys.stdout.buffer.flush()\nexcept KeyboardInterrupt:\n    run.stop(abort=True)\nfinally:\n    run.detach()\n</code></pre> <p>NOTE:</p> <ol> <li>The <code>configuration</code> argument in the <code>submit</code> method can be either <code>dstack.api.Task</code> or <code>dstack.api.Service</code>. </li> <li>If you create <code>dstack.api.Task</code> or <code>dstack.api.Service</code>, you may specify the <code>image</code> argument. If <code>image</code> isn't    specified, the default image will be used. For a private Docker registry, ensure you also pass the <code>registry_auth</code> argument.</li> <li>The <code>repo</code> argument in the <code>submit</code> method allows the mounting of a local folder, a remote repo, or a    programmatically created repo. In this case, the <code>commands</code> argument can refer to the files within this repo.</li> <li>The <code>attach</code> method waits for the run to start and, for <code>dstack.api.Task</code> sets up an SSH tunnel and forwards configured <code>ports</code> to <code>localhost</code>.</li> </ol>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>runs</code> <code>RunCollection</code> <p>Operations with runs.</p> <code>repos</code> <code>RepoCollection</code> <p>Operations with repositories.</p> <code>backends</code> <code>BackendCollection</code> <p>Operations with backends.</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using the default configuration from <code>~/.dstack/config.yml</code> if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>The dstack server URL (e.g. <code>http://localhost:3000/</code> or <code>https://sky.dstack.ai</code>)</p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>The dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for SSH tunneling</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>A client instance</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.RunCollection</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, regions=None, instance_types=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>Union[Task, Service]</code> <p>A run configuration.</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>The path to the configuration file, relative to the root directory of the repo.</p> <code>None</code> <code>repo</code> <code>Union[LocalRepo, RemoteRepo, VirtualRepo]</code> <p>A repo to mount to the run.</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>A list of allowed backend for provisioning.</p> <code>None</code> <code>regions</code> <code>Optional[List[str]]</code> <p>A list of cloud regions for provisioning.</p> <code>None</code> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration. Overrides the configuration's resources.</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>A spot policy for provisioning.</p> <code>None</code> <code>retry_policy</code> <code>RetryPolicy</code> <p>A retry policy.</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>The max instance running duration in seconds.</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>The max instance price in dollars per hour for provisioning.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>A working directory relative to the repo root directory</p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>A desired name of the run. Must be unique in the project. If not specified, a random name is assigned.</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>Whether local ports should be reserved in advance.</p> <code>True</code> <p>Returns:</p> Type Description <code>Run</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.RepoCollection</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Initializes the repo and configures its credentials in the project. Must be invoked before mounting the repo to a run.</p> <p>Example:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\",\n)\nclient.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Once the repo is initialized, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The repo to initialize.</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for accessing the remote repo.</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>The GitHub OAuth token to access the remote repo.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>ports</code> <code>List[PortMapping]</code> <p>Port numbers/mapping to expose</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration.</p>"},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>port</code> <code>PortMapping</code> <p>The port, that application listens to or the mapping</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration.</p> <code>model</code> <code>Optional[ModelMapping]</code> <p>Mapping of the model for the OpenAI-compatible endpoint.</p> <code>auth</code> <code>bool</code> <p>Enable the authorization. Defaults to <code>True</code>.</p> <code>replicas</code> <code>Range[int]</code> <p>The range of the number of replicas. Defaults to <code>1</code>.</p> <code>scaling</code> <code>Range[int]</code> <p>Optional[ScalingSpec]: The auto-scaling configuration.</p>"},{"location":"docs/reference/api/python/##dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>RunStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False, replica_num=0, job_num=0)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/##dstack.api.Resources","title":"<code>dstack.api.Resources</code>","text":"<p>Creates required resources specification.</p> <p>Parameters:</p> Name Type Description Default <code>cpu</code> <code>Optional[Range[int]]</code> <p>The number of CPUs</p> <code>DEFAULT_CPU_COUNT</code> <code>memory</code> <code>Optional[Range[Memory]]</code> <p>The size of RAM memory (e.g., <code>\"16GB\"</code>)</p> <code>DEFAULT_MEMORY_SIZE</code> <code>gpu</code> <code>Optional[GPUSpec]</code> <p>The GPU spec</p> <code>None</code> <code>shm_size</code> <code>Optional[Range[Memory]]</code> <p>The of shared memory (e.g., <code>\"8GB\"</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.</p> <code>None</code> <code>disk</code> <code>Optional[DiskSpec]</code> <p>The disk spec</p> <code>None</code> <p>Returns:</p> Type Description <code>ResourcesSpec</code> <p>resources specification</p>"},{"location":"docs/reference/api/python/##dstack.api.GPU","title":"<code>dstack.api.GPU</code>","text":"<p>Creates GPU specification.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[List[str]]</code> <p>The name of the GPU (e.g., <code>\"A100\"</code> or <code>\"H100\"</code>)</p> <code>None</code> <code>count</code> <code>Optional[Range[int]]</code> <p>The number of GPUs</p> <code>DEFAULT_GPU_COUNT</code> <code>memory</code> <code>Optional[Range[Memory]]</code> <p>The size of a single GPU memory (e.g., <code>\"16GB\"</code>)</p> <code>None</code> <code>total_memory</code> <code>Optional[Range[Memory]]</code> <p>The total size of all GPUs memory (e.g., <code>\"32GB\"</code>)</p> <code>None</code> <code>compute_capability</code> <code>Optional[float]</code> <p>The minimum compute capability of the GPU (e.g., <code>7.5</code>)</p> <code>None</code> <p>Returns:</p> Type Description <code>GPUSpec</code> <p>GPU specification</p>"},{"location":"docs/reference/api/python/##dstack.api.Disk","title":"<code>dstack.api.Disk</code>","text":"<p>Creates disk specification.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Range[Memory]</code> <p>The size of the disk (e.g., <code>\"100GB\"</code>)</p> required <p>Returns:</p> Type Description <code>DiskSpec</code> <p>disk specification</p>"},{"location":"docs/reference/api/python/##dstack.api.LocalRepo","title":"<code>dstack.api.LocalRepo</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Example:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=LocalRepo.from_dir(\".\"), # Mount the current folder to the run\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.LocalRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>LocalRepo</code> <p>A local repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.RemoteRepo","title":"<code>dstack.api.RemoteRepo</code>","text":"<p>Creates an instance of a remote Git repo for mounting to a submitted run.</p> <p>Using a locally checked-out remote Git repo:</p> <pre><code>repo=RemoteRepo.from_dir(repo_dir=\".\")\n</code></pre> <p>Using a remote Git repo by a URL:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\"\n)\n</code></pre> <p>Initialize the repo before mounting it.</p> <pre><code>client.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Finally, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_url","title":"<code>from_url(repo_url, repo_branch=None, repo_hash=None)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>The URL of a remote Git repo</p> required <code>repo_branch</code> <code>Optional[str]</code> <p>The name of the remote branch. Must be specified if <code>hash</code> is not specified.</p> <code>None</code> <code>repo_hash</code> <code>Optional[str]</code> <p>The hash of the revision. Must be specified if <code>branch</code> is not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.VirtualRepo","title":"<code>dstack.api.VirtualRepo</code>","text":"<p>Allows mounting a repo created programmatically.</p> <p>Example:</p> <pre><code>virtual_repo = VirtualRepo(repo_id=\"some-unique-repo-id\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"requirements.txt\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"train.py\")\n\nrun = client.runs.submit(\n    configuration=...,\n    repo=virtual_repo,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>repo_id</code> <p>A unique identifier of the repo</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file","title":"<code>add_file(path, content)</code>","text":"<p>Adds a given file to the repo.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path inside the repo to add the file.</p> <code>content</code> <code>bytes</code> <p>The contents of the file.</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file_from_package","title":"<code>add_file_from_package(package, path)</code>","text":"<p>Includes a file from a given package to the repo.</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>Union[ModuleType, str]</code> <p>A package to include the file from.</p> <code>path</code> <code>str</code> <p>The path to the file to include to the repo. Must be relative to the package directory.</p>"},{"location":"docs/reference/api/python/##dstack.api.RegistryAuth","title":"<code>dstack.api.RegistryAuth</code>","text":"<p>Credentials for pulling a private Docker image.</p> <p>Attributes:</p> Name Type Description <code>username</code> <code>str</code> <p>The username</p> <code>password</code> <code>str</code> <p>The password or access token</p>"},{"location":"docs/reference/api/python/#dstack.api.BackendType","title":"<code>dstack.api.BackendType</code>","text":"<p>Attributes:</p> Name Type Description <code>AWS</code> <code>BackendType</code> <p>Amazon Web Services</p> <code>AZURE</code> <code>BackendType</code> <p>Microsoft Azure</p> <code>CUDO</code> <code>BackendType</code> <p>Cudo</p> <code>DSTACK</code> <code>BackendType</code> <p>dstack Sky</p> <code>GCP</code> <code>BackendType</code> <p>Google Cloud Platform</p> <code>DATACRUNCH</code> <code>BackendType</code> <p>DataCrunch</p> <code>KUBERNETES</code> <code>BackendType</code> <p>Kubernetes</p> <code>LAMBDA</code> <code>BackendType</code> <p>Lambda Cloud</p> <code>RUNPOD</code> <code>BackendType</code> <p>Runpod Cloud</p> <code>TENSORDOCK</code> <code>BackendType</code> <p>TensorDock Marketplace</p> <code>VASTAI</code> <code>BackendType</code> <p>Vast.ai Marketplace</p>"},{"location":"docs/reference/api/rest/","title":"REST API","text":""},{"location":"docs/reference/cli/","title":"CLI","text":""},{"location":"docs/reference/cli/#commands","title":"Commands","text":""},{"location":"docs/reference/cli/#dstack-server","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p> <pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL] [--default]\n                     [--no-default] [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to INFO.\n  --default             Update the default project configuration\n  --no-default          Do not update the default project configuration\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/#dstack-init","title":"dstack init","text":"<p>This command initializes the current folder as a repo.</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access the remote repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use git\n</code></pre> Git credentials <p>If the current folder is a Git repo, the command authorizes <code>dstack</code> to access it. By default, the command uses the default Git credentials configured for the repo.  You can override these credentials via <code>--token</code> (OAuth token) or <code>--git-identity</code>.</p> Custom SSH key <p>By default, this command generates an SSH key that will be used for port forwarding and SSH access to running workloads.  You can override this key via <code>--ssh-identity</code>.</p>"},{"location":"docs/reference/cli/#dstack-run","title":"dstack run","text":"<p>This command runs a given configuration.</p> <pre><code>$ dstack run . --help\nUsage: dstack run [--project NAME] [-h [TYPE]] [-f FILE] [-n RUN_NAME] [-d]\n                  [-y] [--max-offers MAX_OFFERS] [--profile NAME]\n                  [--max-price PRICE] [--max-duration DURATION] [-b NAME]\n                  [-r NAME] [--instance-type NAME]\n                  [--pool POOL_NAME | --reuse | --dont-destroy | --idle-duration IDLE_DURATION | --instance NAME]\n                  [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                  [--retry | --no-retry | --retry-duration DURATION]\n                  [-e KEY=VALUE] [--gpu SPEC] [--disk RANGE]\n                  working_dir\n\nPositional Arguments:\n  working_dir\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit. TYPE is one of task,\n                        dev-environment, service\n  -f, --file FILE       The path to the run configuration file. Defaults to\n                        WORKING_DIR/.dstack.yml\n  -n, --name RUN_NAME   The name of the run. If not specified, a random name\n                        is assigned\n  -d, --detach          Do not poll logs and run status\n  -y, --yes             Do not ask for plan confirmation\n  --max-offers MAX_OFFERS\n                        Number of offers to show in the run plan\n  -e, --env KEY=VALUE   Environment variables\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk RANGE          Request the size range of disk for the run. Example\n                        --disk 100GB...\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  --max-duration DURATION\n                        The maximum duration of the run\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nPools:\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n  --instance NAME       Reuse instance from pool with name NAME\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n</code></pre> .gitignore <p>When running anything via CLI, <code>dstack</code> uses the exact version of code from your project directory.</p> <p>If there are large files, consider creating a <code>.gitignore</code> file to exclude them for better performance.</p>"},{"location":"docs/reference/cli/#dstack-ps","title":"dstack ps","text":"<p>This command shows the status of runs.</p> <pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-stop","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p> <pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/#dstack-logs","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p> <pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--replica REPLICA]\n                   [--job JOB]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose\n  -a, --attach          Set up an SSH tunnel, and print logs as they follow.\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --replica REPLICA     The relica number. Defaults to 0.\n  --job JOB             The job number inside the replica. Defaults to 0.\n</code></pre>"},{"location":"docs/reference/cli/#dstack-config","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p> <pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN]\n                     [--default] [--remove] [--no-default]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  --default          Set the project as default. It will be used when\n                     --project is omitted in commands.\n  --remove           Delete project configuration\n  --no-default       Do not prompt to set the project as default\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool","title":"dstack pool","text":"<p>Pools allow for managing the lifecycle of instances and reusing them across runs.  The default pool is created automatically.</p>"},{"location":"docs/reference/cli/#dstack-pool-add","title":"dstack pool add","text":"<p>The <code>dstack pool add</code> command adds an instance to a pool. If no pool name is specified, the instance goes to the default pool.</p> <pre><code>$ dstack pool add --help\nUsage: dstack pool add [-h] [-y] [--profile NAME] [--max-price PRICE]\n                       [-b NAME] [-r NAME] [--instance-type NAME]\n                       [--pool POOL_NAME] [--reuse] [--dont-destroy]\n                       [--idle-duration IDLE_DURATION]\n                       [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                       [--retry | --no-retry | --retry-duration DURATION]\n                       [--cpu SPEC] [--memory SIZE] [--shared-memory SIZE]\n                       [--gpu SPEC] [--disk SIZE]\n\nOptions:\n  -h, --help            show this help message and exit\n  -y, --yes             Don't ask for confirmation\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n\nResources:\n  --cpu SPEC            Request the CPU count. Default: 2..\n  --memory SIZE         Request the size of RAM. The format is SIZE:MB|GB|TB.\n                        Default: 8GB..\n  --shared-memory SIZE  Request the size of Shared Memory. The format is\n                        SIZE:MB|GB|TB.\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk SIZE           Request the size of disk for the run. Example --disk\n                        100GB...\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-ps","title":"dstack pool ps","text":"<p>The <code>dstack pool ps</code> command lists all active instances of a pool. If no pool name is specified, default pool instances are displayed.</p> <pre><code>$ dstack pool ps --help\nUsage: dstack pool ps [-h] [--pool POOL_NAME] [-w]\n\nShow instances in the pool\n\nOptions:\n  -h, --help        show this help message and exit\n  --pool POOL_NAME  The name of the pool. If not set, the default pool will be\n                    used\n  -w, --watch       Watch instances in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-create","title":"dstack pool create","text":"<p>The <code>dstack pool create</code> command creates a new pool.</p> <pre><code>$ dstack pool create --help\nUsage: dstack pool create [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-list","title":"dstack pool list","text":"<p>The <code>dstack pool list</code> lists all existing pools.</p> <pre><code>$ dstack pool delete --help\nUsage: dstack pool delete [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-delete","title":"dstack pool delete","text":"<p>The <code>dstack pool delete</code> command deletes a specified pool.</p> <pre><code>$ dstack pool delete --help\nUsage: dstack pool delete [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway","title":"dstack gateway","text":"<p>A gateway is required for running services. It handles ingress traffic, authorization, domain mapping, model mapping for the OpenAI-compatible endpoint, and so on.</p>"},{"location":"docs/reference/cli/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p> <pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-create","title":"dstack gateway create","text":"<p>The <code>dstack gateway create</code> command creates a new gateway instance in the project.</p> <pre><code>$ dstack gateway create --help\nUsage: dstack gateway create [-h] --backend {aws,azure,gcp,kubernetes}\n                             --region REGION [--set-default] [--name NAME]\n                             --domain DOMAIN\n\nOptions:\n  -h, --help            show this help message and exit\n  --backend {aws,azure,gcp,kubernetes}\n  --region REGION\n  --set-default         Set as default gateway for the project\n  --name NAME           Set a custom name for the gateway\n  --domain DOMAIN       Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p> <pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-update","title":"dstack gateway update","text":"<p>The <code>dstack gateway update</code> command updates the specified gateway.</p> <pre><code>$ dstack gateway update --help\nUsage: dstack gateway update [-h] [--set-default] [--domain DOMAIN] name\n\nPositional Arguments:\n  name             The name of the gateway\n\nOptions:\n  -h, --help       show this help message and exit\n  --set-default    Set it the default gateway for the project\n  --domain DOMAIN  Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#environment-variables","title":"Environment variables","text":"Name Description Default <code>DSTACK_CLI_LOG_LEVEL</code> Configures CLI logging level <code>INFO</code> <code>DSTACK_PROFILE</code> Has the same effect as <code>--profile</code> <code>None</code> <code>DSTACK_PROJECT</code> Has the same effect as <code>--project</code> <code>None</code> <code>DSTACK_DEFAULT_CREDS_DISABLED</code> Disables default credentials detection if set <code>None</code> <code>DSTACK_LOCAL_BACKEND_ENABLED</code> Enables local backend for debug if set <code>None</code> <code>DSTACK_RUNNER_VERSION</code> Sets exact runner version for debug <code>latest</code> <code>DSTACK_SERVER_ADMIN_TOKEN</code> Has the same effect as <code>--token</code> <code>None</code> <code>DSTACK_SERVER_DIR</code> Sets path to store data and server configs <code>~/.dstack/server</code> <code>DSTACK_SERVER_HOST</code> Has the same effect as <code>--host</code> <code>127.0.0.1</code> <code>DSTACK_SERVER_LOG_LEVEL</code> Has the same effect as <code>--log-level</code> <code>INFO</code> <code>DSTACK_SERVER_LOG_FORMAT</code> Sets format of log output. Can be <code>rich</code>, <code>standard</code>, <code>json</code>. <code>rich</code> <code>DSTACK_SERVER_PORT</code> Has the same effect as <code>--port</code> <code>3000</code> <code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> Sets root logger log level <code>ERROR</code> <code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> Sets uvicorn logger log level <code>ERROR</code>"},{"location":"docs/reference/dstack.yml/dev-environment/","title":"dev-environment","text":"<p>The <code>dev-environment</code> configuration type allows running dev environments.</p> <p>Configuration files must have a name ending with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable) and can be located in the project's root directory or any nested folder. Any configuration can be run via <code>dstack run . -f PATH</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses the default Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: dev-environment\n\npython: \"3.11\"\n\nide: vscode\n</code></pre> <p>nvcc</p> <p>Note that the default Docker image doesn't bundle <code>nvcc</code>, which is required for building custom CUDA kernels.  To install it, use <code>conda install cuda</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#docker-image","title":"Docker image","text":"<pre><code>type: dev-environment\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nide: vscode\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry. </p> <pre><code>type: dev-environment\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\nide: vscode\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: dev-environment\n\nide: vscode\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n\n  # Shared memory\n  shm_size: 16GB\n\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU names and their quantity. Examples: <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100),  <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB),  <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#environment-variables","title":"Environment variables","text":"<pre><code>type: dev-environment\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\nide: vscode\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#default-environment-variables","title":"Default environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/reference/dstack.yml/dev-environment/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: dev-environment\n\nide: vscode\n\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for dev environments is <code>on-demand</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: dev-environment\n\nide: vscode\n\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: dev-environment\n\nide: vscode\n\nregions: [eu-west-1, eu-west-2]\n</code></pre> <p>The <code>dev-environment</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ide","title":"<code>ide</code> -  The IDE to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#version","title":"<code>version</code> - (Optional) The version of the IDE.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#init","title":"<code>init</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisionig (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisionig (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisionig (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for re-submitting the run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_price","title":"<code>max_price</code> - (Optional) The maximum price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for termination instances. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-gpu","title":"<code>resources.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The VRAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#total_memory","title":"<code>total_memory</code> - (Optional) The total VRAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-disk","title":"<code>resources.disk</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/service/","title":"service","text":"<p>The <code>service</code> configuration type allows running services.</p> <p>Filename</p> <p>Configuration files must have a name ending with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>serve.dstack.yml</code> are both acceptable) and can be located in the project's root directory or any nested folder. Any configuration can be run via <code>dstack run</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/service/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses the default Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: service\n\npython: \"3.11\"\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n</code></pre> <p>nvcc</p> <p>Note that the default Docker image doesn't bundle <code>nvcc</code>, which is required for building custom CUDA kernels.  To install it, use <code>conda install cuda</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#docker-image","title":"Docker image","text":"<pre><code>type: service\n\nimage: dstackai/base:py3.11-0.4rc4-cuda-12.1\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n</code></pre> Private Docker registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: service\n\nimage: dstackai/base:py3.11-0.4rc4-cuda-12.1\n\ncommands:\n  - python3 -m http.server\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\nport: 8000\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#openai-compatible-interface","title":"OpenAI-compatible interface","text":"<p>By default, if you run a service, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>If you run a model, you can optionally configure the mapping to make it accessible via the  OpenAI-compatible interface.</p> <pre><code>type: service\n\npython: \"3.11\"\n\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nresources:\n  gpu: 24GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre> <p>In this case, with such a configuration, once the service is up, you'll be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface. See services for more detail.</p>"},{"location":"docs/reference/dstack.yml/service/#replicas-and-auto-scaling","title":"Replicas and auto-scaling","text":"<p>By default, <code>dstack</code> runs a single replica of the service. You can configure the number of replicas as well as the auto-scaling policy.</p> <pre><code>type: service\n\npython: \"3.11\"\n\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nresources:\n  gpu: 24GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n\nreplicas: 1..4\nscaling:\n  metric: rps\n  target: 10\n</code></pre> <p>If you specify the minimum number of replicas as <code>0</code>, the service will scale down to zero when there are no requests.</p>"},{"location":"docs/reference/dstack.yml/service/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: service\n\npython: \"3.11\"\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size 2 # Match the number of GPUs\nport: 8000\n\nresources:\n  # 2 GPUs of 80GB\n  gpu: 80GB:2\n\n  disk: 200GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n  format: openai\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU names and their quantity. Examples: <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100),  <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB),  <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#authorization","title":"Authorization","text":"<p>By default, the service endpoint requires the <code>Authorization</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>. Authorization can be disabled by setting <code>auth</code> to <code>false</code>.</p> <pre><code>type: service\n\npython: \"3.11\"\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n\nauth: false\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#environment-variables","title":"Environment variables","text":"<pre><code>type: service\n\npython: \"3.11\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nresources:\n  gpu: 24GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above), <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#default-environment-variables","title":"Default environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/reference/dstack.yml/service/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: service\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for services is <code>auto</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: service\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: service\n\ncommands:\n  - python3 -m http.server\n\nport: 8000\n\nregions: [eu-west-1, eu-west-2]\n</code></pre> <p>The <code>service</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/service/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/service/#port","title":"<code>port</code> -  The port, that application listens on or the mapping.","text":""},{"location":"docs/reference/dstack.yml/service/#model","title":"<code>model</code> - (Optional) Mapping of the model for the OpenAI-compatible endpoint.","text":""},{"location":"docs/reference/dstack.yml/service/#auth","title":"<code>auth</code> - (Optional) Enable the authorization. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#replicas","title":"<code>replicas</code> - (Optional) The range . Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_scaling","title":"<code>scaling</code> - (Optional) The auto-scaling configuration.","text":""},{"location":"docs/reference/dstack.yml/service/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/service/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/service/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/service/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/service/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/service/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/service/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/service/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisionig (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisionig (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisionig (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for re-submitting the run.","text":""},{"location":"docs/reference/dstack.yml/service/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#max_price","title":"<code>max_price</code> - (Optional) The maximum price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/service/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/service/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/service/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for termination instances. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#model_1","title":"<code>model</code>","text":""},{"location":"docs/reference/dstack.yml/service/#type","title":"<code>type</code> -  The type of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The name of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#format","title":"<code>format</code> -  The serving format.","text":""},{"location":"docs/reference/dstack.yml/service/#scaling","title":"<code>scaling</code>","text":""},{"location":"docs/reference/dstack.yml/service/#metric","title":"<code>metric</code> -  The target metric to track.","text":""},{"location":"docs/reference/dstack.yml/service/#target","title":"<code>target</code> -  The target value of the metric.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_up_delay","title":"<code>scale_up_delay</code> - (Optional) The delay in seconds before scaling up. Defaults to <code>300</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_down_delay","title":"<code>scale_down_delay</code> - (Optional) The delay in seconds before scaling down. Defaults to <code>600</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/service/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/service/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/service/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/service/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/service/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The VRAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#total_memory","title":"<code>total_memory</code> - (Optional) The total VRAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/service/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/service/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/service/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/service/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/task/","title":"task","text":"<p>The <code>task</code> configuration type allows running tasks.</p> <p>Configuration files must have a name ending with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable) and can be located in the project's root directory or any nested folder. Any configuration can be run via <code>dstack run . -f PATH</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/task/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses the default Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: task\n\npython: \"3.11\"\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> <p>nvcc</p> <p>Note that the default Docker image doesn't bundle <code>nvcc</code>, which is required for building custom CUDA kernels.  To install it, use <code>conda install cuda</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#_ports","title":"Ports","text":"<p>A task can configure ports. In this case, if the task is running an application on a port, <code>dstack run</code>  will securely allow you to access this port from your local machine through port forwarding.</p> <pre><code>type: task\n\npython: \"3.11\"\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python fine-tuning/qlora/train.py\n\nports:\n  - 6000\n</code></pre> <p>When running it, <code>dstack run</code> forwards <code>6000</code> port to <code>localhost:6000</code>, enabling secure access. See tasks for more detail.</p>"},{"location":"docs/reference/dstack.yml/task/#docker-image","title":"Docker image","text":"<pre><code>type: dev-environment\n\nimage: dstackai/base:py3.11-0.4rc4-cuda-12.1\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: dev-environment\n\nimage: dstackai/base:py3.11-0.4rc4-cuda-12.1\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: task\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n\n  # Shared memory\n  shm_size: 16GB\n\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU names and their quantity. Examples: <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100),  <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB),  <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#environment-variables","title":"Environment variables","text":"<pre><code>type: task\n\npython: \"3.11\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.env</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#default-environment-variables","title":"Default environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run <code>DSTACK_NODES_NUM</code> The number of nodes in the run <code>DSTACK_NODE_RANK</code> The rank of the node <code>DSTACK_MASTER_NODE_IP</code> The internal IP address the master node"},{"location":"docs/reference/dstack.yml/task/#nodes_1","title":"Nodes","text":"<p>By default, the task runs on a single node. However, you can run it on a cluster of nodes.</p> <pre><code>type: task\n\n# The size of the cluster\nnodes: 2\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r requirements.txt\n  - torchrun\n    --nproc_per_node=$DSTACK_GPUS_PER_NODE\n    --node_rank=$DSTACK_NODE_RANK\n    --nnodes=$DSTACK_NODES_NUM\n    --master_addr=$DSTACK_MASTER_NODE_IP\n    --master_port=8008 resnet_ddp.py\n    --num_epochs 20\n\nresources:\n  gpu: 24GB\n</code></pre> <p>If you run the task, <code>dstack</code> first provisions the master node and then runs the other nodes of the cluster. All nodes are provisioned in the same region.</p> <p><code>dstack</code> is easy to use with <code>accelerate</code>, <code>torchrun</code>, and other distributed frameworks. All you need to do is pass the corresponding environment variables such as <code>DSTACK_GPUS_PER_NODE</code>, <code>DSTACK_NODE_RANK</code>, <code>DSTACK_NODES_NUM</code>, <code>DSTACK_MASTER_NODE_IP</code>, and <code>DSTACK_GPUS_NUM</code> (see System environment variables).</p> Backends <p>Running on multiple nodes is supported only with AWS, GCP, and Azure.</p>"},{"location":"docs/reference/dstack.yml/task/#arguments","title":"Arguments","text":"<p>You can parameterize tasks with user arguments using <code>${{ run.args }}</code> in the configuration.</p> <pre><code>type: task\n\npython: \"3.11\"\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py ${{ run.args }}\n</code></pre> <p>Now, you can pass your arguments to the <code>dstack run</code> command.  See tasks for more detail.</p>"},{"location":"docs/reference/dstack.yml/task/#web-applications","title":"Web applications","text":"<p>Here's an example of using <code>ports</code> to run web apps with <code>tasks</code>. </p> <pre><code>type: task\n\npython: \"3.11\"\n\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\n\nports: \n  - 8501\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: task\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for tasks is <code>auto</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: task\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: task\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nregions: [eu-west-1, eu-west-2]\n</code></pre> <p>The <code>task</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/task/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/task/#nodes","title":"<code>nodes</code> - (Optional) Number of nodes. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/task/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/task/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/task/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/task/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/task/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/task/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/task/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/task/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisionig (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisionig (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisionig (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for re-submitting the run.","text":""},{"location":"docs/reference/dstack.yml/task/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#max_price","title":"<code>max_price</code> - (Optional) The maximum price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/task/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/task/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/task/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for termination instances. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/task/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/task/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/task/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/task/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/task/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The VRAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#total_memory","title":"<code>total_memory</code> - (Optional) The total VRAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/task/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object; see examples.","text":""},{"location":"docs/reference/dstack.yml/task/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/task/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/task/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":"<p>The <code>~/.dstack/server/config.yml</code> file is used by the <code>dstack</code> server to configure cloud accounts.</p> <p>Projects</p> <p>For flexibility, <code>dstack</code> server permits you to configure backends for multiple projects.  If you intend to use only one project, name it <code>main</code>.</p>"},{"location":"docs/reference/server/config.yml/#examples","title":"Examples","text":""},{"location":"docs/reference/server/config.yml/#aws_1","title":"AWS","text":"Access keyDefault credentials <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: KKAAUKLIZ5EHKICAOASV\n      secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/reference/server/config.yml/#azure_1","title":"Azure","text":"ClientDefault credentials <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/reference/server/config.yml/#gcp_1","title":"GCP","text":"Service accountDefault credentials <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: service_account\n      filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: default\n</code></pre>"},{"location":"docs/reference/server/config.yml/#lambda_1","title":"Lambda","text":"<pre><code>projects:\n- name: main\n  backends:\n  - type: lambda\n    creds:\n      type: api_key\n      api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/reference/server/config.yml/#tensordock_1","title":"TensorDock","text":"<pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre>"},{"location":"docs/reference/server/config.yml/#vastai_1","title":"Vast.ai","text":"<pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre>"},{"location":"docs/reference/server/config.yml/#cudo","title":"CUDO","text":"<pre><code>projects:\n- name: main\n  backends:\n  - type: cudo\n    project_id: my-cudo-project\n    creds:\n      type: api_key\n      api_key: 7487240a466624b48de22865589\n</code></pre>"},{"location":"docs/reference/server/config.yml/#datacrunch_1","title":"DataCrunch","text":"<pre><code>projects:\n- name: main\n  backends:\n  - type: datacrunch\n    creds:\n      type: api_key\n      client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n      client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/reference/server/config.yml/#kubernetes_1","title":"Kubernetes","text":"Self-managedManaged <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_host: localhost # The external IP address of any node\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>For more details on configuring clouds, please refer to Installation.</p>"},{"location":"docs/reference/server/config.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/server/config.yml/#_projects","title":"<code>projects</code> -  The list of projects.","text":""},{"location":"docs/reference/server/config.yml/#projects","title":"<code>projects[n]</code>","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The name of the project.","text":""},{"location":"docs/reference/server/config.yml/#backends","title":"<code>backends</code> -  The list of backends.","text":""},{"location":"docs/reference/server/config.yml/#aws","title":"<code>projects[n].backends[type=aws]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>aws</code>.","text":""},{"location":"docs/reference/server/config.yml/#vpc_name","title":"<code>vpc_name</code> - (Optional) The VPC name.","text":""},{"location":"docs/reference/server/config.yml/#vpc_ids","title":"<code>vpc_ids</code> - (Optional) The mapping from AWS regions to VPC IDs.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#aws-creds","title":"<code>projects[n].backends[type=aws].creds</code>","text":"Access keyDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>access_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#access_key","title":"<code>access_key</code> -  The access key.","text":""},{"location":"docs/reference/server/config.yml/#secret_key","title":"<code>secret_key</code> -  The secret key.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#azure","title":"<code>projects[n].backends[type=azure]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>azure</code>.","text":""},{"location":"docs/reference/server/config.yml/#tenant_id","title":"<code>tenant_id</code> -  The tenant ID.","text":""},{"location":"docs/reference/server/config.yml/#subscription_id","title":"<code>subscription_id</code> -  The subscription ID.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#azure-creds","title":"<code>projects[n].backends[type=azure].creds</code>","text":"ClientDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>client</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#datacrunch","title":"<code>projects[n].backends[type=datacrunch]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>datacrunch</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#datacrunch-creds","title":"<code>projects[n].backends[type=datacrunch].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#gcp","title":"<code>projects[n].backends[type=gcp]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>gcp</code>.","text":""},{"location":"docs/reference/server/config.yml/#project_id","title":"<code>project_id</code> -  The project ID.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#gcp-creds","title":"<code>projects[n].backends[type=gcp].creds</code>","text":"Service accountDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>service_account</code>.","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"<code>filename</code> -  The path to the service account file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the service account file.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#lambda","title":"<code>projects[n].backends[type=lambda]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>lambda</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#lambda-creds","title":"<code>projects[n].backends[type=lambda].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#tensordock","title":"<code>projects[n].backends[type=tensordock]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>tensordock</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#tensordock-creds","title":"<code>projects[n].backends[type=tensordock].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#api_token","title":"<code>api_token</code> -  The API token.","text":""},{"location":"docs/reference/server/config.yml/#vastai","title":"<code>projects[n].backends[type=vastai]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>vastai</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#vastai-creds","title":"<code>projects[n].backends[type=vastai].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#kubernetes","title":"<code>projects[n].backends[type=kubernetes]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>kubernetes</code>.","text":""},{"location":"docs/reference/server/config.yml/#_kubeconfig","title":"<code>kubeconfig</code> -  The kubeconfig configuration.","text":""},{"location":"docs/reference/server/config.yml/#_networking","title":"<code>networking</code> - (Optional) The networking configuration.","text":""},{"location":"docs/reference/server/config.yml/#kubeconfig","title":"<code>projects[n].backends[type=kubernetes].kubeconfig</code>","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"<code>filename</code> -  The path to the kubeconfig file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the kubeconfig file.","text":""},{"location":"docs/reference/server/config.yml/#networking","title":"<code>projects[n].backends[type=kubernetes].networking</code>","text":""},{"location":"docs/reference/server/config.yml/#ssh_host","title":"<code>ssh_host</code> - (Optional) The external IP address of any node.","text":""},{"location":"docs/reference/server/config.yml/#ssh_port","title":"<code>ssh_port</code> - (Optional) Any port accessible outside of the cluster.","text":""},{"location":"blog/archive/2024/","title":"2024","text":""}]}