{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"terms/","title":"Terms of service","text":""},{"location":"terms/#agreement-to-terms","title":"Agreement to terms","text":"<p>We are dstack GmbH (\"Company,\" \"we,\" \"us,\" \"our\"), a company registered in Germany at Franz-Joseph-Stra\u00dfe, 11, Munich, Bayern 80801.</p> <p>These Legal Terms constitute a legally binding agreement made between you, whether personally or on behalf of an entity (\"you\"), and dstack GmbH, concerning your access to and use of the Services. You agree that by accessing the Services, you have read, understood, and agreed to be bound by all of these Legal Terms. IF YOU DO NOT AGREE WITH ALL OF THESE LEGAL TERMS, THEN YOU ARE EXPRESSLY PROHIBITED FROM USING THE SERVICES AND YOU MUST DISCONTINUE USE IMMEDIATELY.</p> <p>Supplemental terms and conditions or documents that may be posted on the Services from time to time are hereby expressly incorporated herein by reference. We reserve the right, in our sole discretion, to make changes or modifications to these Legal Terms from time to time. We will alert you about any changes by updating the \"Last updated\" date of these Legal Terms, and you waive any right to receive specific notice of each such change. It is your responsibility to periodically review these Legal Terms to stay informed of updates. You will be subject to, and will be deemed to have been made aware of and to have accepted, the changes in any revised Legal Terms by your continued use of the Services after the date such revised Legal Terms are posted.</p>"},{"location":"terms/#1-our-services","title":"1. Our services","text":"<p>The information provided when using the Services is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country. Accordingly, those persons who choose to access the Services from other locations do so on their own initiative and are solely responsible for compliance with local laws, if and to the extent local laws are applicable.</p> <p>The Services are not tailored to comply with industry-specific regulations (Health Insurance Portability and Accountability Act (HIPAA), Federal Information Security Management Act (FISMA), etc.), so if your interactions would be subjected to such laws, you may not use the Services. You may not use the Services in a way that would violate the Gramm-Leach-Bliley Act (GLBA).</p>"},{"location":"terms/#2-intelliectual-property-rights","title":"2. Intelliectual property rights","text":"<p>Our intellectual property</p> <p>We are the owner or the licensee of all intellectual property rights in our Services, including all source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics in the Services ( collectively, the \"Content\"), as well as the trademarks, service marks, and logos contained therein (the \"Marks\").</p> <p>Our Content and Marks are protected by copyright and trademark laws (and various other intellectual property rights and unfair competition laws) and treaties in the United States and around the world.</p> <p>The Content and Marks are provided in or through the Services \"AS IS\" for your personal, non-commercial use or internal business purpose only.</p> <p>Your use of our Services</p> <p>Subject to your compliance with these Legal Terms, including the \"Prohibited activities\" section below, we grant you a non-exclusive, non-transferable, revocable license to:</p> <ul> <li>access the Services; and</li> <li>download or print a copy of any portion of the Content to which you have properly gained access. solely for your personal, non-commercial use or internal business purpose.</li> </ul> <p>Except as set out in this section or elsewhere in our Legal Terms, no part of the Services and no Content or Marks may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>If you wish to make any use of the Services, Content, or Marks other than as set out in this section or elsewhere in our Legal Terms, please address your request to: hello@dstack.ai. If we ever grant you the permission to post, reproduce, or publicly display any part of our Services or Content, you must identify us as the owners or licensors of the Services, Content, or Marks and ensure that any copyright or proprietary notice appears or is visible on posting, reproducing, or displaying our Content.</p> <p>We reserve all rights not expressly granted to you in and to the Services, Content, and Marks.</p> <p>Any breach of these Intellectual Property Rights will constitute a material breach of our Legal Terms and your right to use our Services will terminate immediately.</p> <p>Your submissions</p> <p>Please review this section and the \"Prohibited activities\" section carefully prior to using our Services to understand the (a) rights you give us and (b) obligations you have when you post or upload any content through the Services.</p> <p>Submissions: By directly sending us any question, comment, suggestion, idea, feedback, or other information about the Services (\"Submissions\"), you agree to assign to us all intellectual property rights in such Submission. You agree that we shall own this Submission and be entitled to its unrestricted use and dissemination for any lawful purpose, commercial or otherwise, without acknowledgment or compensation to you.</p> <p>You are responsible for what you post or upload: By sending us Submissions through any part of the Services you: * confirm that you have read and agree with our \"Prohibited activities\" and will not post, send, publish, upload, or * transmit through the Services any Submission that is illegal, harassing, hateful, harmful, defamatory, obscene, * bullying, abusive, discriminatory, threatening to any person or group, sexually explicit, false, inaccurate, deceitful,   or misleading; * to the extent permissible by applicable law, waive any and all moral rights to any such Submission; * warrant that any such Submission are original to you or that you have the necessary rights and licenses to submit such    Submissions and that you have full authority to grant us the above-mentioned rights in relation to your Submissions; and * warrant and represent that your Submissions do not constitute confidential information.</p> <p>You are solely responsible for your Submissions and you expressly agree to reimburse us for any and all losses that we may suffer because of your breach of (a) this section, (b) any third party\u2019s intellectual property rights, or (c) applicable law.</p>"},{"location":"terms/#3-user-representations","title":"3. User representations","text":"<p>By using the Services, you represent and warrant that: (1) all registration information you submit will be true, accurate, current, and complete; (2) you will maintain the accuracy of such information and promptly update such registration information as necessary; (3) you have the legal capacity and you agree to comply with these Legal Terms; ( 4) you are not a minor in the jurisdiction in which you reside; (5) you will not access the Services through automated or non-human means, whether through a bot, script or otherwise; (6) you will not use the Services for any illegal or unauthorized purpose; and (7) your use of the Services will not violate any applicable law or regulation.</p> <p>If you provide any information that is untrue, inaccurate, not current, or incomplete, we have the right to suspend or terminate your account and refuse any and all current or future use of the Services (or any portion thereof).</p>"},{"location":"terms/#4-user-registration","title":"4. User registration","text":"<p>You may be required to register to use the Services. You agree to keep your password confidential and will be responsible for all use of your account and password. We reserve the right to remove, reclaim, or change a username you select if we determine, in our sole discretion, that such username is inappropriate, obscene, or otherwise objectionable.</p>"},{"location":"terms/#5-purchases-and-payment","title":"5. Purchases and payment","text":"<p>We accept the following forms of payment:</p> <ul> <li>Visa</li> <li>Mastercard</li> </ul> <p>You agree to provide current, complete, and accurate purchase and account information for all purchases made via the Services. You further agree to promptly update account and payment information, including email address, payment method, and payment card expiration date, so that we can complete your transactions and contact you as needed. Sales tax will be added to the price of purchases as deemed required by us. We may change prices at any time. All payments shall be in US dollars.</p> <p>You agree to pay all charges at the prices then in effect for your purchases and any applicable shipping fees, and you authorize us to charge your chosen payment provider for any such amounts upon placing your order. We reserve the right to correct any errors or mistakes in pricing, even if we have already requested or received payment.</p> <p>We reserve the right to refuse any order placed through the Services. We may, in our sole discretion, limit or cancel quantities purchased per person, per household, or per order. These restrictions may include orders placed by or under the same customer account, the same payment method, and/or orders that use the same billing or shipping address. We reserve the right to limit or prohibit orders that, in our sole judgment, appear to be placed by dealers, resellers, or distributors.</p>"},{"location":"terms/#6-subscriptions","title":"6. Subscriptions","text":"<p>Billing and Renewal</p> <p>e.g. by topping up their balance manually using their credit card.</p> <p>Cancellation</p> <p>You can cancel your subscription at any time by contacting us using the contact information provided below. Your cancellation will take effect at the end of the current paid term. If you have any questions or are unsatisfied with our Services, please email us at hello@dstack.ai .</p> <p>Fee Changes</p> <p>We may, from time to time, make changes to the subscription fee and will communicate any price changes to you in accordance with applicable law.</p>"},{"location":"terms/#7-software","title":"7. Software","text":"<p>We may include software for use in connection with our Services. If such software is accompanied by an end user license agreement (\"EULA\"), the terms of the EULA will govern your use of the software. If such software is not accompanied by a EULA, then we grant to you a non-exclusive, revocable, personal, and non-transferable license to use such software solely in connection with our services and in accordance with these Legal Terms. Any software and any related documentation is provided \"AS IS\" without warranty of any kind, either express or implied, including, without limitation, the implied warranties of merchantability, fitness for a particular purpose, or non-infringement. You accept any and all risk arising out of use or performance of any software. You may not reproduce or redistribute any software except in accordance with the EULA or these Legal Terms.</p>"},{"location":"terms/#8-prohibited-activities","title":"8. Prohibited activities","text":"<p>You may not access or use the Services for any purpose other than that for which we make the Services available. The Services may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us.</p> <p>As a user of the Services, you agree not to:</p> <ul> <li>Systematically retrieve data or other content from the Services to create or compile, directly or indirectly, a   collection, compilation, database, or directory without written permission from us.</li> <li>Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such   as user passwords.</li> <li>Circumvent, disable, or otherwise interfere with security-related features of the Services, including features that   prevent or restrict the use or copying of any Content or enforce limitations on the use of the Services and/or the   Content contained therein.</li> <li>Disparage, tarnish, or otherwise harm, in our opinion, us and/or the Services.</li> <li>Use any information obtained from the Services in order to harass, abuse, or harm another person.</li> <li>Make improper use of our support services or submit false reports of abuse or misconduct.</li> <li>Use the Services in a manner inconsistent with any applicable laws or regulations.</li> <li>Engage in unauthorized framing of or linking to the Services.</li> <li>Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material, including   excessive use of capital letters and spamming (continuous posting of repetitive text), that interferes with any   party\u2019s uninterrupted use and enjoyment of the Services or modifies, impairs, disrupts, alters, or interferes with the   use, features, functions, operation, or maintenance of the Services.</li> <li>Engage in any automated use of the system, such as using scripts to send comments or messages, or using any data   mining, robots, or similar data gathering and extraction tools.</li> <li>Delete the copyright or other proprietary rights notice from any Content.</li> <li>Attempt to impersonate another user or person or use the username of another user.</li> <li>Upload or transmit (or attempt to upload or to transmit) any material that acts as a passive or active information   collection or transmission mechanism, including without limitation, clear graphics interchange formats (\"gifs\"), 1\u00d71   pixels, web bugs, cookies, or other similar devices (sometimes referred to as \"spyware\" or \"passive collection   mechanisms\" or \"pcms\").</li> <li>Interfere with, disrupt, or create an undue burden on the Services or the networks or services connected to the   Services.</li> <li>Harass, annoy, intimidate, or threaten any of our employees or agents engaged in providing any portion of the Services   to you.</li> <li>Attempt to bypass any measures of the Services designed to prevent or restrict access to the Services, or any portion   of the Services.</li> <li>Copy or adapt the Services' software, including but not limited to Flash, PHP, HTML, JavaScript, or other code.</li> <li>Except as permitted by applicable law, decipher, decompile, disassemble, or reverse engineer any of the software   comprising or in any way making up a part of the Services.</li> <li>Except as may be the result of standard search engine or Internet browser usage, use, launch, develop, or distribute   any automated system, including without limitation, any spider, robot, cheat utility, scraper, or offline reader that   accesses the Services, or use or launch any unauthorized script or other software.</li> <li>Use a buying agent or purchasing agent to make purchases on the Services.</li> <li>Make any unauthorized use of the Services, including collecting usernames and/or email addresses of users by   electronic or other means for the purpose of sending unsolicited email, or creating user accounts by automated means   or under false pretenses.</li> <li>Use the Services as part of any effort to compete with us or otherwise use the Services and/or the Content for any   revenue-generating endeavor or commercial enterprise.</li> </ul>"},{"location":"terms/#9-user-generated-contributions","title":"9. User generated contributions","text":"<p>The Services does not offer users to submit or post content.</p>"},{"location":"terms/#10-contribution-license","title":"10. Contribution license","text":"<p>You and Services agree that we may access, store, process, and use any information and personal data that you provide following the terms of the Privacy Policy and your choices (including settings).</p> <p>By submitting suggestions or other feedback regarding the Services, you agree that we can use and share such feedback for any purpose without compensation to you.</p>"},{"location":"terms/#11-social-media","title":"11. Social media","text":"<p>As part of the functionality of the Services, you may link your account with online accounts you have with third-party service providers (each such account, a \"Third-Party Account\") by either: (1) providing your Third-Party Account login information through the Services; or (2) allowing us to access your Third-Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third-Party Account. You represent and warrant that you are entitled to disclose your Third-Party Account login information to us and/or grant us access to your Third-Party Account, without breach by you of any of the terms and conditions that govern your use of the applicable Third-Party Account, and without obligating us to pay any fees or making us subject to any usage limitations imposed by the third-party service provider of the Third-Party Account. By granting us access to any Third-Party Accounts, you understand that (1) we may access, make available, and store (if applicable) any content that you have provided to and stored in your Third-Party Account (the \"Social Network Content\") so that it is available on and through the Services via your account, including without limitation any friend lists and (2) we may submit to and receive from your Third-Party Account additional information to the extent you are notified when you link your account with the Third-Party Account. Depending on the Third-Party Accounts you choose and subject to the privacy settings that you have set in such Third-Party Accounts, personally identifiable information that you post to your Third-Party Accounts may be available on and through your account on the Services. Please note that if a Third-Party Account or associated service becomes unavailable or our access to such Third-Party Account is terminated by the third-party service provider, then Social Network Content may no longer be available on and through the Services. You will have the ability to disable the connection between your account on the Services and your Third-Party Accounts at any time. PLEASE NOTE THAT YOUR RELATIONSHIP WITH THE THIRD-PARTY SERVICE PROVIDERS ASSOCIATED WITH YOUR THIRD-PARTY ACCOUNTS IS GOVERNED SOLELY BY YOUR AGREEMENT(S) WITH SUCH THIRD-PARTY SERVICE PROVIDERS. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality, or non-infringement, and we are not responsible for any Social Network Content. You acknowledge and agree that we may access your email address book associated with a Third-Party Account and your contacts list stored on your mobile device or tablet computer solely for purposes of identifying and informing you of those contacts who have also registered to use the Services. You can deactivate the connection between the Services and your Third-Party Account by contacting us using the contact information below or through your account settings (if applicable). We will attempt to delete any information stored on our servers that was obtained through such Third-Party Account, except the username and profile picture that become associated with your account.</p>"},{"location":"terms/#12-third-party-websites-and-content","title":"12. Third-party websites and content","text":"<p>The Services may contain (or you may be sent via the Site) links to other websites (\"Third-Party Websites\") as well as articles, photographs, text, graphics, pictures, designs, music, sound, video, information, applications, software, and other content or items belonging to or originating from third parties (\"Third-Party Content\"). Such Third-Party Websites and Third-Party Content are not investigated, monitored, or checked for accuracy, appropriateness, or completeness by us, and we are not responsible for any Third-Party Websites accessed through the Services or any Third-Party Content posted on, available through, or installed from the Services, including the content, accuracy, offensiveness, opinions, reliability, privacy practices, or other policies of or contained in the Third-Party Websites or the Third-Party Content. Inclusion of, linking to, or permitting the use or installation of any Third-Party Websites or any Third-Party Content does not imply approval or endorsement thereof by us. If you decide to leave the Services and access the Third-Party Websites or to use or install any Third-Party Content, you do so at your own risk, and you should be aware these Legal Terms no longer govern. You should review the applicable terms and policies, including privacy and data gathering practices, of any website to which you navigate from the Services or relating to any applications you use or install from the Services. Any purchases you make through Third-Party Websites will be through other websites and from other companies, and we take no responsibility whatsoever in relation to such purchases which are exclusively between you and the applicable third party. You agree and acknowledge that we do not endorse the products or services offered on Third-Party Websites and you shall hold us blameless from any harm caused by your purchase of such products or services. Additionally, you shall hold us blameless from any losses sustained by you or harm caused to you relating to or resulting in any way from any Third-Party Content or any contact with Third-Party Websites.</p>"},{"location":"terms/#13-services-management","title":"13. Services management","text":"<p>We reserve the right, but not the obligation, to: (1) monitor the Services for violations of these Legal Terms; (2) take appropriate legal action against anyone who, in our sole discretion, violates the law or these Legal Terms, including without limitation, reporting such user to law enforcement authorities; (3) in our sole discretion and without limitation, refuse, restrict access to, limit the availability of, or disable (to the extent technologically feasible) any of your Contributions or any portion thereof; (4) in our sole discretion and without limitation, notice, or liability, to remove from the Services or otherwise disable all files and content that are excessive in size or are in any way burdensome to our systems; and (5) otherwise manage the Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Services.</p>"},{"location":"terms/#14-privacy-policy","title":"14. Privacy policy","text":"<p>We care about data privacy and security. Please review our Privacy Policy. By using the Services, you agree to be bound by our Privacy Policy, which is incorporated into these Legal Terms. Please be advised the Services are hosted in Germany and United States. If you access the Services from any other region of the world with laws or other requirements governing personal data collection, use, or disclosure that differ from applicable laws in Germany and United States, then through your continued use of the Services, you are transferring your data to Germany and United States, and you expressly consent to have your data transferred to and processed in Germany and United States.</p>"},{"location":"terms/#15-term-and-termination","title":"15. Term and termination","text":"<p>These Legal Terms shall remain in full force and effect while you use the Services. WITHOUT LIMITING ANY OTHER PROVISION OF THESE LEGAL TERMS, WE RESERVE THE RIGHT TO, IN OUR SOLE DISCRETION AND WITHOUT NOTICE OR LIABILITY, DENY ACCESS TO AND USE OF THE SERVICES (INCLUDING BLOCKING CERTAIN IP ADDRESSES), TO ANY PERSON FOR ANY REASON OR FOR NO REASON, INCLUDING WITHOUT LIMITATION FOR BREACH OF ANY REPRESENTATION, WARRANTY, OR COVENANT CONTAINED IN THESE LEGAL TERMS OR OF ANY APPLICABLE LAW OR REGULATION. WE MAY TERMINATE YOUR USE OR PARTICIPATION IN THE SERVICES OR DELETE YOUR ACCOUNT AND ANY CONTENT OR INFORMATION THAT YOU POSTED AT ANY TIME, WITHOUT WARNING, IN OUR SOLE DISCRETION.</p> <p>If we terminate or suspend your account for any reason, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p>"},{"location":"terms/#16-modifications-and-interruptions","title":"16. Modifications and interruptions","text":"<p>We reserve the right to change, modify, or remove the contents of the Services at any time or for any reason at our sole discretion without notice. However, we have no obligation to update any information on our Services. We will not be liable to you or any third party for any modification, price change, suspension, or discontinuance of the Services.</p> <p>We cannot guarantee the Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Services, resulting in interruptions, delays, or errors. We reserve the right to change, revise, update, suspend, discontinue, or otherwise modify the Services at any time or for any reason without notice to you. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Services during any downtime or discontinuance of the Services. Nothing in these Legal Terms will be construed to obligate us to maintain and support the Services or to supply any corrections, updates, or releases in connection therewith.</p>"},{"location":"terms/#17-governing-law","title":"17. Governing law","text":"<p>These Legal Terms are governed by and interpreted following the laws of Germany, and the use of the United Nations Convention of Contracts for the International Sales of Goods is expressly excluded. If your habitual residence is in the EU, and you are a consumer, you additionally possess the protection provided to you by obligatory provisions of the law in your country to residence. dstack GmbH and yourself both agree to submit to the non-exclusive jurisdiction of the courts of Bayern, which means that you may make a claim to defend your consumer protection rights in regards to these Legal Terms in Germany, or in the EU country in which you reside.</p>"},{"location":"terms/#18-dispute-resolution","title":"18. Dispute resolution","text":"<p>Informal Negotiations</p> <p>To expedite resolution and control the cost of any dispute, controversy, or claim related to these Legal Terms (each a \" Dispute\" and collectively, the \"Disputes\") brought by either you or us (individually, a \"Party\" and collectively, the \" Parties\"), the Parties agree to first attempt to negotiate any Dispute (except those Disputes expressly provided below) informally for at least thirty (30) days before initiating arbitration. Such informal negotiations commence upon written notice from one Party to the other Party.</p> <p>Binding Arbitration</p> <p>Any dispute arising from the relationships between the Parties to these Legal Terms shall be determined by one arbitrator who will be chosen in accordance with the Arbitration and Internal Rules of the European Court of Arbitration being part of the European Centre of Arbitration having its seat in Strasbourg, and which are in force at the time the application for arbitration is filed, and of which adoption of this clause constitutes acceptance. The seat of arbitration shall be Munich , Germany . The language of the proceedings shall be German . Applicable rules of substantive law shall be the law of Germany .</p> <p>Restrictions</p> <p>The Parties agree that any arbitration shall be limited to the Dispute between the Parties individually. To the full extent permitted by law, (a) no arbitration shall be joined with any other proceeding; (b) there is no right or authority for any Dispute to be arbitrated on a class-action basis or to utilize class action procedures; and (c) there is no right or authority for any Dispute to be brought in a purported representative capacity on behalf of the general public or any other persons.</p> <p>Exceptions to Informal Negotiations and Arbitration</p> <p>The Parties agree that the following Disputes are not subject to the above provisions concerning informal negotiations binding arbitration: (a) any Disputes seeking to enforce or protect, or concerning the validity of, any of the intellectual property rights of a Party; (b) any Dispute related to, or arising from, allegations of theft, piracy, invasion of privacy, or unauthorized use; and (c) any claim for injunctive relief. If this provision is found to be illegal or unenforceable, then neither Party will elect to arbitrate any Dispute falling within that portion of this provision found to be illegal or unenforceable and such Dispute shall be decided by a court of competent jurisdiction within the courts listed for jurisdiction above, and the Parties agree to submit to the personal jurisdiction of that court.</p>"},{"location":"terms/#19-corrections","title":"19. Corrections","text":"<p>There may be information on the Services that contains typographical errors, inaccuracies, or omissions, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information on the Services at any time, without prior notice.</p>"},{"location":"terms/#20-disclaimer","title":"20. Disclaimer","text":"<p>THE SERVICES ARE PROVIDED ON AN AS-IS AND AS-AVAILABLE BASIS. YOU AGREE THAT YOUR USE OF THE SERVICES WILL BE AT YOUR SOLE RISK. TO THE FULLEST EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED, IN CONNECTION WITH THE SERVICES AND YOUR USE THEREOF, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. WE MAKE NO WARRANTIES OR REPRESENTATIONS ABOUT THE ACCURACY OR COMPLETENESS OF THE SERVICES' CONTENT OR THE CONTENT OF ANY WEBSITES OR MOBILE APPLICATIONS LINKED TO THE SERVICES AND WE WILL ASSUME NO LIABILITY OR RESPONSIBILITY FOR ANY (1) ERRORS, MISTAKES, OR INACCURACIES OF CONTENT AND MATERIALS, (2) PERSONAL INJURY OR PROPERTY DAMAGE, OF ANY NATURE WHATSOEVER, RESULTING FROM YOUR ACCESS TO AND USE OF THE SERVICES, (3) ANY UNAUTHORIZED ACCESS TO OR USE OF OUR SECURE SERVERS AND/OR ANY AND ALL PERSONAL INFORMATION AND/OR FINANCIAL INFORMATION STORED THEREIN, (4) ANY INTERRUPTION OR CESSATION OF TRANSMISSION TO OR FROM THE SERVICES, (5) ANY BUGS, VIRUSES, TROJAN HORSES, OR THE LIKE WHICH MAY BE TRANSMITTED TO OR THROUGH THE SERVICES BY ANY THIRD PARTY, AND/OR (6) ANY ERRORS OR OMISSIONS IN ANY CONTENT AND MATERIALS OR FOR ANY LOSS OR DAMAGE OF ANY KIND INCURRED AS A RESULT OF THE USE OF ANY CONTENT POSTED, TRANSMITTED, OR OTHERWISE MADE AVAILABLE VIA THE SERVICES. WE DO NOT WARRANT, ENDORSE, GUARANTEE, OR ASSUME RESPONSIBILITY FOR ANY PRODUCT OR SERVICE ADVERTISED OR OFFERED BY A THIRD PARTY THROUGH THE SERVICES, ANY HYPERLINKED WEBSITE, OR ANY WEBSITE OR MOBILE APPLICATION FEATURED IN ANY BANNER OR OTHER ADVERTISING, AND WE WILL NOT BE A PARTY TO OR IN ANY WAY BE RESPONSIBLE FOR MONITORING ANY TRANSACTION BETWEEN YOU AND ANY THIRD-PARTY PROVIDERS OF PRODUCTS OR SERVICES. AS WITH THE PURCHASE OF A PRODUCT OR SERVICE THROUGH ANY MEDIUM OR IN ANY ENVIRONMENT, YOU SHOULD USE YOUR BEST JUDGMENT AND EXERCISE CAUTION WHERE APPROPRIATE.</p>"},{"location":"terms/#21-limitations-of-liability","title":"21. Limitations of liability","text":"<p>IN NO EVENT WILL WE OR OUR DIRECTORS, EMPLOYEES, OR AGENTS BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY DIRECT, INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL, OR PUNITIVE DAMAGES, INCLUDING LOST PROFIT, LOST REVENUE, LOSS OF DATA, OR OTHER DAMAGES ARISING FROM YOUR USE OF THE SERVICES, EVEN IF WE HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, OUR LIABILITY TO YOU FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM OF THE ACTION, WILL AT ALL TIMES BE LIMITED TO THE LESSER OF THE AMOUNT PAID, IF ANY, BY YOU TO US DURING THE zero (0) MONTH PERIOD PRIOR TO ANY CAUSE OF ACTION ARISING OR $0.00 USD. CERTAIN US STATE LAWS AND INTERNATIONAL LAWS DO NOT ALLOW LIMITATIONS ON IMPLIED WARRANTIES OR THE EXCLUSION OR LIMITATION OF CERTAIN DAMAGES. IF THESE LAWS APPLY TO YOU, SOME OR ALL OF THE ABOVE DISCLAIMERS OR LIMITATIONS MAY NOT APPLY TO YOU, AND YOU MAY HAVE ADDITIONAL RIGHTS.</p>"},{"location":"terms/#22-indemnification","title":"22. Indemnification","text":"<p>You agree to defend, indemnify, and hold us harmless, including our subsidiaries, affiliates, and all of our respective officers, agents, partners, and employees, from and against any loss, damage, liability, claim, or demand, including reasonable attorneys\u2019 fees and expenses, made by any third party due to or arising out of: (1) use of the Services; (2) breach of these Legal Terms; (3) any breach of your representations and warranties set forth in these Legal Terms; (4) your violation of the rights of a third party, including but not limited to intellectual property rights; or (5) any overt harmful act toward any other user of the Services with whom you connected via the Services. Notwithstanding the foregoing, we reserve the right, at your expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate, at your expense, with our defense of such claims. We will use reasonable efforts to notify you of any such claim, action, or proceeding which is subject to this indemnification upon becoming aware of it.</p>"},{"location":"terms/#23-user-data","title":"23. User data","text":"<p>We will maintain certain data that you transmit to the Services for the purpose of managing the performance of the Services, as well as data relating to your use of the Services. Although we perform regular routine backups of data, you are solely responsible for all data that you transmit or that relates to any activity you have undertaken using the Services. You agree that we shall have no liability to you for any loss or corruption of any such data, and you hereby waive any right of action against us arising from any such loss or corruption of such data.</p>"},{"location":"terms/#24-electronic-communications-transactions-and-signatures","title":"24. Electronic communications, transactions, and signatures","text":"<p>Visiting the Services, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications, and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Services, satisfy any legal requirement that such communication be in writing. YOU HEREBY AGREE TO THE USE OF ELECTRONIC SIGNATURES, CONTRACTS, ORDERS, AND OTHER RECORDS, AND TO ELECTRONIC DELIVERY OF NOTICES, POLICIES, AND RECORDS OF TRANSACTIONS INITIATED OR COMPLETED BY US OR VIA THE SERVICES. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances, or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by any means other than electronic means.</p>"},{"location":"terms/#25-california-users-and-residents","title":"25. California users and residents","text":"<p>If any complaint with us is not satisfactorily resolved, you can contact the Complaint Assistance Unit of the Division of Consumer Services of the California Department of Consumer Affairs in writing at 1625 North Market Blvd., Suite N 112, Sacramento, California 95834 or by telephone at (800) 952-5210 or (916) 445-1254.</p>"},{"location":"terms/#26-miscellaneous","title":"26. Miscellaneous","text":"<p>These Legal Terms and any policies or operating rules posted by us on the Services or in respect to the Services constitute the entire agreement and understanding between you and us. Our failure to exercise or enforce any right or provision of these Legal Terms shall not operate as a waiver of such right or provision. These Legal Terms operate to the fullest extent permissible by law. We may assign any or all of our rights and obligations to others at any time. We shall not be responsible or liable for any loss, damage, delay, or failure to act caused by any cause beyond our reasonable control. If any provision or part of a provision of these Legal Terms is determined to be unlawful, void, or unenforceable, that provision or part of the provision is deemed severable from these Legal Terms and does not affect the validity and enforceability of any remaining provisions. There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Legal Terms or use of the Services. You agree that these Legal Terms will not be construed against us by virtue of having drafted them. You hereby waive any and all defenses you may have based on the electronic form of these Legal Terms and the lack of signing by the parties hereto to execute these Legal Terms.</p>"},{"location":"terms/#27-contact-us","title":"27. Contact us","text":"<p>In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us at hello@dstack.ai.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/archive/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/amd-on-runpod/","title":"Supporting AMD accelerators on RunPod","text":"<p>While <code>dstack</code> helps streamline the orchestration of containers for AI, its primary goal is to offer vendor independence and portability, ensuring compatibility across different hardware and cloud providers.</p> <p>Inspired by the recent <code>MI300X</code> benchmarks, we are pleased to announce that RunPod is the first cloud provider to offer AMD GPUs through <code>dstack</code>, with support for other cloud providers and on-prem servers to follow.</p>"},{"location":"blog/amd-on-runpod/#specification","title":"Specification","text":"<p>For the reference, below is a comparison of the <code>MI300X</code> and <code>H100 SXM</code> specs, incl. the prices offered by RunPod.</p> MI300X H100X SXM On-demand pricing $3.99/hr $3.99/hr VRAM 192 GB 80GB Memory bandwidth 5.3 TB/s 3.4TB/s FP16 2,610 TFLOPs 1,979 TFLOPs FP8 5,220 TFLOPs 3,958 TFLOPs <p>One of the main advantages of the <code>MI300X</code> is its VRAM. For example, with the <code>H100 SXM</code>, you wouldn't be able to fit the FP16 version of Llama 3.1 405B into a single node with 8 GPUs\u2014you'd have to use FP8 instead. However, with the <code>MI300X</code>, you can fit FP16 into a single node with 8 GPUs, and for FP8, you'd only need 4 GPUs.</p> <p>With the latest update , you can now specify an AMD GPU under <code>resources</code>. Below are a few examples.</p>"},{"location":"blog/amd-on-runpod/#configuration","title":"Configuration","text":"ServiceDev environment <p>Here's an example of a service that deploys Llama 3.1 70B in FP16 using TGI .</p> <p> <pre><code>type: service\nname: amd-service-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct\n  - TRUST_REMOTE_CODE=true\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n\nspot_policy: auto\n\nmodel:\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-70B-Instruct\n  format: openai\n</code></pre> <p>Here's an example of a dev environment using TGI 's Docker image:</p> <pre><code>type: dev-environment\nname: amd-dev-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\nide: vscode\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n\nspot_policy: auto\n</code></pre> <p>Docker image</p> <p>Please note that if you want to use AMD, specifying <code>image</code> is currently required. This must be an image that includes ROCm drivers.</p> <p>To request multiple GPUs, specify the quantity after the GPU name, separated by a colon, e.g., <code>MI300X:4</code>.</p> <p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p> Control plane <p>If you specify <code>model</code> when running a service, <code>dstack</code> will automatically register the model on the gateway's global endpoint and allow you to use it for chat via the control plane UI.</p> <p></p>"},{"location":"blog/amd-on-runpod/#whats-next","title":"What's next?","text":"<ol> <li>The examples above demonstrate the use of TGI .  AMD accelerators can also be used with other frameworks like vLLM, Ollama, etc., and we'll be adding more examples soon.</li> <li>RunPod is the first cloud provider where dstack supports AMD. More cloud providers will be supported soon as well.</li> <li>Want to give RunPod and <code>dstack</code> a try? Make sure you've signed up for RunPod ,     then set up the <code>dstack server</code>. </li> </ol> <p>Have questioned or feedback? Join our Discord   server.</p>"},{"location":"blog/dstack-sky-own-cloud-accounts/","title":"dstack Sky now supports your own cloud accounts","text":"<p>dstack Sky   enables you to access GPUs from the global marketplace at the most competitive rates. However, sometimes you may want to use your own cloud accounts.  With today's release, both options are now supported.</p> <p></p>"},{"location":"blog/dstack-sky-own-cloud-accounts/#configure-backends","title":"Configure backends","text":"<p>To use your own cloud account, open the project settings and edit the corresponding backend.</p> <p></p> <p>You can configure your cloud accounts for any of the supported providers, including AWS, GCP, Azure, TensorDock, Lambda, CUDO, RunPod, and Vast.ai.</p> <p>Additionally, you can disable certain backends if you do not plan to use them.</p> <p>Typically, if you prefer using your own cloud accounts, it's recommended that you use the  open-source version  of <code>dstack</code>. However, if you prefer not to host it yourself, now you can use <code>dstack Sky</code>  with your own cloud accounts as well.</p> <p>Seeking the cheapest on-demand and spot cloud GPUs? dstack Sky  has you covered!</p> <p>Need help, have a question, or just want to stay updated?</p> <p>Join Discord</p>"},{"location":"blog/dstack-sky/","title":"Introducing dstack Sky","text":"<p>Today we're previewing <code>dstack Sky</code>, a service built on top of  <code>dstack</code> that enables you to get GPUs at competitive rates from a wide pool of providers.</p> <p></p>"},{"location":"blog/dstack-sky/#tldr","title":"TL;DR","text":"<ul> <li>GPUs at competitive rates from multiple providers</li> <li>No need for your own cloud accounts</li> <li>Compatible with <code>dstack</code>'s CLI and API</li> <li>A pre-configured gateway for deploying services</li> </ul>"},{"location":"blog/dstack-sky/#introduction","title":"Introduction","text":"<p><code>dstack</code> is an open-source tool designed for managing AI infrastructure across various cloud platforms. It's lighter and more specifically geared towards AI tasks compared to Kubernetes.</p> <p>Due to its support for multiple cloud providers, <code>dstack</code> is frequently used to access on-demand and spot GPUs  across multiple clouds.  From our users, we've learned that managing various cloud accounts, quotas, and billing can be cumbersome.</p> <p>To streamline this process, we introduce <code>dstack Sky</code>, a managed service that enables users to access GPUs from multiple providers through <code>dstack</code> \u2013 without needing an account in each cloud provider. </p>"},{"location":"blog/dstack-sky/#what-is-dstack-sky","title":"What is dstack Sky?","text":"<p>Instead of running <code>dstack server</code> yourself, you point <code>dstack config</code> to a project set up with <code>dstack Sky</code>.</p> <pre><code>$ dstack config --url https://sky.dstack.ai \\\n    --project my-awesome-project \\\n    --token ca1ee60b-7b3f-8943-9a25-6974c50efa75\n</code></pre> <p>Now, you can use <code>dstack</code>'s CLI or API \u2013 just like you would with your own cloud accounts.</p> <pre><code>$ dstack run . -b tensordock -b vastai\n\n #  BACKEND     REGION  RESOURCES                    SPOT  PRICE \n 1  vastai      canada  16xCPU/64GB/1xRTX4090/1TB    no    $0.35\n 2  vastai      canada  16xCPU/64GB/1xRTX4090/400GB  no    $0.34\n 3  tensordock  us      8xCPU/48GB/1xRTX4090/480GB   no    $0.74\n    ...\n Shown 3 of 50 offers, $0.7424 max\n\nContinue? [y/n]:\n</code></pre> <p>Backends</p> <p><code>dstack Sky</code> supports the same backends as the open-source version, except that you don't need to set them up. By default, it uses all supported backends.</p> <p>You can use both on-demand and spot instances without needing to manage quotas, as they are automatically handled for you.</p> <p>With <code>dstack Sky</code> you can use all of <code>dstack</code>'s features, incl. dev environments,  tasks, services, and  fleets.</p> <p>To use services, the open-source version requires setting up a gateway with your own domain.  <code>dstack Sky</code> comes with a pre-configured gateway.</p> <pre><code>$ dstack gateway list\n BACKEND  REGION     NAME    ADDRESS       DOMAIN                            DEFAULT\n aws      eu-west-1  dstack  3.252.79.143  my-awesome-project.sky.dstack.ai  \u2713\n</code></pre> <p>If you run it with <code>dstack Sky</code>, the service's endpoint will be available at <code>https://&lt;run name&gt;.&lt;project name&gt;.sky.dstack.ai</code>.</p> <p>Let's say we define a service:</p> <pre><code>type: service\n# Deploys Mixtral 8x7B with Ollama\n\n# Serve model using Ollama's Docker image\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull mixtral\n  - fg\nport: 11434\n\n# Configure hardware requirements\nresources:\n  gpu: 48GB..80GB\n\n# Enable OpenAI compatible endpoint\nmodel:\n  type: chat\n  name: mixtral\n  format: openai\n</code></pre> <p>If it has a <code>model</code> mapping, the model will be accessible at <code>https://gateway.&lt;project name&gt;.sky.dstack.ai</code> via the OpenAI compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.&lt;project name&gt;.sky.dstack.ai\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mixtral\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Now, you can choose \u2014 either use <code>dstack</code> via the open-source version or via <code>dstack Sky</code>,  or even use them side by side.</p> <p>Credits</p> <p>Are you an active contributor to the AI community? Request free <code>dstack Sky</code> credits.</p> <p><code>dstack Sky</code>  is live on Product Hunt. Support it by giving it your vote!</p> <p>Join Discord</p>"},{"location":"blog/tpu-on-gcp/","title":"Using TPUs for fine-tuning and deploying LLMs","text":"<p>If you\u2019re using or planning to use TPUs with Google Cloud, you can now do so via <code>dstack</code>. Just specify the TPU version and the number of cores  (separated by a dash), in the <code>gpu</code> property under <code>resources</code>. </p> <p>Read below to find out how to use TPUs with <code>dstack</code> for fine-tuning and deploying LLMs, leveraging open-source tools like Hugging Face\u2019s  Optimum TPU   and vLLM .</p> <p>Below is an example of a dev environment:</p> <pre><code>type: dev-environment\nname: vscode-tpu    \n\npython: 3.11\nide: vscode\n\nresources:\n  gpu: v2-8\n</code></pre> <p>If you've configured the <code>gcp</code> backend, <code>dstack</code> will automatically provision the dev environment with a TPU.</p> <p>Currently, maximum 8 TPU cores can be specified, so the maximum supported values are <code>v2-8</code>, <code>v3-8</code>, <code>v4-8</code>, <code>v5litepod-8</code>,  and <code>v5e-8</code>. Multi-host TPU support, allowing for larger numbers of cores, is coming soon.</p>"},{"location":"blog/tpu-on-gcp/#deployment","title":"Deployment","text":"<p>You can use any serving framework, such as vLLM, TGI. Here's an example of a service that deploys Llama 3.1 8B using  Optimum TPU  and vLLM .</p> Optimum TPUvLLM <p> <p><pre><code>type: service\nname: llama31-service-optimum-tpu\n\nimage: dstackai/optimum-tpu:llama31\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_TOTAL_TOKENS=4096\n  - MAX_BATCH_PREFILL_TOKENS=4095\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n\nspot_policy: auto\nresources:\n  gpu: v5litepod-4 \n\nmodel:\n  format: tgi\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> </p> <p>Once the pull request  is merged,  the official Docker image can be used instead of <code>dstackai/optimum-tpu:llama31</code>.</p> <p> <p><pre><code>type: service\nname: llama31-service-vllm-tpu\n\nenv:\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - HUGGING_FACE_HUB_TOKEN\n  - DATE=20240828\n  - TORCH_VERSION=2.5.0\n  - VLLM_TARGET_DEVICE=tpu\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n  - git clone https://github.com/vllm-project/vllm.git\n  - cd vllm\n  - pip install -r requirements-tpu.txt\n  - apt-get install -y libopenblas-base libopenmpi-dev libomp-dev\n  - python setup.py develop\n  - vllm serve $MODEL_ID \n      --tensor-parallel-size 4 \n      --max-model-len $MAX_MODEL_LEN\n      --port 8000\nport:\n  - 8000\n\nspot_policy: auto\nresources:\n  gpu: v5litepod-4\n\nmodel:\n  format: openai\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> </p> Control plane <p>If you specify <code>model</code> when running a service, <code>dstack</code> will automatically register the model on the gateway's global endpoint and allow you to use it for chat via the control plane UI.</p> <p></p>"},{"location":"blog/tpu-on-gcp/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for serving LLMs with their corresponding TPUs. </p> Model size bfloat16 TPU int8 TPU 8B 16GB v5litepod-4 8GB v5litepod-4 70B 140GB v5litepod-16 70GB v5litepod-16 405B 810GB v5litepod-64 405GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for serving transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"blog/tpu-on-gcp/#supported-frameworks","title":"Supported frameworks","text":"Framework Quantization Note TGI bfloat16 To deploy with TGI, Optimum TPU must be used. vLLM int8, bfloat16 int8 quantization still requires the same memory because the weights are first moved to the TPU in bfloat16, and then converted to int8. See the pull request  for more details."},{"location":"blog/tpu-on-gcp/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p>"},{"location":"blog/tpu-on-gcp/#fine-tuning","title":"Fine-tuning","text":"<p>Below is an example of fine-tuning Llama 3.1 8B using Optimum TPU   and the Abirate/english_quotes  dataset.</p> <pre><code>type: task\nname: optimum-tpu-llama-train\n\npython: \"3.11\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\ncommands:\n  - git clone -b add_llama_31_support https://github.com/dstackai/optimum-tpu.git\n  - mkdir -p optimum-tpu/examples/custom/\n  - cp examples/fine-tuning/optimum-tpu/llama31/train.py optimum-tpu/examples/custom/train.py\n  - cp examples/fine-tuning/optimum-tpu/llama31/config.yaml optimum-tpu/examples/custom/config.yaml\n  - cd optimum-tpu\n  - pip install -e . -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install datasets evaluate\n  - pip install accelerate -U\n  - pip install peft\n  - python examples/custom/train.py examples/custom/config.yaml\n\n\nresources:\n  gpu: v5litepod-8\n</code></pre>"},{"location":"blog/tpu-on-gcp/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning LLMs with their corresponding TPUs.</p> Model size LoRA TPU 8B 16GB v5litepod-8 70B 160GB v5litepod-16 405B 950GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for fine-tuning transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"blog/tpu-on-gcp/#supported-frameworks_1","title":"Supported frameworks","text":"Framework Quantization Note TRL bfloat16 To fine-tune using TRL, Optimum TPU is recommended. TRL doesn't support Llama 3.1 out of the box. Pytorch XLA bfloat16"},{"location":"blog/tpu-on-gcp/#whats-next","title":"What's next?","text":"<ol> <li>Browse Optimum TPU ,    Optimum TPU TGI  and    vLLM .</li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol> <p>Multi-host TPUs</p> <p>If you\u2019d like to use <code>dstack</code> with more than eight TPU cores, upvote the corresponding issue .</p>"},{"location":"blog/volumes-on-runpod/","title":"Using volumes to optimize cold starts on RunPod","text":"<p>Deploying custom models in the cloud often faces the challenge of cold start times, including the time to provision a new instance and download the model. This is especially relevant for services with autoscaling when new model replicas need to be provisioned quickly. </p> <p>Let's explore how <code>dstack</code> optimizes this process using volumes, with an example of deploying a model on RunPod.</p> <p>Suppose you want to deploy Llama 3.1 on RunPod as a service:</p> <pre><code>type: service\nname: llama31-service-tgi\n\nreplicas: 1..2\nscaling:\n  metric: rps\n  target: 30\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - text-generation-launcher\nport: 80\n\nspot_policy: auto\n\nresources:\n  gpu: 24GB\n\nmodel:\n  format: openai\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> <p>When you run <code>dstack apply</code>, it creates a public endpoint with one service replica. <code>dstack</code> will then automatically scale the service by adjusting the number of replicas based on traffic.</p> <p>When starting each replica, <code>text-generation-launcher</code> downloads the model to the <code>/data</code> folder. For Llama 3.1 8B, this usually takes under a minute, but larger models may take longer. Repeated downloads can significantly affect auto-scaling efficiency.</p> <p>Great news: RunPod supports network volumes, which we can use for caching models across multiple replicas.</p> <p>With <code>dstack</code>, you can create a RunPod volume using the following configuration:</p> <pre><code>type: volume\nname: llama31-volume\n\nbackend: runpod\nregion: EU-SE-1\n\n# Required size\nsize: 100GB\n</code></pre> <p>Go ahead and create it via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/mist/volumes/runpod.dstack.yml\n</code></pre> <p>Once the volume is created, attach it to your service by updating the configuration file and mapping the  volume name to the <code>/data</code> path.</p> <pre><code>type: service\nname: llama31-service-tgi\n\nreplicas: 1..2\nscaling:\n  metric: rps\n  target: 30\n\nvolumes:\n - name: llama31-volume\n   path: /data\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - text-generation-launcher\nport: 80\n\nspot_policy: auto\n\nresources:\n  gpu: 24GB\n\nmodel:\n  format: openai\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> <p>In this case, <code>dstack</code> attaches the specified volume to each new replica. This ensures the model is downloaded only once, reducing cold start time in proportion to the model size.</p> <p>A notable feature of RunPod is that volumes can be attached to multiple containers simultaneously. This capability is particularly useful for autoscalable services or distributed tasks.</p> <p>Using volumes not only optimizes inference cold start times but also enhances the efficiency of data and model checkpoint loading during training and fine-tuning. Whether you're running tasks or dev environments, leveraging volumes can significantly streamline your workflow and improve overall performance.</p>"},{"location":"changelog/","title":"Blog","text":""},{"location":"docs/","title":"What is dstack?","text":"<p><code>dstack</code> is a streamlined alternative to Kubernetes, specifically designed for AI. It simplifies container orchestration for AI workloads both in the cloud and on-prem, speeding up the development, training, and deployment of AI models.</p> <p><code>dstack</code> is easy to use with any cloud providers as well as on-prem servers. </p>"},{"location":"docs/#accelerators","title":"Accelerators","text":"<p><code>dstack</code> supports <code>NVIDIA GPU</code>, <code>AMD GPU</code>, and <code>Google Cloud TPU</code> out of the box.</p>"},{"location":"docs/#how-does-it-work","title":"How does it work?","text":"<p>Before using <code>dstack</code>, ensure you've installed the server, or signed up for dstack Sky .</p>"},{"location":"docs/#1-define-configurations","title":"1. Define configurations","text":"<p><code>dstack</code> supports the following configurations:</p> <ul> <li>Dev environments \u2014 for interactive development using a desktop IDE</li> <li>Tasks \u2014 for scheduling jobs (incl. distributed jobs) or running web apps</li> <li>Services \u2014 for deployment of models and web apps (with auto-scaling and authorization)</li> <li>Fleets \u2014 for managing cloud and on-prem clusters</li> <li>Volumes \u2014 for managing persisted volumes</li> <li>Gateways \u2014 for configuring the ingress traffic and public endpoints</li> </ul> <p>Configuration can be defined as YAML files within your repo.</p>"},{"location":"docs/#2-apply-configurations","title":"2. Apply configurations","text":"<p>Apply the configuration either via the <code>dstack apply</code> CLI command or through a programmatic API.</p> <p><code>dstack</code> automatically manages provisioning, job queuing, auto-scaling, networking, volumes, run failures, out-of-capacity errors, port-forwarding, and more \u2014 across clouds and on-prem clusters.</p>"},{"location":"docs/#why-dstack","title":"Why dstack?","text":"<p><code>dstack</code>'s founder and CEO explains the challenges <code>dstack</code> addresses for AI and Ops teams.</p>"},{"location":"docs/#where-do-i-start","title":"Where do I start?","text":"<ol> <li>Proceed to installation</li> <li>See quickstart</li> <li>Browse examples</li> <li>Join Discord </li> </ol>"},{"location":"docs/dev-environments/","title":"Dev environments","text":"<p>A dev environment lets you provision a remote machine with your code, dependencies, and resources, and access it with your desktop IDE. </p> <p>Dev environments are perfect when you need to run code interactively.</p>"},{"location":"docs/dev-environments/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project repo. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode\n\npython: \"3.11\"\n# Uncomment to use a custom Docker image\n#image: dstackai/base:py3.10-0.5-cuda-12.1\n\nide: vscode\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  # Required resources\n  gpu: 24GB\n</code></pre> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p> <p>Reference</p> <p>See .dstack.yml for all the options supported by dev environments, along with multiple examples.</p>"},{"location":"docs/dev-environments/#run-a-configuration","title":"Run a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ dstack apply -f examples/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                SPOT  PRICE\n 1  runpod   CA-MTL-1  9xCPU, 48GB, A5000:24GB  yes   $0.11\n 2  runpod   EU-SE-1   9xCPU, 43GB, A5000:24GB  yes   $0.11\n 3  gcp      us-west4  4xCPU, 16GB, L4:24GB     yes   $0.214516\n\nSubmit the run vscode? [y/n]: y\n\nLaunching `vscode`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+vscode/workflow\n</code></pre> <p><code>dstack apply</code> automatically uploads the code from the current repo, including your local uncommitted changes. To avoid uploading large files, ensure they are listed in <code>.gitignore</code>.</p>"},{"location":"docs/dev-environments/#vs-code","title":"VS Code","text":"<p>To open the dev environment in your desktop IDE, use the link from the output  (such as <code>vscode://vscode-remote/ssh-remote+fast-moth-1/workflow</code>).</p> <p></p>"},{"location":"docs/dev-environments/#ssh","title":"SSH","text":"<p>Alternatively, while the CLI is attached to the run, you can connect to the dev environment via SSH:</p> <pre><code>$ ssh fast-moth-1\n</code></pre>"},{"location":"docs/dev-environments/#manage-runs","title":"Manage runs","text":""},{"location":"docs/dev-environments/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code>  command lists all running jobs and their statuses.  Use <code>--watch</code> (or <code>-w</code>) to monitor the live status of runs.</p>"},{"location":"docs/dev-environments/#stop-a-run","title":"Stop a run","text":"<p>Once the run exceeds the <code>max_duration</code>, or when you use <code>dstack stop</code>,  the dev environment is stopped. Use <code>--abort</code> or <code>-x</code> to stop the run abruptly. </p>"},{"location":"docs/dev-environments/#manage-fleets","title":"Manage fleets","text":"<p>By default, <code>dstack apply</code> reuses <code>idle</code> instances from one of the existing fleets,  or creates a new fleet through backends.</p> <p>Idle duration</p> <p>To ensure the created fleets are deleted automatically, set <code>termination_idle_time</code>. By default, it's set to <code>5min</code>.</p> <p>Creation policy</p> <p>To ensure <code>dstack apply</code> always reuses an existing fleet and doesn't create a new one, pass <code>--reuse</code> to <code>dstack apply</code> (or set <code>creation_policy</code> to <code>reuse</code> in the task configuration). The default policy is <code>reuse_or_create</code>.</p>"},{"location":"docs/dev-environments/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks, and      services</li> <li>See fleets on how to manage fleets</li> </ol> <p>Reference</p> <p>See .dstack.yml for all the options supported by dev environments, along with multiple examples.</p>"},{"location":"docs/quickstart/","title":"Quickstart","text":"<p>Before using <code>dstack</code>, ensure you've installed the server, or signed up for dstack Sky .</p>"},{"location":"docs/quickstart/#initialize-a-repo","title":"Initialize a repo","text":"<p>To use <code>dstack</code>'s CLI in a folder, first run <code>dstack init</code> within that folder.</p> <pre><code>$ mkdir quickstart &amp;&amp; cd quickstart\n$ dstack init\n</code></pre> <p>Your folder can be a regular local folder or a Git repo.</p>"},{"location":"docs/quickstart/#run-a-configuration","title":"Run a configuration","text":"Dev environmentTaskService <p>A dev environment lets you provision a remote machine with your code, dependencies, and resources, and access it  with your desktop IDE.</p>"},{"location":"docs/quickstart/#define-a-configuration","title":"Define a configuration","text":"<p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode\n\npython: \"3.11\"\n# Uncomment to use a custom Docker image\n#image: dstackai/base:py3.10-0.5-cuda-12.1\n\nide: vscode\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\n# Uncomment to request resources\n#resources:\n#  gpu: 24GB\n</code></pre>"},{"location":"docs/quickstart/#run-the-configuration","title":"Run the configuration","text":"<p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f .dstack.yml\n\n #  BACKEND  REGION           RESOURCES                 SPOT  PRICE\n 1  gcp      us-west4         2xCPU, 8GB, 100GB (disk)  yes   $0.010052\n 2  azure    westeurope       2xCPU, 8GB, 100GB (disk)  yes   $0.0132\n 3  gcp      europe-central2  2xCPU, 8GB, 100GB (disk)  yes   $0.013248\n\nSubmit the run vscode? [y/n]: y\n\nLaunching `vscode`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+vscode/workflow\n</code></pre> <p>Open the link to access the dev environment using your desktop IDE.</p> <p>A task allows you to schedule a job or run a web app. It lets you configure  dependencies, resources, ports, the number of nodes (if you want to run the task on a cluster), etc.</p>"},{"location":"docs/quickstart/#define-a-configuration_1","title":"Define a configuration","text":"<p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: streamlit\n\npython: \"3.11\"\n# Uncomment to use a custom Docker image\n#image: dstackai/base:py3.10-0.5-cuda-12.1\n\n# Commands of the task\ncommands:\n  - pip install streamlit\n  - streamlit hello\n# Ports to forward\nports:\n  - 8501\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\n# Uncomment to request resources\n#resources:\n#  gpu: 24GB\n</code></pre>"},{"location":"docs/quickstart/#run-the-configuration_1","title":"Run the configuration","text":"<p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f streamlit.dstack.yml\n\n #  BACKEND  REGION           RESOURCES                 SPOT  PRICE\n 1  gcp      us-west4         2xCPU, 8GB, 100GB (disk)  yes   $0.010052\n 2  azure    westeurope       2xCPU, 8GB, 100GB (disk)  yes   $0.0132\n 3  gcp      europe-central2  2xCPU, 8GB, 100GB (disk)  yes   $0.013248\n\nSubmit the run streamlit? [y/n]: y\n\nContinue? [y/n]: y\n\nProvisioning `streamlit`...\n---&gt; 100%\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: http://localhost:8501\n</code></pre> <p><code>dstack apply</code> automatically forwards the remote ports to <code>localhost</code> for convenient access.</p> <p>A service allows you to deploy a web app or a model as a scalable endpoint. It lets you configure dependencies, resources, authorizarion, auto-scaling rules, etc. </p> Prerequisites <p>If you're using the open-source server, you must set up a gateway before you can run a service.</p> <p>If you're using dstack Sky , the gateway is already set up for you.</p>"},{"location":"docs/quickstart/#define-a-configuration_2","title":"Define a configuration","text":"<p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: streamlit-service\n\npython: \"3.11\"\n# Uncomment to use a custom Docker image\n#image: dstackai/base:py3.10-0.5-cuda-12.1\n\n# Commands of the service\ncommands:\n  - pip install streamlit\n  - streamlit hello\n# Port of the service\nport: 8501\n\n# Comment to enable authorizartion\nauth: False\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\n# Uncomment to request resources\n#resources:\n#  gpu: 24GB\n</code></pre>"},{"location":"docs/quickstart/#run-the-configuration_2","title":"Run the configuration","text":"<p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f streamlit.dstack.yml\n\n #  BACKEND  REGION           RESOURCES                 SPOT  PRICE\n 1  gcp      us-west4         2xCPU, 8GB, 100GB (disk)  yes   $0.010052\n 2  azure    westeurope       2xCPU, 8GB, 100GB (disk)  yes   $0.0132\n 3  gcp      europe-central2  2xCPU, 8GB, 100GB (disk)  yes   $0.013248\n\nSubmit the run streamlit? [y/n]: y\n\nContinue? [y/n]: y\n\nProvisioning `streamlit`...\n---&gt; 100%\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: https://streamlit-service.example.com\n</code></pre> <p>One the service is up, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p><code>dstack apply</code> automatically uploads the code from the current repo, including your local uncommitted changes.</p>"},{"location":"docs/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Something not working? Make sure to check out the troubleshooting guide.</p>"},{"location":"docs/quickstart/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks,      services, and fleets </li> <li>Browse examples</li> <li>Join the community via Discord </li> </ol> <p>Examples</p> <p>To see how dev environments, tasks, services, and fleets can be used for  training and deploying AI models, check out the examples.</p>"},{"location":"docs/services/","title":"Services","text":"<p>A service allows you to deploy a web app or a model as a scalable endpoint. It lets you configure dependencies, resources, authorizarion, auto-scaling rules, etc.</p> <p>Services are provisioned behind a gateway which provides an HTTPS endpoint mapped to your domain, handles authentication, distributes load, and performs auto-scaling.</p> Gateways <p>If you're using the open-source server, you must set up a gateway before you can run a service.</p> <p>If you're using dstack Sky , the gateway is already set up for you.</p>"},{"location":"docs/services/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>service.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama31-service\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.10\"\n\n# Required environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096\n# Expose the vllm server port\nport: 8000\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  # Change to what is required\n  gpu: 24GB\n\n# Comment if you don't to access the model via https://gateway.&lt;gateway domain&gt;\nmodel:\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n  format: openai\n</code></pre> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p> <p>Auto-scaling</p> <p>By default, the service is deployed to a single instance. However, you can specify the number of replicas and scaling policy. In this case, <code>dstack</code> auto-scales it based on the load.</p> <p>Reference</p> <p>See .dstack.yml for all the options supported by services, along with multiple examples.</p>"},{"location":"docs/services/#run-a-service","title":"Run a service","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HUGGING_FACE_HUB_TOKEN=...\n\n$ dstack apply -f service.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:3  yes   $0.33\n\nSubmit the run llama31-service? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nService is published at https://llama31-service.example.com\n</code></pre> <p><code>dstack apply</code> automatically uploads the code from the current repo, including your local uncommitted changes. To avoid uploading large files, ensure they are listed in <code>.gitignore</code>.</p>"},{"location":"docs/services/#access-the-endpoint","title":"Access the endpoint","text":"<p>One the service is up, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>By default, the service endpoint requires the <code>Authorization</code> header with <code>Bearer &lt;dstack token&gt;</code>.</p> <pre><code>$ curl https://llama31-service.example.com/v1/chat/completions \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -d '{\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Compose a poem that explains the concept of recursion in programming.\"\n            }\n        ]\n    }'\n</code></pre> <p>Authorization can be disabled by setting <code>auth</code> to <code>false</code> in the service configuration file.</p>"},{"location":"docs/services/#gateway-endpoint","title":"Gateway endpoint","text":"<p>In case the service has the model mapping configured, you will also be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p>"},{"location":"docs/services/#manage-runs","title":"Manage runs","text":""},{"location":"docs/services/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code>  command lists all running jobs and their statuses.  Use <code>--watch</code> (or <code>-w</code>) to monitor the live status of runs.</p>"},{"location":"docs/services/#stop-a-run","title":"Stop a run","text":"<p>Once the run exceeds the <code>max_duration</code>, or when you use <code>dstack stop</code>,  the dev environment is stopped. Use <code>--abort</code> or <code>-x</code> to stop the run abruptly. </p>"},{"location":"docs/services/#manage-fleets","title":"Manage fleets","text":"<p>By default, <code>dstack apply</code> reuses <code>idle</code> instances from one of the existing fleets,  or creates a new fleet through backends.</p> <p>Idle duration</p> <p>To ensure the created fleets are deleted automatically, set <code>termination_idle_time</code>. By default, it's set to <code>5min</code>.</p> <p>Creation policy</p> <p>To ensure <code>dstack apply</code> always reuses an existing fleet and doesn't create a new one, pass <code>--reuse</code> to <code>dstack apply</code> (or set <code>creation_policy</code> to <code>reuse</code> in the task configuration). The default policy is <code>reuse_or_create</code>.</p>"},{"location":"docs/services/#whats-next","title":"What's next?","text":"<ol> <li>Check the TGI  and vLLM  examples</li> <li>See gateways on how to set up a gateway</li> <li>Browse examples</li> <li>See fleets on how to manage fleets</li> </ol> <p>Reference</p> <p>See .dstack.yml for all the options supported by services, along with multiple examples.</p>"},{"location":"docs/tasks/","title":"Tasks","text":"<p>A task allows you to schedule a job or run a web app. It lets you configure dependencies, resources, ports, and more. Tasks can be distributed and run on clusters.</p> <p>Tasks are ideal for training and fine-tuning jobs. They can also be used instead of services if you want to run a web app but don't need a public endpoint.</p>"},{"location":"docs/tasks/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: axolotl-train\n\n# Using the official Axolotl's Docker image\nimage: winglian/axolotl-cloud:main-20240429-py3.11-cu121-2.2.1\n\n# Required environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\n# Commands of the task\ncommands:\n  - accelerate launch -m axolotl.cli.train examples/fine-tuning/axolotl/config.yaml\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # Two or more GPU\n    count: 2..\n</code></pre> <p>If you don't specify your Docker image, <code>dstack</code> uses the base image (pre-configured with Python, Conda, and essential CUDA drivers).</p> <p>Distributed tasks</p> <p>By default, tasks run on a single instance. However, you can specify the number of nodes. In this case, the task will run a cluster of instances.</p> <p>Reference</p> <p>See .dstack.yml for all the options supported by tasks, along with multiple examples.</p>"},{"location":"docs/tasks/#run-a-configuration","title":"Run a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HUGGING_FACE_HUB_TOKEN=...\n$ WANDB_API_KEY=...\n\n$ dstack apply -f examples/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:3  yes   $0.33\n\nSubmit the run axolotl-train? [y/n]: y\n\nLaunching `axolotl-train`...\n---&gt; 100%\n\n{'loss': 1.4967, 'grad_norm': 1.2734375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n  0% 1/24680 [00:13&lt;95:34:17, 13.94s/it]\n  6% 73/1300 [00:48&lt;13:57,  1.47it/s]\n</code></pre> <p><code>dstack apply</code> automatically uploads the code from the current repo, including your local uncommitted changes. To avoid uploading large files, ensure they are listed in <code>.gitignore</code>.</p> <p>Ports</p> <p>If the task specifies <code>ports</code>, <code>dstack run</code> automatically forwards them to your local machine for convenient and secure access.</p> <p>Queueing tasks</p> <p>By default, if <code>dstack apply</code> cannot find capacity, the task fails.  To queue the task and wait for capacity, specify the <code>retry</code>  property in the task configuration.</p>"},{"location":"docs/tasks/#manage-runs","title":"Manage runs","text":""},{"location":"docs/tasks/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code>  command lists all running jobs and their statuses.  Use <code>--watch</code> (or <code>-w</code>) to monitor the live status of runs.</p>"},{"location":"docs/tasks/#stop-a-run","title":"Stop a run","text":"<p>Once the run exceeds the <code>max_duration</code>, or when you use <code>dstack stop</code>,  the dev environment is stopped. Use <code>--abort</code> or <code>-x</code> to stop the run abruptly. </p>"},{"location":"docs/tasks/#manage-fleets","title":"Manage fleets","text":"<p>By default, <code>dstack apply</code> reuses <code>idle</code> instances from one of the existing fleets,  or creates a new fleet through backends.</p> <p>Idle duration</p> <p>To ensure the created fleets are deleted automatically, set <code>termination_idle_time</code>. By default, it's set to <code>5min</code>.</p> <p>Creation policy</p> <p>To ensure <code>dstack apply</code> always reuses an existing fleet and doesn't create a new one, pass <code>--reuse</code> to <code>dstack apply</code> (or set <code>creation_policy</code> to <code>reuse</code> in the task configuration). The default policy is <code>reuse_or_create</code>.</p>"},{"location":"docs/tasks/#whats-next","title":"What's next?","text":"<ol> <li>Check the Axolotl example</li> <li>Browse all examples</li> <li>See fleets on how to manage fleets</li> </ol> <p>Reference</p> <p>See .dstack.yml for all the options supported by tasks, along with multiple examples.</p>"},{"location":"docs/concepts/fleets/","title":"Fleets","text":"<p>Fleets enable efficient provisioning and management of clusters and instances, both in the cloud and on-prem. Once a fleet is created, it can be reused by dev environments, tasks, and services.</p>"},{"location":"docs/concepts/fleets/#define-a-configuration","title":"Define a configuration","text":"<p>To create a fleet, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>fleet.dstack.yml</code> are both acceptable).</p> CloudSSH <p>What is a cloud fleet?</p> <p>By default, when running dev environments, tasks, and services, <code>dstack</code>  reuses <code>idle</code> instances from existing fleets or creates a new cloud fleet on the fly.</p> <p>If you want more control over the lifecycle of cloud instances, you can create a cloud fleet manually.  This allows you to reuse a fleet over a longer period and across multiple runs. You can also delete the fleet only when needed.</p> <p>To create a cloud fleet, specify <code>resources</code>, <code>nodes</code>,  and other optional parameters.</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: ah-fleet-distrib\n\n# Size of the cluster\nnodes: 2\n# Ensure instances are interconnected\nplacement: cluster\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n</code></pre> <p>When you apply this configuration, <code>dstack</code> will create cloud instances using the configured backends according  to the specified parameters.</p> <p>Network</p> <p>Set <code>placement</code> to <code>cluster</code> if the nodes should be interconnected (e.g. if you'd like to use them for multi-node tasks).  In that case, <code>dstack</code> will provision all nodes in the same backend and region.</p> <p>Note that cloud fleets aren't supported for the <code>kubernetes</code>, <code>vastai</code>, and <code>runpod</code> backends.</p> <p>What is an SSH fleet?</p> <p>If you\u2019d like to run dev environments, tasks, and services on arbitrary on-prem servers via <code>dstack</code>, you can  create an SSH fleet.</p> <p>To create an SSH fleet, specify <code>ssh_config</code> to allow the <code>dstack</code> server to connect to these servers via SSH.</p> <p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-ssh-fleet\n\n# Ensure instances are interconnected\nplacement: cluster\n\n# The user, private SSH key, and hostnames of the on-prem servers\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre> <p>When you apply this configuration, <code>dstack</code> will connect to the specified hosts using the provided SSH credentials,  install the dependencies, and configure these servers as a fleet.</p> <p>Requirements</p> <p>Hosts should be pre-installed with Docker. Systems with NVIDIA GPUs should also be pre-installed with CUDA 12.1 and NVIDIA Container Toolkit . The user should have <code>sudo</code> access.</p> Environment variables <p>For SSH fleets, it's possible to pre-configure environment variables.  These variables will be used when installing the <code>dstack-shim</code> service on hosts  and running containers.</p> <p>For example, these variables can be used to configure a proxy:</p> <pre><code>type: fleet\nname: my-fleet\n\nplacement: cluster\n\nenv:\n  - HTTP_PROXY=http://proxy.example.com:80\n  - HTTPS_PROXY=http://proxy.example.com:80\n  - NO_PROXY=localhost,127.0.0.1\n\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre> <p>Network</p> <p>Set <code>placement</code> to <code>cluster</code> if the hosts are interconnected (e.g. if you'd like to use them for multi-node tasks). In that case, by default, <code>dstack</code> will automatically detect the private network.  You can specify the <code>network</code> parameter manually.</p> <p>Note that to use SSH fleets, you don't need any backends at all.</p> <p>See <code>.dstack.yml</code> for all the options supported by the fleet configuration.</p>"},{"location":"docs/concepts/fleets/#create-or-update-a-fleet","title":"Create or update a fleet","text":"<p>To create or update the fleet, simply call the <code>dstack apply</code> command:</p> <pre><code>$ dstack apply -f examples/fine-tuning/alignment-handbook/fleet-distributed.dstack.yml\n</code></pre>"},{"location":"docs/concepts/fleets/#ensure-the-fleet-is-created","title":"Ensure the fleet is created","text":"<p>To ensure the fleet is created, use the <code>dstack fleet</code> command:</p> <pre><code>$ dstack fleet\n\n FLEET     INSTANCE  BACKEND              GPU             PRICE    STATUS  CREATED \n my-fleet  0         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago      \n           1         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago    \n</code></pre> <p>Once the status of instances changes to <code>idle</code>, they can be used by dev environments, tasks, and services.</p>"},{"location":"docs/concepts/fleets/#troubleshooting-ssh-fleets","title":"Troubleshooting SSH fleets","text":"<p>Resources</p> <p>If you're creating an SSH fleet, ensure that the GPU, memory, and disk size are detected properly. If GPU isn't detected, ensure that the hosts meet the requirements (see above).</p> <p>If the status doesn't change to <code>idle</code> after a few minutes, ensure that  the hosts meet the requirements (see above).</p> <p>If the requirements are met but the fleet still fails to be created, check <code>/root/.dstack/shim.log</code> for logs  on the hosts specified in <code>ssh_config</code>.</p>"},{"location":"docs/concepts/fleets/#manage-fleets","title":"Manage fleets","text":""},{"location":"docs/concepts/fleets/#list-fleets","title":"List fleets","text":"<p>The <code>dstack fleet</code> command lists fleet instances and theri status:</p> <pre><code>$ dstack fleet\n\n FLEET     INSTANCE  BACKEND              GPU             PRICE    STATUS  CREATED \n my-fleet  0         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago      \n           1         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago    \n</code></pre>"},{"location":"docs/concepts/fleets/#delete-fleets","title":"Delete fleets","text":"<p>When a fleet isn't used by run, you can delete it via <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f cluster.dstack.yaml\nDelete the fleet my-gcp-fleet? [y/n]: y\nFleet my-gcp-fleet deleted\n</code></pre> <p>You can pass either the path to the configuration file or the fleet name directly.</p> <p>To terminate and delete specific instances from a fleet, pass <code>-i INSTANCE_NUM</code>.</p>"},{"location":"docs/concepts/fleets/#termination-policy","title":"Termination policy","text":"<p>If you want a fleet to be automatically deleted after a certain idle time, you can set the you can set the <code>termination_idle_time</code> property.</p>"},{"location":"docs/concepts/fleets/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks, and      services </li> <li>Join the community via Discord </li> </ol> <p>Reference</p> <p>See .dstack.yml for all the options supported by fleets, along with multiple examples.</p>"},{"location":"docs/concepts/gateways/","title":"Gateways","text":"<p>Gateways manage the ingress traffic of running services and provide them with an HTTPS endpoint mapped to your domain, handling authentication, load distribution, and auto-scaling.</p> <p>To run a service, you need at least one gateway set up.</p> <p>If you're using dstack Sky , the gateway is already set up for you.</p>"},{"location":"docs/concepts/gateways/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>gateway.dstack.yml</code> are both acceptable).</p> <pre><code>type: gateway\n# A name of the gateway\nname: example-gateway\n\n# Gateways are bound to a specific backend and region\nbackend: aws\nregion: eu-west-1\n\n# This domain will be used to access the endpoint\ndomain: example.com\n</code></pre> <p>A domain name is required to create a gateway.</p> <p>Reference</p> <p>See .dstack.yml for all the options supported by gateways, along with multiple examples.</p>"},{"location":"docs/concepts/gateways/#create-or-update-a-gateway","title":"Create or update a gateway","text":"<p>To create or update the gateway, simply call the <code>dstack apply</code> command:</p> <pre><code>$ dstack apply . -f examples/deployment/gateway.dstack.yml\nThe example-gateway doesn't exist. Create it? [y/n]: y\n\n BACKEND  REGION     NAME             HOSTNAME  DOMAIN       DEFAULT  STATUS\n aws      eu-west-1  example-gateway            example.com  \u2713        submitted\n</code></pre>"},{"location":"docs/concepts/gateways/#update-dns-records","title":"Update DNS records","text":"<p>Once the gateway is assigned a hostname, go to your domain's DNS settings and add an <code>A</code> DNS record for <code>*.&lt;gateway domain&gt;</code> (e.g., <code>*.example.com</code>) pointing to the gateway's hostname.</p>"},{"location":"docs/concepts/gateways/#manage-gateways","title":"Manage gateways","text":""},{"location":"docs/concepts/gateways/#list-gateways","title":"List gateways","text":"<p>The <code>dstack gateway list</code> command lists existing gateways and their status.</p>"},{"location":"docs/concepts/gateways/#delete-a-gateway","title":"Delete a gateway","text":"<p>To delete a gateway, pass gateway configuration to <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f examples/deployment/gateway.dstack.yml\n</code></pre>"},{"location":"docs/concepts/gateways/#whats-next","title":"What's next?","text":"<ol> <li>See services on how to run services</li> </ol> <p>Reference</p> <p>See .dstack.yml for all the options supported by gateways, along with multiple examples.</p>"},{"location":"docs/concepts/pools/","title":"Pools","text":"<p>Pools enable the efficient reuse of cloud instances and on-premises servers across runs, simplifying their management.</p>"},{"location":"docs/concepts/pools/#adding-instances","title":"Adding instances","text":""},{"location":"docs/concepts/pools/#automatic-provisioning","title":"Automatic provisioning","text":"<p>By default, when using the <code>dstack run</code> command, it tries to reuse an instance from a pool. If no idle instance meets the requirements, <code>dstack</code> automatically provisions a new cloud instance and adds it to the pool.</p> Reuse policy <p>To avoid provisioning new cloud instances with <code>dstack run</code>, use <code>--reuse</code>. Your run will be assigned to an idle instance in the pool. If there are no available idle instances in the pool, the run will fail.</p> Idle duration <p>By default, <code>dstack run</code> sets the idle duration of a newly provisioned instance to <code>5m</code>. This means that if the run is finished and the instance remains idle for longer than five minutes, it is automatically removed from the pool. To override the default idle duration, use  <code>--idle-duration DURATION</code> with <code>dstack run</code>.</p>"},{"location":"docs/concepts/pools/#manual-provisioning","title":"Manual provisioning","text":"<p>To manually provision a cloud instance and add it to a pool, use <code>dstack pool add</code>:</p> <pre><code>$ dstack pool add --gpu 80GB\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n</code></pre> <p>The <code>dstack pool add</code> command allows specifying resource requirements, along with the spot policy, idle duration, max price, retry policy, and other policies.</p> Idle duration <p>The default idle duration if you're using <code>dstack pool add</code> is <code>72h</code>. To override it, use the <code>--idle-duration DURATION</code> argument.</p> <p>You can also specify the policies via <code>.dstack/profiles.yml</code> instead of passing them as arguments. For more details on policies and their defaults, refer to <code>.dstack/profiles.yml</code>.</p> Limitations <p>The <code>dstack pool add</code> command is not supported for Kubernetes, VastAI, and RunPod backends yet.</p>"},{"location":"docs/concepts/pools/#adding-on-prem-clusters","title":"Adding on-prem clusters","text":"<p>Any on-prem server that can be accessed via SSH can be added to a pool and used to run workloads.</p> <p>To add on-prem servers to the pool, use the <code>dstack pool add-ssh</code> command and pass the hostname of your server along with the SSH key.</p> <pre><code>$ dstack pool add-ssh -i ~/.ssh/id_rsa ubuntu@54.73.155.119\n</code></pre> <p>The command accepts the same arguments as the standard <code>ssh</code> command.</p> <p>Requirements</p> <p>On-prem servers should be pre-installed with Docker. Systems with NVIDIA GPUs should also be pre-installed with CUDA 12.1 and NVIDIA Container Toolkit . The user should have <code>sudo</code> access.</p> <p>Once the instance is provisioned, you'll see it in the pool and will be able to run workloads on it.</p>"},{"location":"docs/concepts/pools/#clusters","title":"Clusters","text":"<p>If you want on-prem instances to run multi-node tasks, ensure these on-prem servers share the same private network. Additionally, you need to pass the <code>--network</code> option to <code>dstack pool add-ssh</code>:</p> <pre><code>$ dstack pool add-ssh -i ~/.ssh/id_rsa ubuntu@54.73.155.119 \\\n    --network 10.0.0.0/24\n</code></pre> <p>The <code>--network</code> argument accepts the IP address range (CIDR) of the private network of the instance.</p> <p>Once you've added multiple instances with the same network value, you can use them as a cluster to run multi-node tasks.</p>"},{"location":"docs/concepts/pools/#removing-instances","title":"Removing instances","text":"<p>If the instance remains idle for the configured idle duration, <code>dstack</code> removes it and deletes all cloud resources.</p> <p>To remove an instance from the pool manually, use the <code>dstack pool rm</code> command. </p> <pre><code>$ dstack pool rm &amp;lt;instance name&amp;gt;\n</code></pre>"},{"location":"docs/concepts/pools/#list-instances","title":"List instances","text":"<p>The <code>dstack pool ps</code> command lists active instances and their status (<code>busy</code> or <code>idle</code>).</p>"},{"location":"docs/concepts/projects/","title":"Projects","text":"<p>Projects enable the isolation of different teams and their resources. Each project can configure its own backends and control which users have access to it.</p> <p>While project backends can be configured via <code>~/.dstack/server/config.yml</code>, use the control plane UI to fully manage projects, users, and user permissions.</p>"},{"location":"docs/concepts/projects/#project-backends","title":"Project backends","text":"<p>In addition to <code>~/.dstack/server/config.yml</code>,  a global admin or a project admin can configure backends on the project settings page.</p> <p></p>"},{"location":"docs/concepts/projects/#global-admins","title":"Global admins","text":"<p>A user can be assigned or unassigned a global admin role on the user account settings page. This can only be done by  another global admin.</p> <p></p> <p>The global admin role allows a user to manage all projects and users.</p>"},{"location":"docs/concepts/projects/#project-members","title":"Project members","text":"<p>A user can be added to a project and assigned or unassigned as a project role on the project settings page.</p> <p></p>"},{"location":"docs/concepts/projects/#project-roles","title":"Project roles","text":"<ul> <li>Admin \u2013 The project admin role allows a user to manage the project's settings,   including backends, gateways, and members.</li> <li>Manager \u2013 The project manager role allows a user to manage project members.   Unlike admins, managers cannot configure backends and gateways.</li> <li>User \u2013 A user can manage project resources including runs, fleets, and volumes.</li> </ul>"},{"location":"docs/concepts/projects/#authorization","title":"Authorization","text":""},{"location":"docs/concepts/projects/#user-token","title":"User token","text":"<p>Once created, a user is issued a token. This token can be found on the user account settings page. </p> <p></p> <p>The token must be used for authentication when logging into the control plane UI and when using the CLI or API.</p>"},{"location":"docs/concepts/projects/#setting-up-the-cli","title":"Setting up the CLI","text":"<p>To use the CLI with a specific project, run the <code>dstack config</code> command with the server address, user token, and project name.</p> <p>You can find the command on the project\u2019s settings page:</p> <p></p> API <p>In addition to the UI, managing projects, users, and user permissions can also be done via the REST API).</p>"},{"location":"docs/concepts/volumes/","title":"Volumes","text":"<p>Volumes allow you to persist data between runs. <code>dstack</code> allows to create and attach volumes to  dev environments, tasks, and services.</p> <p>Volumes are currently supported with the <code>aws</code>, <code>gcp</code>, and <code>runpod</code> backends. Support for other backends and SSH fleets is coming soon.</p>"},{"location":"docs/concepts/volumes/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>vol.dstack.yml</code> are both acceptable).</p> <pre><code>type: volume\n# A name of the volume\nname: my-new-volume\n\n# Volumes are bound to a specific backend and region\nbackend: aws\nregion: eu-central-1\n\n# Required size\nsize: 100GB\n</code></pre> <p>If you use this configuration, <code>dstack</code> will create a new volume based on the specified options.</p> <p>Registering existing volumes</p> <p>If you prefer not to create a new volume but to reuse an existing one (e.g., created manually), you can  specify its ID via <code>volume_id</code>. In this case, <code>dstack</code> will register the specified volume so that you can use it with dev environments, tasks, and services.</p> <p>Reference</p> <p>See .dstack.yml for all the options supported by volumes, along with multiple examples.</p>"},{"location":"docs/concepts/volumes/#create-register-or-update-a-volume","title":"Create, register, or update a volume","text":"<p>To create or register the volume, simply call the <code>dstack apply</code> command:</p> <pre><code>$ dstack apply -f volume.dstack.yml\nVolume my-new-volume does not exist yet. Create the volume? [y/n]: y\n\n NAME           BACKEND  REGION        STATUS     CREATED \n my-new-volume  aws      eu-central-1  submitted  now     \n</code></pre> <p>When creating the volume <code>dstack</code> automatically creates an <code>ext4</code> file system on it.</p> <p>Once created, the volume can be attached with dev environments, tasks, and services.</p>"},{"location":"docs/concepts/volumes/#attach-a-volume","title":"Attach a volume","text":"<p>Dev environments, tasks, and services let you attach any number of volumes. To attach a volume, simply specify its name using the <code>volumes</code> property and specify where to mount its contents:</p> <pre><code>type: dev-environment\n# A name of the dev environment\nname: vscode-vol\n\nide: vscode\n\n# Map the name of the volume to any path \nvolumes:\n  - name: my-new-volume\n    path: /volume_data\n</code></pre> <p>Once you run this configuration, the contents of the volume will be attached to <code>/volume_data</code> inside the dev environment,  and its contents will persist across runs.</p> <p>Limitations</p> <p>When you're running a dev environment, task, or service with <code>dstack</code>, it automatically mounts the project folder contents to <code>/workflow</code> (and sets that as the current working directory). Right now, <code>dstack</code> doesn't allow you to  attach volumes to <code>/workflow</code> or any of its subdirectories.</p>"},{"location":"docs/concepts/volumes/#manage-volumes","title":"Manage volumes","text":""},{"location":"docs/concepts/volumes/#list-volumes","title":"List volumes","text":"<p>The <code>dstack volume list</code> command lists created and registered volumes:</p> <pre><code>$ dstack volume list\nNAME            BACKEND  REGION        STATUS  CREATED\n my-new-volume  aws      eu-central-1  active  3 weeks ago\n</code></pre>"},{"location":"docs/concepts/volumes/#delete-volumes","title":"Delete volumes","text":"<p>When the volume isn't attached to any active dev environment, task, or service, you can delete it using <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f vol.dstack.yaml\n</code></pre> <p>If the volume was created using <code>dstack</code>, it will be physically destroyed along with the data. If you've registered an existing volume, it will be de-registered with <code>dstack</code> but will keep the data.</p>"},{"location":"docs/concepts/volumes/#faq","title":"FAQ","text":""},{"location":"docs/concepts/volumes/#can-i-use-volumes-across-backends","title":"Can I use volumes across backends?","text":"<p>Since volumes are backed up by cloud network disks, you can only use them within the same cloud. If you need to access data across different backends, you should either use object storage or replicate the data across multiple volumes.</p>"},{"location":"docs/concepts/volumes/#can-i-use-volumes-across-regions","title":"Can I use volumes across regions?","text":"<p>Typically, network volumes are associated with specific regions, so you can't use them in other regions. Often, volumes are also linked to availability zones, but some providers support volumes that can be used across different availability zones within the same region.</p>"},{"location":"docs/concepts/volumes/#can-i-attach-volumes-to-multiple-runs-or-instances","title":"Can I attach volumes to multiple runs or instances?","text":"<p>You can mount a volume in multiple runs. This feature is currently supported only by the <code>runpod</code> backend.</p>"},{"location":"docs/guides/dstack-sky/","title":"dstack Sky","text":"<p>If you don't want to host the <code>dstack</code> server or would like to access GPU from the <code>dstack</code> marketplace,  sign up with dstack Sky.</p>"},{"location":"docs/guides/dstack-sky/#set-up-the-cli","title":"Set up the CLI","text":"<p>If you've signed up, open your project settings, and copy the <code>dstack config</code> command to point the CLI to the project.</p> <p></p> <p>Then, install the CLI on your machine and use the copied command.</p> <pre><code>$ pip install dstack\n$ dstack config --url https://sky.dstack.ai \\\n    --project peterschmidt85 \\\n    --token bbae0f28-d3dd-4820-bf61-8f4bb40815da\n\nConfiguration is updated at ~/.dstack/config.yml\n</code></pre>"},{"location":"docs/guides/dstack-sky/#configure-clouds","title":"Configure clouds","text":"<p>By default, dstack Sky   uses the GPU from its marketplace, which requires a credit card to be attached in your account settings.</p> <p>To use your own cloud accounts, click the settings icon of the corresponding backend and specify credentials:</p> <p></p> <p>For more details on how to configure your own cloud accounts, check the server/config.yml reference.</p>"},{"location":"docs/guides/dstack-sky/#whats-next","title":"What's next?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/guides/protips/","title":"Protips","text":"<p>Below are tips and tricks to use <code>dstack</code> more efficiently.</p>"},{"location":"docs/guides/protips/#fleets","title":"Fleets","text":"<p>By default, when running dev environments, tasks, or services, <code>dstack apply</code> reuses <code>idle</code>  instances from existing fleets. If no <code>idle</code> instances match the requirements, it creates a new fleet automatically.</p> <p>To avoid creating new fleet automatically,  set  <code>creation_policy</code> to <code>reuse</code> in the configuration.</p> <p>Use fleets configurations to create fleets manually. This reduces startup time for dev environments, tasks, and services, and is very convenient if you want to reuse fleets across runs.</p>"},{"location":"docs/guides/protips/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p> <p>Once the commands work, go ahead and run them as a task or a service.</p> Notebooks <p>VS Code</p> <p>When you access a dev environment using your desktop VS Code, it allows you to work with Jupyter notebooks via its pre-configured and easy-to-use extension.</p> <p>JupyterLab</p> <p>If you prefer to use JupyterLab, you can run it as a task:</p> <pre><code>type: task\n\ncommands:\n    - pip install jupyterlab\n    - jupyter lab --allow-root\n\nports:\n    - 8888\n</code></pre>"},{"location":"docs/guides/protips/#tasks-vs-services","title":"Tasks vs. services","text":"<p>Tasks can be used not only for batch jobs but also for web applications.</p> <pre><code>type: task\nname: streamlit-task\n\npython: \"3.10\"\n\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\nports: \n  - 8501\n</code></pre> <p>While you run a task, <code>dstack apply</code> forwards the remote ports to <code>localhost</code>.</p> <pre><code>$ dstack apply -f app.dstack.yml\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: http://localhost:8501\n</code></pre> <p>This allows you to access the remote <code>8501</code> port on <code>localhost:8501</code> while the CLI is attached.</p> Port mapping <p>If you want to override the local port, use the <code>--port</code> option:</p> <pre><code>$ dstack apply -f app.dstack.yml --port 3000:8501\n</code></pre> <p>This will forward the remote <code>8501</code> port to <code>localhost:3000</code>.</p> <p>Services require a gateway but they also provide additional features for production-grade service deployment not offered by tasks, such as HTTPS domains and auto-scaling. If you run a web app as a task and it works, go ahead and run it as a service.</p>"},{"location":"docs/guides/protips/#environment-variables","title":"Environment variables","text":"<p>If a configuration requires an environment variable that you don't want to hardcode in the YAML, you can define it without assigning a value:</p> <pre><code>type: dev-environment\nname: vscode\n\npython: \"3.10\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\nide: vscode\n</code></pre> <p>Then, you can pass the environment variable either via the shell:</p> <pre><code>HUGGING_FACE_HUB_TOKEN=... dstack apply -f .dstack.yml\n</code></pre> <p>Or via the <code>-e</code> option of the <code>dstack apply</code> command:</p> <pre><code>dstack apply -f .dstack.yml -e HUGGING_FACE_HUB_TOKEN=...\n</code></pre> .env <p>A better way to configure environment variables not hardcoded in YAML is by specifying them in a <code>.env</code> file:</p> <pre><code>HUGGING_FACE_HUB_TOKEN=...\n</code></pre> <p>If you install <code>direnv</code> , it will automatically pass the environment variables from the <code>.env</code> file to the <code>dstack apply</code> command.</p> <p>Remember to add <code>.env</code> to <code>.gitignore</code> to avoid pushing it to the repo.    </p>"},{"location":"docs/guides/protips/#data-and-models","title":"Data and models","text":"<p><code>dstack</code> has support for volumes to persist data across different runs and instance interruptions. Volumes are ideal for storing intermediate work and data that should be quickly accessible.</p> <p>You can also load and save data using an object storage like S3 or HuggingFace Datasets. For models, it's best to use services like HuggingFace Hub. <code>dstack</code> has no explicit support for object storage. You can load and save data directly from your code.</p>"},{"location":"docs/guides/protips/#idle-duration","title":"Idle duration","text":"<p>If you run a dev environment, task, or service via <code>dstack apply</code>, and it creates a new fleet, it sets the idle duration to <code>5m</code>. If instances of the fleet are <code>idle</code> for this time, <code>dstack</code> terminates them.</p> <p>If you create a fleet manually, the idle duration is not set.</p> <p>You can override idle duration for fleets, dev environment, tasks, and services by setting <code>termination_idle_time</code> in the configuration file. </p>"},{"location":"docs/guides/protips/#attached-mode","title":"Attached mode","text":"<p>By default, <code>dstack apply</code> runs in attached mode. This means it streams the logs as they come in and, in the case of a task, forwards its ports to <code>localhost</code>.</p> <p>To run in detached mode, use <code>-d</code> with <code>dstack apply</code>.</p> <p>If you detached the CLI, you can always re-attach to a run via <code>dstack logs -a RUN_NAME</code>.</p>"},{"location":"docs/guides/protips/#gpu","title":"GPU","text":"<p><code>dstack</code> natively supports NVIDIA GPU, AMD GPU, and Google Cloud TPU accelerator chips.</p> <p>The <code>gpu</code> property withing <code>resources</code> (or the <code>--gpu</code> option with <code>dstack apply</code>) allows specifying not only memory size but also GPU vendor, names, their memory, and quantity.</p> <p>Examples:</p> <ul> <li><code>1</code> (any GPU)</li> <li><code>AMD:2</code> (two AMD GPUs)</li> <li><code>A100</code> (A100)</li> <li><code>24GB..</code> (any GPU starting from 24GB)</li> <li><code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB)</li> <li><code>A10G,A100</code> (either A10G or A100)</li> <li><code>A100:80GB</code> (one A100 of 80GB)</li> <li><code>A100:2</code> (two A100)</li> <li><code>MI300X:4</code> (four MI300X)</li> <li><code>A100:40GB:2</code> (two A100 40GB)</li> <li><code>tpu:v2-8</code> (<code>v2</code> Google Cloud TPU with 8 cores)</li> </ul> <p>The GPU vendor is indicated by one of the following case-insensitive values:</p> <ul> <li><code>nvidia</code> (NVIDIA GPUs)</li> <li><code>amd</code> (AMD GPUs)</li> <li><code>tpu</code> (Google Cloud TPUs)</li> </ul> TPU <p>Currently, you can't specify other than 8 TPU cores. This means only single host workloads are supported. Support for multiple hosts is coming soon.</p> AMD <p>Currently, when an AMD GPU is specified, either by name or by vendor, the <code>image</code> property must be specified as well.</p>"},{"location":"docs/guides/protips/#service-quotas","title":"Service quotas","text":"<p>If you're using your own AWS, GCP, Azure, or OCI accounts, before you can use GPUs or spot instances, you have to request the corresponding service quotas for each type of instance in each region.</p> AWS <p>Check this guide   on EC2 service quotas. The relevant service quotas include:</p> <ul> <li><code>Running On-Demand P instances</code> (on-demand V100, A100 80GB x8)</li> <li><code>All P4, P3 and P2 Spot Instance Requests</code> (spot V100, A100 80GB x8)</li> <li><code>Running On-Demand G and VT instances</code> (on-demand T4, A10G, L4)</li> <li><code>All G and VT Spot Instance Requests</code> (spot T4, A10G, L4)</li> <li><code>Running Dedicated p5 Hosts</code> (on-demand H100)</li> <li><code>All P5 Spot Instance Requests</code> (spot H100)</li> </ul> GCP <p>Check this guide   on Compute Engine service quotas. The relevant service quotas include:</p> <ul> <li><code>NVIDIA V100 GPUs</code> (on-demand V100)</li> <li><code>Preemtible V100 GPUs</code> (spot V100)</li> <li><code>NVIDIA T4 GPUs</code> (on-demand T4)</li> <li><code>Preemtible T4 GPUs</code> (spot T4)</li> <li><code>NVIDIA L4 GPUs</code> (on-demand L4)</li> <li><code>Preemtible L4 GPUs</code> (spot L4)</li> <li><code>NVIDIA A100 GPUs</code> (on-demand A100)</li> <li><code>Preemtible A100 GPUs</code> (spot A100)</li> <li><code>NVIDIA A100 80GB GPUs</code> (on-demand A100 80GB)</li> <li><code>Preemtible A100 80GB GPUs</code> (spot A100 80GB)</li> <li><code>NVIDIA H100 GPUs</code> (on-demand H100)</li> <li><code>Preemtible H100 GPUs</code> (spot H100)</li> </ul> Azure <p>Check this guide   on Azure service quotas. The relevant service quotas include:</p> <ul> <li><code>Total Regional Spot vCPUs</code> (any spot instances)</li> <li><code>Standard NCASv3_T4 Family vCPUs</code> (on-demand T4)</li> <li><code>Standard NVADSA10v5 Family vCPUs</code> (on-demand A10)</li> <li><code>Standard NCADS_A100_v4 Family vCPUs</code> (on-demand A100 80GB)</li> <li><code>Standard NDASv4_A100 Family vCPUs</code> (on-demand A100 40GB x8)</li> <li><code>Standard NDAMSv4_A100Family vCPUs</code> (on-demand A100 80GB x8)</li> <li><code>Standard NCadsH100v5 Family vCPUs</code> (on-demand H100)</li> <li><code>Standard NDSH100v5 Family vCPUs</code> (on-demand H100 x8)</li> </ul> OCI <p>Check this guide   on requesting OCI service limits increase. The relevant service category is compute. The relevant resources include:</p> <ul> <li><code>GPUs for GPU.A10 based VM and BM instances</code> (on-demand A10)</li> <li><code>GPUs for GPU2 based VM and BM instances</code> (on-demand P100)</li> <li><code>GPUs for GPU3 based VM and BM instances</code> (on-demand V100)</li> </ul> <p>Note, for AWS, GCP, and Azure, service quota values are measured with the number of CPUs rather than GPUs.</p>"},{"location":"docs/guides/troubleshooting/","title":"Troubleshooting","text":""},{"location":"docs/guides/troubleshooting/#reporting-issues","title":"Reporting issues","text":"<p>When you encounter a problem and need help, it's essential to report it as a GitHub issue . Please avoid brining up the issue to Discord before reporting it.</p> <p>Steps to reproduce</p> <p>Make sure to provide clear, detailed steps to reproduce the issue. This will allow us to troubleshoot it and request any additional information. Include server logs, CLI outputs, and configuration samples.  Avoid using screenshots for logs or errors\u2014use text instead. </p> <p>See these examples for well-reported issues: this  and this .</p>"},{"location":"docs/guides/troubleshooting/#typical-issues","title":"Typical issues","text":""},{"location":"docs/guides/troubleshooting/#provisioning-fails","title":"Provisioning fails","text":"<p>In certain cases, running <code>dstack apply</code> may produce the following output:</p> <pre><code>wet-mangust-1 provisioning completed (failed)\nAll provisioning attempts failed. This is likely due to cloud providers not having enough capacity. Check CLI and server logs for more details.\n</code></pre>"},{"location":"docs/guides/troubleshooting/#backend-configuration","title":"Backend configuration","text":"<p>If runs consistently fail to provision due to insufficient capacity, it\u2019s likely there is a backend configuration issue. Ensure that your backends are configured correctly and check the server logs for any errors.</p>"},{"location":"docs/guides/troubleshooting/#service-quotas","title":"Service quotas","text":"<p>If some runs fail to provision, it may be due to an insufficient service quota. For cloud providers like AWS, GCP, Azure, and OCI, you often need to request an increased service quota before you can use specific instances.</p>"},{"location":"docs/guides/troubleshooting/#resources","title":"Resources","text":"<p>Another possible cause of the insufficient capacity error is that <code>dstack</code> cannot find an instance that meets the requirements specified in <code>resources</code>.</p> GPU <p>The <code>gpu</code> property allows you to specify the GPU name, memory, and quantity. Examples include <code>A100</code> (one GPU), <code>A100:40GB</code> ( one GPU with exact memory), <code>A100:4</code> (four GPUs), etc. If you specify a GPU name without a quantity, it defaults to <code>1</code>. </p> <p>If you request one GPU but only instances with eight GPUs are available, <code>dstack</code> won\u2019t be able to provide it. Use range syntax to specify a range, such as <code>A100:1..8</code> (one to eight GPUs) or <code>A100:1..</code> (one or more GPUs).</p> Disk <p>If you don't specify the <code>disk</code> property, <code>dstack</code> defaults it to <code>100GB</code>.  In case there is no such instance available, <code>dstack</code> won\u2019t be able to provide it.  Use range syntax to specify a range, such as <code>50GB..100GB</code> (from fifty GBs to one hundred GBs) or <code>50GB..</code>  (fifty GBs or more).</p>"},{"location":"docs/guides/troubleshooting/#run-fails","title":"Run fails","text":"<p>There could be several reasons for a run failing after successful provisioning. </p> <p>Termination reason</p> <p>To find out why, use <code>-v</code> (stands for <code>--verbose</code>) with <code>dstack ps</code>. This will show the run's status and any failure reasons.</p>"},{"location":"docs/guides/troubleshooting/#spot-interruption","title":"Spot interruption","text":"<p>If a run fails after provisioning with the termination reason <code>INTERRUPTED_BY_NO_CAPACITY</code>, it is likely that the run was using spot instances and was interrupted. To address this, you can either set the <code>spot_policy</code> to <code>on-demand</code> or specify the  <code>retry</code> property.</p>"},{"location":"docs/guides/troubleshooting/#cant-run-a-service","title":"Can't run a service","text":""},{"location":"docs/guides/troubleshooting/#gateway-configuration","title":"Gateway configuration","text":"<p>The most common reason a service fails to start is either because you haven\u2019t created a gateway or haven\u2019t set up the correct DNS record pointing to the gateway's hostname.</p>"},{"location":"docs/guides/troubleshooting/#service-endpoint-doesnt-work","title":"Service endpoint doesn't work","text":""},{"location":"docs/guides/troubleshooting/#authorization","title":"Authorization","text":"<p>If the service endpoint returns a 403 error, it is likely because the <code>Authorization</code>  header with the correct <code>dstack</code> token was not provided.</p>"},{"location":"docs/guides/troubleshooting/#ssh-fleets","title":"SSH fleets","text":"<p>If you attempt to run a service on an SSH fleet, it won't work due to a known issue  that is expected to be fixed soon.</p>"},{"location":"docs/guides/troubleshooting/#cannot-access-a-dev-environment-or-a-tasks-ports","title":"Cannot access a dev environment or a task's ports","text":"<p>When running a dev environment or task with configured ports, <code>dstack apply</code>  automatically forwards remote ports to <code>localhost</code> via SSH for easy and secure access. If you interrupt the command, the port forwarding will be disconnected. To reattach, use <code>dstack logs --attach &lt;run name</code>.</p>"},{"location":"docs/guides/troubleshooting/#windows","title":"Windows","text":"<p>If you're using the CLI on Windows, make sure to run it through WSL by following these instructions.  Native support will be available soon.</p>"},{"location":"docs/guides/troubleshooting/#an-ssh-fleet-doesnt-provision","title":"An SSH fleet doesn't provision","text":"<p>If you set up an SSH fleet and it fails to provision after a long wait, first check the server logs.  Also, review the  <code>/root/.dstack/shim.log</code> file on each host used to create the fleet.</p>"},{"location":"docs/guides/troubleshooting/#questions","title":"Questions","text":"<p>Community</p> <p>If you have a question, please feel free to ask it in our Discord server.</p>"},{"location":"docs/installation/","title":"Installation","text":"<p>To use the open-source version of <code>dstack</code> with your own cloud accounts or on-prem clusters, follow this guide.</p> <p>If you don't want to host the <code>dstack</code> server (or want to access GPU marketplace), skip installation and proceed to dstack Sky .</p>"},{"location":"docs/installation/#configure-backends","title":"Configure backends","text":"<p>To use <code>dstack</code> with your own cloud accounts, or Kubernetes, create the <code>~/.dstack/server/config.yml</code> file and configure backends.</p>"},{"location":"docs/installation/#start-the-server","title":"Start the server","text":"<p>Once backends are configured, proceed and start the server:</p> pipDocker <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <pre><code>$ docker run -p 3000:3000 \\\n    -v $HOME/.dstack/server/:/root/.dstack/server \\\n    dstackai/dstack\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <p>For more details on how to deploy <code>dstack</code> using Docker, check its Docker repo.</p> <p>The <code>dstack</code> server can run anywhere: on your laptop, a dedicated server, or in the cloud. Once it's up, you can use either the CLI or the API.</p> State <p>By default, the <code>dstack</code> server stores its state locally in <code>~/.dstack/server</code>. To store it externally, use the <code>DSTACK_DATABASE_URL</code> and  <code>DSTACK_SERVER_CLOUDWATCH_LOG_GROUP</code> environment variables.</p> <p>If you want backend credentials and user tokens to be encrypted, you can set up encryption keys via <code>~/.dstack/server/config.yml</code>.</p>"},{"location":"docs/installation/#set-up-the-cli","title":"Set up the CLI","text":"<p>To point the CLI to the <code>dstack</code> server, configure it with the server address, user token, and project name:</p> <pre><code>$ pip install dstack\n$ dstack config --url http://127.0.0.1:3000 \\\n    --project main \\\n    --token bbae0f28-d3dd-4820-bf61-8f4bb40815da\n\nConfiguration is updated at ~/.dstack/config.yml\n</code></pre> <p>This configuration is stored in <code>~/.dstack/config.yml</code>.</p> Windows <p>If you're using the CLI on Windows, make sure to run it through WSL by following these instructions .  Native support will be available soon.</p>"},{"location":"docs/installation/#create-ssh-fleets","title":"Create SSH fleets","text":"<p>If you want the <code>dstack</code> server to run containers on your on-prem clusters, create SSH fleets.</p>"},{"location":"docs/installation/#whats-next","title":"What's next?","text":"<ol> <li>Check the server/config.yml reference on how to configure backends</li> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<ul> <li><code>dev-environment</code></li> <li><code>task</code></li> <li><code>service</code></li> </ul>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Sometimes, you may want to reuse the same parameters across different <code>.dstack.yml</code> configurations.</p> <p>This can be achieved by defining those parameters in a profile.</p> <p>Profiles can be defined on the repository level (via the <code>.dstack/profiles.yml</code> file in the root directory of the repository) or on the global level (via the <code>~/.dstack/profiles.yml</code> file).</p> <p>Any profile can be marked as default so that it will be applied automatically for any run. Otherwise, you can refer to a specific profile via <code>--profile NAME</code> in <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: my-profile\n\n    # The spot pololicy can be \"spot\", \"on-demand\", or \"auto\"\n    spot_policy: auto\n\n    # Limit the maximum price of the instance per hour\n    max_price: 1.5\n\n    # Stop any run if it runs longer that this duration\n    max_duration: 1d\n\n    # Use only these backends\n    backends: [azure, lambda]\n\n    # If set to true, this profile will be applied automatically\n    default: true\n</code></pre> <p>The profile configuration supports many properties. See below.</p>"},{"location":"docs/reference/profiles.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/profiles.yml/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/profiles.yml/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/profiles.yml/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/profiles.yml/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/profiles.yml/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/profiles.yml/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/profiles.yml/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/profiles.yml/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/profiles.yml/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/profiles.yml/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/profiles.yml/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/profiles.yml/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/profiles.yml/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/profiles.yml/#name","title":"<code>name</code> -  The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code>.","text":""},{"location":"docs/reference/profiles.yml/#default","title":"<code>default</code> - (Optional) If set to true, <code>dstack run</code> will use this profile by default..","text":""},{"location":"docs/reference/profiles.yml/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/profiles.yml/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/profiles.yml/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/api/python/","title":"Python API","text":"<p>The Python API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and displaying its logs.</p> <pre><code>import sys\n\nfrom dstack.api import Task, GPU, Client, Resources\n\nclient = Client.from_config()\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",  # If not specified, a random name is assigned \n    configuration=task,\n    repo=None, # Specify to mount additional files\n)\n\nrun.attach()\n\ntry:\n    for log in run.logs():\n        sys.stdout.buffer.write(log)\n        sys.stdout.buffer.flush()\nexcept KeyboardInterrupt:\n    run.stop(abort=True)\nfinally:\n    run.detach()\n</code></pre> <p>NOTE:</p> <ol> <li>The <code>configuration</code> argument in the <code>submit</code> method can be either <code>dstack.api.Task</code> or <code>dstack.api.Service</code>. </li> <li>If you create <code>dstack.api.Task</code> or <code>dstack.api.Service</code>, you may specify the <code>image</code> argument. If <code>image</code> isn't    specified, the default image will be used. For a private Docker registry, ensure you also pass the <code>registry_auth</code> argument.</li> <li>The <code>repo</code> argument in the <code>submit</code> method allows the mounting of a local folder, a remote repo, or a    programmatically created repo. In this case, the <code>commands</code> argument can refer to the files within this repo.</li> <li>The <code>attach</code> method waits for the run to start and, for <code>dstack.api.Task</code> sets up an SSH tunnel and forwards configured <code>ports</code> to <code>localhost</code>.</li> </ol>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>runs</code> <code>RunCollection</code> <p>Operations with runs.</p> <code>repos</code> <code>RepoCollection</code> <p>Operations with repositories.</p> <code>backends</code> <code>BackendCollection</code> <p>Operations with backends.</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using the default configuration from <code>~/.dstack/config.yml</code> if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>The dstack server URL (e.g. <code>http://localhost:3000/</code> or <code>https://sky.dstack.ai</code>)</p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>The dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for SSH tunneling</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>A client instance</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.RunCollection</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, regions=None, instance_types=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>Union[Task, Service]</code> <p>A run configuration.</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>The path to the configuration file, relative to the root directory of the repo.</p> <code>None</code> <code>repo</code> <code>Union[LocalRepo, RemoteRepo, VirtualRepo]</code> <p>A repo to mount to the run.</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>A list of allowed backend for provisioning.</p> <code>None</code> <code>regions</code> <code>Optional[List[str]]</code> <p>A list of cloud regions for provisioning.</p> <code>None</code> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration. Overrides the configuration's resources.</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>A spot policy for provisioning.</p> <code>None</code> <code>retry_policy</code> <code>RetryPolicy</code> <p>A retry policy.</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>The max instance running duration in seconds.</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>The max instance price in dollars per hour for provisioning.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>A working directory relative to the repo root directory</p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>A desired name of the run. Must be unique in the project. If not specified, a random name is assigned.</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>Whether local ports should be reserved in advance.</p> <code>True</code> <p>Returns:</p> Type Description <code>Run</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.RepoCollection</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Initializes the repo and configures its credentials in the project. Must be invoked before mounting the repo to a run.</p> <p>Example:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\",\n)\nclient.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Once the repo is initialized, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The repo to initialize.</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for accessing the remote repo.</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>The GitHub OAuth token to access the remote repo.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":""},{"location":"docs/reference/api/python/#nodes","title":"<code>nodes</code> - (Optional) Number of nodes. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The run name.","text":""},{"location":"docs/reference/api/python/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/api/python/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/api/python/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/api/python/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/api/python/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/api/python/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/api/python/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/api/python/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/api/python/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/api/python/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/api/python/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/api/python/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/api/python/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/api/python/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/api/python/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/api/python/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/api/python/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/api/python/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/api/python/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/api/python/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/api/python/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/api/python/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/api/python/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/api/python/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":""},{"location":"docs/reference/api/python/#port","title":"<code>port</code> -  The port, that application listens on or the mapping.","text":""},{"location":"docs/reference/api/python/#model","title":"<code>model</code> - (Optional) Mapping of the model for the OpenAI-compatible endpoint.","text":""},{"location":"docs/reference/api/python/#https","title":"<code>https</code> - (Optional) Enable HTTPS. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/api/python/#auth","title":"<code>auth</code> - (Optional) Enable the authorization. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/api/python/#replicas","title":"<code>replicas</code> - (Optional) The number of replicas. Can be a number (e.g. <code>2</code>) or a range (<code>0..4</code> or <code>1..8</code>). If it's a range, the <code>scaling</code> property is required. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#_scaling","title":"<code>scaling</code> - (Optional) The auto-scaling rules. Required if <code>replicas</code> is set to a range.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The run name.","text":""},{"location":"docs/reference/api/python/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/api/python/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/api/python/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/api/python/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/api/python/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/api/python/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/api/python/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/api/python/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/api/python/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/api/python/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/api/python/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/api/python/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/api/python/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/api/python/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/api/python/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/api/python/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/api/python/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/api/python/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/api/python/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/api/python/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/api/python/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/api/python/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/api/python/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>RunStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None, bind_address=None)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False, replica_num=0, job_num=0)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/#dstack.api.Resources","title":"<code>dstack.api.Resources</code>","text":""},{"location":"docs/reference/api/python/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/api/python/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/api/python/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/api/python/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements.","text":""},{"location":"docs/reference/api/python/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/api/python/#dstack.api.GPU","title":"<code>dstack.api.GPU</code>","text":""},{"location":"docs/reference/api/python/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The name of the GPU (e.g., <code>A100</code> or <code>H100</code>).","text":""},{"location":"docs/reference/api/python/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/api/python/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/api/python/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/api/python/#dstack.api.Disk","title":"<code>dstack.api.Disk</code>","text":""},{"location":"docs/reference/api/python/#size","title":"<code>size</code> -  Disk size.","text":""},{"location":"docs/reference/api/python/#dstack.api.LocalRepo","title":"<code>dstack.api.LocalRepo</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Example:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=LocalRepo.from_dir(\".\"), # Mount the current folder to the run\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.LocalRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>LocalRepo</code> <p>A local repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo","title":"<code>dstack.api.RemoteRepo</code>","text":"<p>Creates an instance of a remote Git repo for mounting to a submitted run.</p> <p>Using a locally checked-out remote Git repo:</p> <pre><code>repo=RemoteRepo.from_dir(repo_dir=\".\")\n</code></pre> <p>Using a remote Git repo by a URL:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\"\n)\n</code></pre> <p>Initialize the repo before mounting it.</p> <pre><code>client.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Finally, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_url","title":"<code>from_url(repo_url, repo_branch=None, repo_hash=None)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>The URL of a remote Git repo</p> required <code>repo_branch</code> <code>Optional[str]</code> <p>The name of the remote branch. Must be specified if <code>hash</code> is not specified.</p> <code>None</code> <code>repo_hash</code> <code>Optional[str]</code> <p>The hash of the revision. Must be specified if <code>branch</code> is not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo","title":"<code>dstack.api.VirtualRepo</code>","text":"<p>Allows mounting a repo created programmatically.</p> <p>Example:</p> <pre><code>virtual_repo = VirtualRepo(repo_id=\"some-unique-repo-id\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"requirements.txt\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"train.py\")\n\nrun = client.runs.submit(\n    configuration=...,\n    repo=virtual_repo,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>repo_id</code> <p>A unique identifier of the repo</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file","title":"<code>add_file(path, content)</code>","text":"<p>Adds a given file to the repo.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path inside the repo to add the file.</p> <code>content</code> <code>bytes</code> <p>The contents of the file.</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file_from_package","title":"<code>add_file_from_package(package, path)</code>","text":"<p>Includes a file from a given package to the repo.</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>Union[ModuleType, str]</code> <p>A package to include the file from.</p> <code>path</code> <code>str</code> <p>The path to the file to include to the repo. Must be relative to the package directory.</p>"},{"location":"docs/reference/api/python/#dstack.api.RegistryAuth","title":"<code>dstack.api.RegistryAuth</code>","text":""},{"location":"docs/reference/api/python/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/api/python/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/api/python/#dstack.api.Scaling","title":"<code>dstack.api.Scaling</code>","text":""},{"location":"docs/reference/api/python/#metric","title":"<code>metric</code> -  The target metric to track. Currently, the only supported value is <code>rps</code> (meaning requests per second).","text":""},{"location":"docs/reference/api/python/#target","title":"<code>target</code> -  The target value of the metric. The number of replicas is calculated based on this number and automatically adjusts (scales up or down) as this metric changes.","text":""},{"location":"docs/reference/api/python/#scale_up_delay","title":"<code>scale_up_delay</code> - (Optional) The delay in seconds before scaling up. Defaults to <code>300</code>.","text":""},{"location":"docs/reference/api/python/#scale_down_delay","title":"<code>scale_down_delay</code> - (Optional) The delay in seconds before scaling down. Defaults to <code>600</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.BackendType","title":"<code>dstack.api.BackendType</code>","text":"<p>Attributes:</p> Name Type Description <code>AWS</code> <code>BackendType</code> <p>Amazon Web Services</p> <code>AZURE</code> <code>BackendType</code> <p>Microsoft Azure</p> <code>CUDO</code> <code>BackendType</code> <p>Cudo</p> <code>DSTACK</code> <code>BackendType</code> <p>dstack Sky</p> <code>GCP</code> <code>BackendType</code> <p>Google Cloud Platform</p> <code>DATACRUNCH</code> <code>BackendType</code> <p>DataCrunch</p> <code>KUBERNETES</code> <code>BackendType</code> <p>Kubernetes</p> <code>LAMBDA</code> <code>BackendType</code> <p>Lambda Cloud</p> <code>RUNPOD</code> <code>BackendType</code> <p>Runpod Cloud</p> <code>TENSORDOCK</code> <code>BackendType</code> <p>TensorDock Marketplace</p> <code>VASTAI</code> <code>BackendType</code> <p>Vast.ai Marketplace</p>"},{"location":"docs/reference/api/rest/","title":"REST API","text":""},{"location":"docs/reference/cli/","title":"CLI","text":""},{"location":"docs/reference/cli/#commands","title":"Commands","text":""},{"location":"docs/reference/cli/#dstack-server","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p> <pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL] [--default]\n                     [--no-default] [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to INFO.\n  --default             Update the default project configuration\n  --no-default          Do not update the default project configuration\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/#dstack-init","title":"dstack init","text":"<p>This command must be called inside a folder before you can run <code>dstack apply</code>.</p> <p>Git credentials</p> <p>If the current folder is a remote Git repository, <code>dstack init</code> ensures that <code>dstack</code> can access it. By default, the command uses the remote repo's default Git credentials. These can be overridden with  <code>--git-identity</code> (private SSH key) or <code>--token</code> (OAuth token).</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access the remote repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use git\n</code></pre> <p>User SSH key</p> <p>By default, <code>dstack</code> uses its own SSH key to access instances (<code>~/.dstack/ssh/id_rsa</code>).  It is possible to override this key via the <code>--ssh-identity</code> argument.</p>"},{"location":"docs/reference/cli/#dstack-apply","title":"dstack apply","text":"<p>This command applies a given configuration. If a resource does not exist, <code>dstack apply</code> creates the resource. If a resource exists, <code>dstack apply</code> updates the resource in-place or re-creates the resource if the update is not possible.</p> <pre><code>$ dstack apply --help\nUsage: dstack apply [--project NAME] [-h [TYPE]] [-f FILE] [--force] [-y]\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit.\n  -f, --file FILE       The path to the configuration file. Defaults to\n                        $PWD/.dstack.yml\n  --force               Force apply when no changes detected\n  -y, --yes             Do not ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-delete","title":"dstack delete","text":"<p>This command deletes the resources defined by a given configuration.</p> <pre><code>$ dstack delete --help\nUsage: dstack delete [-h] [--project NAME] [-f FILE] [-y]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -f, --file FILE       The path to the configuration file. Defaults to\n                        $PWD/.dstack.yml\n  -y, --yes             Do not ask for confirmation\n</code></pre> <p>NOTE:</p> <p>The <code>dstack delete</code> command currently supports only <code>gateway</code> configurations. Support for other configuration types is coming soon.</p>"},{"location":"docs/reference/cli/#dstack-ps","title":"dstack ps","text":"<p>This command shows the status of runs.</p> <pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-stop","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p> <pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/#dstack-logs","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p> <pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--replica REPLICA]\n                   [--job JOB]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose\n  -a, --attach          Set up an SSH tunnel, and print logs as they follow.\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --replica REPLICA     The relica number. Defaults to 0.\n  --job JOB             The job number inside the replica. Defaults to 0.\n</code></pre>"},{"location":"docs/reference/cli/#dstack-config","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p> <pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN]\n                     [--default] [--remove] [--no-default]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  --default          Set the project as default. It will be used when\n                     --project is omitted in commands.\n  --remove           Delete project configuration\n  --no-default       Do not prompt to set the project as default\n</code></pre>"},{"location":"docs/reference/cli/#dstack-fleet","title":"dstack fleet","text":"<p>Fleets enable efficient provisioning and management of clusters and instances.</p>"},{"location":"docs/reference/cli/#dstack-fleet-list","title":"dstack fleet list","text":"<p>The <code>dstack fleet list</code> command displays fleets and instances.</p> <pre><code>$ dstack fleet list --help\nUsage: dstack fleet list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-fleet-delete","title":"dstack fleet delete","text":"<p>The <code>dstack fleet delete</code> deletes fleets and instances. Cloud instances are terminated upon deletion.</p> <pre><code>$ dstack fleet delete --help\nUsage: dstack fleet delete [-h] [-i INSTANCE_NUM] [-y] name\n\nPositional Arguments:\n  name                  The name of the fleet\n\nOptions:\n  -h, --help            show this help message and exit\n  -i, --instance INSTANCE_NUM\n                        The instances to delete\n  -y, --yes             Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway","title":"dstack gateway","text":"<p>A gateway is required for running services. It handles ingress traffic, authorization, domain mapping, model mapping for the OpenAI-compatible endpoint, and so on.</p>"},{"location":"docs/reference/cli/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p> <pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-create","title":"dstack gateway create","text":"<p>The <code>dstack gateway create</code> command creates a new gateway instance in the project.</p> <pre><code>$ dstack gateway create --help\nUsage: dstack gateway create [-h] --backend {aws,azure,gcp,kubernetes}\n                             --region REGION [--set-default] [--name NAME]\n                             --domain DOMAIN\n\nOptions:\n  -h, --help            show this help message and exit\n  --backend {aws,azure,gcp,kubernetes}\n  --region REGION\n  --set-default         Set as default gateway for the project\n  --name NAME           Set a custom name for the gateway\n  --domain DOMAIN       Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p> <pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-update","title":"dstack gateway update","text":"<p>The <code>dstack gateway update</code> command updates the specified gateway.</p> <pre><code>$ dstack gateway update --help\nUsage: dstack gateway update [-h] [--set-default] [--domain DOMAIN] name\n\nPositional Arguments:\n  name             The name of the gateway\n\nOptions:\n  -h, --help       show this help message and exit\n  --set-default    Set it the default gateway for the project\n  --domain DOMAIN  Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-volume","title":"dstack volume","text":"<p>The volumes commands.</p>"},{"location":"docs/reference/cli/#dstack-volume-list","title":"dstack volume list","text":"<p>The <code>dstack volume list</code> command lists volumes.</p> <pre><code>$ dstack volume list --help\nUsage: dstack volume list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-volume-delete","title":"dstack volume delete","text":"<p>The <code>dstack volume delete</code> command deletes volumes.</p> <pre><code>$ dstack volume delete --help\nUsage: dstack volume delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the volume\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-run","title":"dstack run","text":"<p>This command runs a given configuration.</p> <p>Deprecation</p> <p><code>dstack run</code> is deprecated in favor of <code>dstack apply</code>.</p> <pre><code>$ dstack run . --help\nUsage: dstack run [--project NAME] [-h [TYPE]] [-f FILE] [-y] [-n RUN_NAME]\n                  [-d] [--max-offers MAX_OFFERS] [-e KEY[=VALUE]] [--gpu SPEC]\n                  [--disk RANGE] [--profile NAME] [--max-price PRICE]\n                  [--max-duration DURATION] [-b NAME] [-r NAME]\n                  [--instance-type NAME]\n                  [--pool POOL_NAME | --reuse | --dont-destroy | --idle-duration IDLE_DURATION | --instance NAME]\n                  [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                  [--retry | --no-retry | --retry-duration DURATION]\n                  working_dir\n\nPositional Arguments:\n  working_dir\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit. TYPE is one of task,\n                        dev-environment, service\n  -f, --file FILE       The path to the configuration file. Defaults to\n                        $PWD/.dstack.yml\n  -y, --yes             Do not ask for confirmation\n  -n, --name RUN_NAME   The name of the run. If not specified, a random name\n                        is assigned\n  -d, --detach          Do not poll logs and run status\n  --max-offers MAX_OFFERS\n                        Number of offers to show in the run plan\n  -e, --env KEY[=VALUE]\n                        Environment variables\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk RANGE          Request the size range of disk for the run. Example\n                        --disk 100GB...\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  --max-duration DURATION\n                        The maximum duration of the run\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nPools:\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n  --instance NAME       Reuse instance from pool with name NAME\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n</code></pre> .gitignore <p>When running anything via CLI, <code>dstack</code> uses the exact version of code from your project directory.</p> <p>If there are large files, consider creating a <code>.gitignore</code> file to exclude them for better performance.</p>"},{"location":"docs/reference/cli/#dstack-pool","title":"dstack pool","text":"<p>Pools allow for managing the lifecycle of instances and reusing them across runs.  The default pool is created automatically.</p> <p>Deprecation</p> <p>Pools are deprecated in favor of fleets and will be removed in 0.19.0.</p>"},{"location":"docs/reference/cli/#dstack-pool-add","title":"dstack pool add","text":"<p>The <code>dstack pool add</code> command provisions a cloud instance and adds it to a pool. If no pool name is specified, the instance goes to the default pool.</p> <pre><code>$ dstack pool add --help\nUsage: dstack pool add [-h] [-y] [--profile NAME] [--max-price PRICE]\n                       [-b NAME] [-r NAME] [--instance-type NAME]\n                       [--pool POOL_NAME] [--reuse] [--dont-destroy]\n                       [--idle-duration IDLE_DURATION]\n                       [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                       [--retry | --no-retry | --retry-duration DURATION]\n                       [--cpu SPEC] [--memory SIZE] [--shared-memory SIZE]\n                       [--gpu SPEC] [--disk SIZE]\n\nOptions:\n  -h, --help            show this help message and exit\n  -y, --yes             Don't ask for confirmation\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n\nResources:\n  --cpu SPEC            Request the CPU count. Default: 2..\n  --memory SIZE         Request the size of RAM. The format is SIZE:MB|GB|TB.\n                        Default: 8GB..\n  --shared-memory SIZE  Request the size of Shared Memory. The format is\n                        SIZE:MB|GB|TB.\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk SIZE           Request the size of disk for the run. Example --disk\n                        100GB...\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-add-ssh","title":"dstack pool add-ssh","text":"<p>The <code>dstack pool add-ssh</code> command adds an existing remote instance to a pool. If no pool name is specified, the instance goes to the default pool.</p> <pre><code>$ dstack pool add-ssh --help\nUsage: dstack pool add-ssh [-h] -i SSH_PRIVATE_KEY [-p SSH_PORT]\n                           [-l LOGIN_NAME] [--region REGION]\n                           [--pool POOL_NAME] [--name INSTANCE_NAME]\n                           [--network NETWORK]\n                           destination\n\nPositional Arguments:\n  destination\n\nOptions:\n  -h, --help            show this help message and exit\n  -i SSH_PRIVATE_KEY    The private SSH key path for SSH\n  -p SSH_PORT           SSH port to connect\n  -l LOGIN_NAME         User to login\n  --region REGION       Host region\n  --pool POOL_NAME      Pool name\n  --name INSTANCE_NAME  Set the name of the instance\n  --network NETWORK     Network address for multinode setup. Format &lt;ip\n                        address&gt;/&lt;netmask&gt;\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-ps","title":"dstack pool ps","text":"<p>The <code>dstack pool ps</code> command lists all active instances of a pool. If no pool name is specified, default pool instances are displayed.</p> <pre><code>$ dstack pool ps --help\nUsage: dstack pool ps [-h] [--pool POOL_NAME] [-w]\n\nShow instances in the pool\n\nOptions:\n  -h, --help        show this help message and exit\n  --pool POOL_NAME  The name of the pool. If not set, the default pool will be\n                    used\n  -w, --watch       Watch instances in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-rm","title":"dstack pool rm","text":"<p>The <code>dstack pool rm</code> command removes an instance from a pool. Cloud instances are terminated upon removal.</p> <pre><code>$ dstack pool rm --help\nUsage: dstack pool rm [-h] [--pool POOL_NAME] [--force] [-y] instance_name\n\nPositional Arguments:\n  instance_name     The name of the instance\n\nOptions:\n  -h, --help        show this help message and exit\n  --pool POOL_NAME  The name of the pool. If not set, the default pool will be\n                    used\n  --force           The name of the instance\n  -y, --yes         Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-create","title":"dstack pool create","text":"<p>The <code>dstack pool create</code> command creates a new pool.</p> <pre><code>$ dstack pool create --help\nUsage: dstack pool create [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-list","title":"dstack pool list","text":"<p>The <code>dstack pool list</code> command lists all existing pools.</p> <pre><code>$ dstack pool list --help\nUsage: dstack pool list [-h] [-v VERBOSE]\n\nList available pools\n\nOptions:\n  -h, --help            show this help message and exit\n  -v, --verbose VERBOSE\n                        Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-set-default","title":"dstack pool set-default","text":"<p>The <code>dstack pool set-default</code> command sets the project's default pool.</p> <pre><code>$ dstack pool set-default --help\nUsage: dstack pool set-default [-h] --pool POOL_NAME\n\nOptions:\n  -h, --help        show this help message and exit\n  --pool POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-delete","title":"dstack pool delete","text":"<p>The <code>dstack pool delete</code> command deletes a specified pool.</p> <pre><code>$ dstack pool delete --help\nUsage: dstack pool delete [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DSTACK_CLI_LOG_LEVEL</code> \u2013 (Optional) Configures CLI logging level. Defaults to <code>INFO</code>.</li> <li><code>DSTACK_SERVER_LOG_LEVEL</code> \u2013 (Optional) Has the same effect as <code>--log-level</code>. Defaults to <code>INFO</code>.</li> <li><code>DSTACK_SERVER_HOST</code> \u2013 (Optional) Has the same effect as <code>--host</code>. Defaults to <code>127.0.0.1</code>.</li> <li><code>DSTACK_SERVER_PORT</code> \u2013 (Optional) Has the same effect as <code>--port</code>. Defaults to <code>3000</code>.</li> <li><code>DSTACK_SERVER_ADMIN_TOKEN</code> \u2013 (Optional) Has the same effect as <code>--token</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_DATABASE_URL</code> \u2013 (Optional) The database URL to use instead of default SQLite. Currently <code>dstack</code> supports Postgres. Example: <code>postgresql+asyncpg://myuser:mypassword@localhost:5432/mydatabase</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_SERVER_CLOUDWATCH_LOG_GROUP</code> \u2013 (Optional) The CloudWatch Logs group for workloads logs. If not set, the default file-based log storage is used.</li> <li><code>DSTACK_SERVER_CLOUDWATCH_LOG_REGION</code> \u2014 (Optional) The CloudWatch Logs region. Defaults to <code>None</code>.</li> <li><code>DSTACK_SERVER_DIR</code> \u2013 (Optional) Sets path to store data and server configs. Defaults to <code>~/.dstack/server</code>.</li> </ul> Internal environment variables <ul> <li><code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> \u2013 (Optional) Sets root logger log level. Defaults to <code>ERROR</code>.</li> <li><code>DSTACK_SERVER_LOG_FORMAT</code> \u2013 (Optional) Sets format of log output. Can be <code>rich</code>, <code>standard</code>, <code>json</code>.. Defaults to <code>rich</code>.</li> <li><code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> \u2013 (Optional) Sets uvicorn logger log level. Defaults to <code>ERROR</code>.</li> <li><code>DSTACK_PROFILE</code> \u2013 (Optional) Has the same effect as <code>--profile</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_PROJECT</code> \u2013 (Optional) Has the same effect as <code>--project</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_RUNNER_VERSION</code> \u2013 (Optional) Sets exact runner version for debug. Defaults to <code>latest</code>.</li> <li><code>DSTACK_DEFAULT_CREDS_DISABLED</code> \u2013 (Optional) Disables default credentials detection if set. Defaults to <code>None</code>.</li> <li><code>DSTACK_LOCAL_BACKEND_ENABLED</code> \u2013 (Optional) Enables local backend for debug if set. Defaults to <code>None</code>.</li> </ul>"},{"location":"docs/reference/dstack.yml/dev-environment/","title":"dev-environment","text":"<p>The <code>dev-environment</code> configuration type allows running dev environments.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n\nide: vscode\n</code></pre> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the corresponding property to true.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nide: vscode\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#docker-image","title":"Docker image","text":"<pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Any custom Docker image\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nide: vscode\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry. </p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Any private Docker image\nimage: ghcr.io/huggingface/text-generation-inference:latest\n# Credentials of the private Docker registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\nide: vscode\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n  # Shared memory (required by multi-gpu)\n  shm_size: 16GB\n  # Disk size\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\nresources:\n  gpu: v2-8\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single TPU device workloads. Multi-TPU support is coming soon.</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#environment-variables","title":"Environment variables","text":"<pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\nide: vscode\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.envrc</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#system-environment-variables","title":"System environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/reference/dstack.yml/dev-environment/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\n# Use either spot or on-demand instances\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for dev environments is <code>on-demand</code>.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\n# Use only listed backends\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\n# Use only listed regions\nregions: [eu-west-1, eu-west-2]\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/#volumes","title":"Volumes","text":"<p>Volumes allow you to persist data between runs. To attach a volume, simply specify its name using the <code>volumes</code> property and specify where to mount its contents:</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\n# Map the name of the volume to any path\nvolumes:\n  - name: my-new-volume\n    path: /volume_data\n</code></pre> <p>Once you run this configuration, the contents of the volume will be attached to <code>/volume_data</code> inside the dev environment, and its contents will persist across runs.</p> Limitations <p>When you're running a dev environment, task, or service with <code>dstack</code>, it automatically mounts the project folder contents to <code>/workflow</code> (and sets that as the current working directory). Right now, <code>dstack</code> doesn't allow you to  attach volumes to <code>/workflow</code> or any of its subdirectories.</p> <p>The <code>dev-environment</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ide","title":"<code>ide</code> -  The IDE to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#version","title":"<code>version</code> - (Optional) The version of the IDE.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#init","title":"<code>init</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> - (Optional) The run name.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-gpu","title":"<code>resources.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-disk","title":"<code>resources.disk</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#volumes_1","title":"<code>volumes</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> -  The name of the volume to mount.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#path","title":"<code>path</code> -  The container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/fleet/","title":"fleet","text":"<p>The <code>fleet</code> configuration type allows creating and updating fleets.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>fleet.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/fleet/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/fleet/#cloud","title":"Cloud","text":"<pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\n# The number of instances\nnodes: 4\n# Ensure the instances are interconnected\nplacement: cluster\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n</code></pre>"},{"location":"docs/reference/dstack.yml/fleet/#ssh","title":"SSH","text":"<pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-ssh-fleet\n\n# Ensure instances are interconnected\nplacement: cluster\n\n# The user, private SSH key, and hostnames of the on-prem servers\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre>"},{"location":"docs/reference/dstack.yml/fleet/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/fleet/#name","title":"<code>name</code> - (Optional) The fleet name.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_ssh_config","title":"<code>ssh_config</code> - (Optional) The parameters for adding instances via SSH.","text":""},{"location":"docs/reference/dstack.yml/fleet/#nodes","title":"<code>nodes</code> - (Optional) The number of instances.","text":""},{"location":"docs/reference/dstack.yml/fleet/#placement","title":"<code>placement</code> - (Optional) The placement of instances: <code>any</code> or <code>cluster</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_resources","title":"<code>resources</code> - (Optional) The resources requirements.","text":""},{"location":"docs/reference/dstack.yml/fleet/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_retry","title":"<code>retry</code> - (Optional) The policy for provisioning retry. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/fleet/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying idle instances. Defaults to <code>3d</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#ssh_config","title":"<code>ssh_config</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#user","title":"<code>user</code> - (Optional) The user to log in with on all hosts.","text":""},{"location":"docs/reference/dstack.yml/fleet/#port","title":"<code>port</code> - (Optional) The SSH port to connect to.","text":""},{"location":"docs/reference/dstack.yml/fleet/#identity_file","title":"<code>identity_file</code> - (Optional) The private key to use for all hosts.","text":""},{"location":"docs/reference/dstack.yml/fleet/#hosts","title":"<code>hosts</code> -  The per host connection parameters: a hostname or an object that overrides default ssh parameters.","text":""},{"location":"docs/reference/dstack.yml/fleet/#network","title":"<code>network</code> - (Optional) The network address for cluster setup in the format <code>&lt;ip&gt;/&lt;netmask&gt;</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#ssh_confighostsn","title":"<code>ssh_config.hosts[n]</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#hostname","title":"<code>hostname</code> -  The IP address or domain to connect to.","text":""},{"location":"docs/reference/dstack.yml/fleet/#port","title":"<code>port</code> - (Optional) The SSH port to connect to for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#user","title":"<code>user</code> - (Optional) The user to log in with for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#identity_file","title":"<code>identity_file</code> - (Optional) The private key to use for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/fleet/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object.","text":""},{"location":"docs/reference/dstack.yml/fleet/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/","title":"gateway","text":"<p>The <code>gateway</code> configuration type allows creating and updating gateways.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>gateway.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/gateway/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/gateway/#new-gateway","title":"Creating a new gateway","text":"<pre><code>type: gateway\n# A name of the gateway\nname: example-gateway\n\n# Gateways are bound to a specific backend and region\nbackend: aws\nregion: eu-west-1\n\n# This domain will be used to access the endpoint\ndomain: example.com\n</code></pre>"},{"location":"docs/reference/dstack.yml/gateway/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/gateway/#name","title":"<code>name</code> - (Optional) The gateway name.","text":""},{"location":"docs/reference/dstack.yml/gateway/#default","title":"<code>default</code> - (Optional) Make the gateway default.","text":""},{"location":"docs/reference/dstack.yml/gateway/#backend","title":"<code>backend</code> -  The gateway backend.","text":""},{"location":"docs/reference/dstack.yml/gateway/#region","title":"<code>region</code> -  The gateway region.","text":""},{"location":"docs/reference/dstack.yml/gateway/#domain","title":"<code>domain</code> - (Optional) The gateway domain, e.g. <code>example.com</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#public_ip","title":"<code>public_ip</code> - (Optional) Allocate public IP for the gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#_certificate","title":"<code>certificate</code> - (Optional) The SSL certificate configuration. Defaults to <code>type: lets-encrypt</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#certificatetypelets-encrypt","title":"<code>certificate[type=lets-encrypt]</code>","text":""},{"location":"docs/reference/dstack.yml/gateway/#type","title":"<code>type</code> -  Automatic certificates by Let's Encrypt. Must be <code>lets-encrypt</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#certificatetypeacm","title":"<code>certificate[type=acm]</code>","text":""},{"location":"docs/reference/dstack.yml/gateway/#type","title":"<code>type</code> -  Certificates by AWS Certificate Manager (ACM). Must be <code>acm</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#arn","title":"<code>arn</code> -  The ARN of the wildcard ACM certificate for the domain.","text":""},{"location":"docs/reference/dstack.yml/service/","title":"service","text":"<p>The <code>service</code> configuration type allows running services.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>serve.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/service/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the corresponding property to true.</p> <p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\n # Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#docker-image","title":"Docker image","text":"<pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Any custom Docker image\nimage: dstackai/base:py3.10-0.5-cuda-12.1\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre> Private Docker registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Any private Docker iamge\nimage: dstackai/base:py3.10-0.5-cuda-12.1\n# Credentials of the private registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\n# Commands of the service  \ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#model-mapping","title":"Model gateway","text":"<p>By default, if you run a service, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>If you run a model, you can optionally configure the mapping to make it accessible via the  OpenAI-compatible interface.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama31-service\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096\n# Expose the port of the service\nport: 8000\n\nresources:\n  # Change to what is required\n  gpu: 24GB\n\n# Comment if you don't want to access the model via https://gateway.&lt;gateway domain&gt;\nmodel:\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n  format: openai\n</code></pre> <p>In this case, with such a configuration, once the service is up, you'll be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <p>The <code>format</code> supports only <code>tgi</code> (Text Generation Inference) and <code>openai</code> (if you are using Text Generation Inference or vLLM with OpenAI-compatible mode).</p> Chat template <p>By default, <code>dstack</code> loads the chat template from the model's repository. If it is not present there, manual configuration is required.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code --quantize gptq\nport: 8000\n\nresources:\n  gpu: 80GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Llama-2-13B-chat-GPTQ\n  format: tgi\n  chat_template: \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\\n' + system_message + '\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '&lt;s&gt;[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' &lt;/s&gt;' }}{% endif %}{% endfor %}\"\n  eos_token: \"&lt;/s&gt;\"\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#limitations","title":"Limitations","text":"<p>Please note that model mapping is an experimental feature with the following limitations:</p> <ol> <li>Doesn't work if your <code>chat_template</code> uses <code>bos_token</code>. As a workaround, replace <code>bos_token</code> inside <code>chat_template</code> with the token content itself.</li> <li>Doesn't work if <code>eos_token</code> is defined in the model repository as a dictionary. As a workaround, set <code>eos_token</code> manually, as shown in the example above (see Chat template).</li> </ol> <p>If you encounter any other issues, please make sure to file a GitHub issue.</p>"},{"location":"docs/reference/dstack.yml/service/#auto-scaling","title":"Auto-scaling","text":"<p>By default, <code>dstack</code> runs a single replica of the service. You can configure the number of replicas as well as the auto-scaling rules.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama31-service\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096\n# Expose the port of the service\nport: 8000\n\nresources:\n  # Change to what is required\n  gpu: 24GB\n\n# Minimum and maximum number of replicas\nreplicas: 1..4\nscaling:\n  # Requests per seconds\n  metric: rps\n  # Target metric value\n  target: 10\n</code></pre> <p>The <code>replicas</code> property can be a number or a range.</p> <p>The <code>metric</code> property of <code>scaling</code> only supports the <code>rps</code> metric (requests per second). In this  case <code>dstack</code> adjusts the number of replicas (scales up or down) automatically based on the load. </p> <p>Setting the minimum number of replicas to <code>0</code> allows the service to scale down to zero when there are no requests.</p>"},{"location":"docs/reference/dstack.yml/service/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size $DSTACK_GPUS_NUM\n# Expose the port of the service\nport: 8000\n\nresources:\n  # 2 GPUs of 80GB\n  gpu: 80GB:2\n\n  # Minimum disk size\n  disk: 200GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#authorization","title":"Authorization","text":"<p>By default, the service endpoint requires the <code>Authorization</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>. Authorization can be disabled by setting <code>auth</code> to <code>false</code>.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Disable authorization\nauth: false\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#environment-variables","title":"Environment variables","text":"<pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama-2-7b-service\n\npython: \"3.10\"\n\n# Environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\n# Commands of the service\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\n# The port of the service\nport: 8000\n\nresources:\n  # Required GPU vRAM\n  gpu: 24GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above), <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.envrc</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#system-environment-variables","title":"System environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/reference/dstack.yml/service/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n\n# Use either spot or on-demand instances\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for services is <code>on-demand</code>.</p>"},{"location":"docs/reference/dstack.yml/service/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n\n# Use only listed backends\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n\n# Use only listed regions\nregions: [eu-west-1, eu-west-2]\n</code></pre>"},{"location":"docs/reference/dstack.yml/service/#volumes","title":"Volumes","text":"<p>Volumes allow you to persist data between runs. To attach a volume, simply specify its name using the <code>volumes</code> property and specify where to mount its contents:</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n\n# Map the name of the volume to any path\nvolumes:\n  - name: my-new-volume\n    path: /volume_data\n</code></pre> <p>Once you run this configuration, the contents of the volume will be attached to <code>/volume_data</code> inside the service,  and its contents will persist across runs.</p> <p>The <code>service</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/service/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/service/#port","title":"<code>port</code> -  The port, that application listens on or the mapping.","text":""},{"location":"docs/reference/dstack.yml/service/#model","title":"<code>model</code> - (Optional) Mapping of the model for the OpenAI-compatible endpoint.","text":""},{"location":"docs/reference/dstack.yml/service/#https","title":"<code>https</code> - (Optional) Enable HTTPS. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#auth","title":"<code>auth</code> - (Optional) Enable the authorization. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#replicas","title":"<code>replicas</code> - (Optional) The number of replicas. Can be a number (e.g. <code>2</code>) or a range (<code>0..4</code> or <code>1..8</code>). If it's a range, the <code>scaling</code> property is required. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_scaling","title":"<code>scaling</code> - (Optional) The auto-scaling rules. Required if <code>replicas</code> is set to a range.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> - (Optional) The run name.","text":""},{"location":"docs/reference/dstack.yml/service/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/service/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/service/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/service/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/service/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/service/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/service/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/service/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/service/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/service/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/service/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/service/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/service/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#model_1","title":"<code>model</code>","text":""},{"location":"docs/reference/dstack.yml/service/#type","title":"<code>type</code> -  The type of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The name of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#format","title":"<code>format</code> -  The serving format. Supported values include <code>openai</code> and <code>tgi</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#scaling","title":"<code>scaling</code>","text":""},{"location":"docs/reference/dstack.yml/service/#metric","title":"<code>metric</code> -  The target metric to track. Currently, the only supported value is <code>rps</code> (meaning requests per second).","text":""},{"location":"docs/reference/dstack.yml/service/#target","title":"<code>target</code> -  The target value of the metric. The number of replicas is calculated based on this number and automatically adjusts (scales up or down) as this metric changes.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_up_delay","title":"<code>scale_up_delay</code> - (Optional) The delay in seconds before scaling up. Defaults to <code>300</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_down_delay","title":"<code>scale_down_delay</code> - (Optional) The delay in seconds before scaling down. Defaults to <code>600</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/service/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/service/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/service/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/service/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/service/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/service/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object.","text":""},{"location":"docs/reference/dstack.yml/service/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/service/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/service/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/service/#volumes_1","title":"<code>volumes</code>","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The name of the volume to mount.","text":""},{"location":"docs/reference/dstack.yml/service/#path","title":"<code>path</code> -  The container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/task/","title":"task","text":"<p>The <code>task</code> configuration type allows running tasks.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/task/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the corresponding property to true.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#_ports","title":"Ports","text":"<p>A task can configure ports. In this case, if the task is running an application on a port, <code>dstack run</code>  will securely allow you to access this port from your local machine through port forwarding.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python fine-tuning/qlora/train.py\n# Expose the port to access TensorBoard\nports:\n  - 6000\n</code></pre> <p>When running it, <code>dstack run</code> forwards <code>6000</code> port to <code>localhost:6000</code>, enabling secure access.</p>"},{"location":"docs/reference/dstack.yml/task/#docker-image","title":"Docker image","text":"<pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# Any custom Docker image\nimage: dstackai/base:py3.10-0.5-cuda-12.1\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: train\n\n# Any private Docker image\nimage: dstackai/base:py3.10-0.5-cuda-12.1\n# Credentials of the private Docker registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#_resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n  # Shared memory (required by multi-gpu)\n  shm_size: 16GB\n  # Disk size\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train\n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install torch~=2.3.0 torch_xla[tpu]~=2.3.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\n  - git clone --recursive https://github.com/pytorch/xla.git\n  - python3 xla/test/test_train_mp_imagenet.py --fake_data --model=resnet50 --num_epochs=1\n\nresources:\n  gpu: v2-8\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single host workloads. Multi-host support is coming soon.</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#environment-variables","title":"Environment variables","text":"<pre><code>type: task\n\npython: \"3.10\"\n\n# Environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HUGGING_FACE_HUB_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p>For instance, you can define environment variables in a <code>.envrc</code> file and utilize tools like <code>direnv</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#system-environment-variables","title":"System environment variables","text":"<p>The following environment variables are available in any run and are passed by <code>dstack</code> by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run <code>DSTACK_NODES_NUM</code> The number of nodes in the run <code>DSTACK_NODE_RANK</code> The rank of the node <code>DSTACK_MASTER_NODE_IP</code> The internal IP address the master node"},{"location":"docs/reference/dstack.yml/task/#distributed-tasks","title":"Distributed tasks","text":"<p>By default, the task runs on a single node. However, you can run it on a cluster of nodes.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train-distrib\n\n# The size of the cluster\nnodes: 2\n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r requirements.txt\n  - torchrun\n    --nproc_per_node=$DSTACK_GPUS_PER_NODE\n    --node_rank=$DSTACK_NODE_RANK\n    --nnodes=$DSTACK_NODES_NUM\n    --master_addr=$DSTACK_MASTER_NODE_IP\n    --master_port=8008 resnet_ddp.py\n    --num_epochs 20\n\nresources:\n  gpu: 24GB\n</code></pre> <p>If you run the task, <code>dstack</code> first provisions the master node and then runs the other nodes of the cluster. All nodes are provisioned in the same region.</p> <p><code>dstack</code> is easy to use with <code>accelerate</code>, <code>torchrun</code>, and other distributed frameworks. All you need to do is pass the corresponding environment variables such as <code>DSTACK_GPUS_PER_NODE</code>, <code>DSTACK_NODE_RANK</code>, <code>DSTACK_NODES_NUM</code>, <code>DSTACK_MASTER_NODE_IP</code>, and <code>DSTACK_GPUS_NUM</code> (see System environment variables).</p> Backends <p>Running on multiple nodes is supported only with the <code>aws</code>, <code>gcp</code>, <code>azure</code>, <code>oci</code> backends, or SSH fleets.</p>"},{"location":"docs/reference/dstack.yml/task/#web-applications","title":"Web applications","text":"<p>Here's an example of using <code>ports</code> to run web apps with <code>tasks</code>. </p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: streamlit-hello\n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\n# Expose the port to access the web app\nports: \n  - 8501\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#spot-policy","title":"Spot policy","text":"<p>You can choose whether to use spot instances, on-demand instances, or any available type.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# Use either spot or on-demand instances\nspot_policy: auto\n</code></pre> <p>The <code>spot_policy</code> accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>. The default for tasks is <code>on-demand</code>.</p>"},{"location":"docs/reference/dstack.yml/task/#queueing-tasks","title":"Queueing tasks","text":"<p>By default, if <code>dstack apply</code> cannot find capacity, the task fails. </p> <p>To queue the task and wait for capacity, specify the <code>retry</code>  property:</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nretry:\n  # Retry on no-capacity errors\n  on_events: [no-capacity]\n  # Retry within 1 day\n  duration: 1d\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#backends_1","title":"Backends","text":"<p>By default, <code>dstack</code> provisions instances in all configured backends. However, you can specify the list of backends:</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# Use only listed backends\nbackends: [aws, gcp]\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#regions_1","title":"Regions","text":"<p>By default, <code>dstack</code> uses all configured regions. However, you can specify the list of regions:</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# Use only listed regions\nregions: [eu-west-1, eu-west-2]\n</code></pre>"},{"location":"docs/reference/dstack.yml/task/#volumes","title":"Volumes","text":"<p>Volumes allow you to persist data between runs. To attach a volume, simply specify its name using the <code>volumes</code> property and specify where to mount its contents:</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# Map the name of the volume to any path\nvolumes:\n  - name: my-new-volume\n    path: /volume_data\n</code></pre> <p>Once you run this configuration, the contents of the volume will be attached to <code>/volume_data</code> inside the task,  and its contents will persist across runs.</p> <p>Limitations</p> <p>When you're running a dev environment, task, or service with <code>dstack</code>, it automatically mounts the project folder contents to <code>/workflow</code> (and sets that as the current working directory). Right now, <code>dstack</code> doesn't allow you to  attach volumes to <code>/workflow</code> or any of its subdirectories.</p> <p>The <code>task</code> configuration type supports many other options. See below.</p>"},{"location":"docs/reference/dstack.yml/task/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/task/#nodes","title":"<code>nodes</code> - (Optional) Number of nodes. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> - (Optional) The run name.","text":""},{"location":"docs/reference/dstack.yml/task/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/task/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/task/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/task/#home_dir","title":"<code>home_dir</code> - (Optional) The absolute path to the home directory inside the container. Defaults to <code>/root</code>. Defaults to <code>/root</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/task/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/task/#setup","title":"<code>setup</code> - (Optional) The bash commands to run on the boot.","text":""},{"location":"docs/reference/dstack.yml/task/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/task/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/task/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/task/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/task/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_retry_policy","title":"<code>retry_policy</code> - (Optional) The policy for resubmitting the run. Deprecated in favor of <code>retry</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is forced to stop. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/task/#pool_name","title":"<code>pool_name</code> - (Optional) The name of the pool. If not set, dstack will use the default name.","text":""},{"location":"docs/reference/dstack.yml/task/#instance_name","title":"<code>instance_name</code> - (Optional) The name of the instance.","text":""},{"location":"docs/reference/dstack.yml/task/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_policy","title":"<code>termination_policy</code> - (Optional) The policy for instance termination. Defaults to <code>destroy-after-idle</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Time to wait before destroying the idle instance. Defaults to <code>5m</code> for <code>dstack run</code> and to <code>3d</code> for <code>dstack pool add</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/task/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/task/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/task/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/task/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/task/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/task/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/task/#size","title":"<code>size</code> -  The disk size. Can be a string (e.g., <code>100GB</code> or <code>100GB..</code>) or an object.","text":""},{"location":"docs/reference/dstack.yml/task/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/task/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/task/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/task/#volumesn","title":"<code>volumes[n]</code>","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> -  The name of the volume to mount.","text":""},{"location":"docs/reference/dstack.yml/task/#path","title":"<code>path</code> -  The container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/volume/","title":"volume","text":"<p>The <code>volume</code> configuration type allows creating, registering, and updating volumes.</p> <p>Configuration files must be inside the project repo, and their names must end with <code>.dstack.yml</code>  (e.g. <code>.dstack.yml</code> or <code>fleet.dstack.yml</code> are both acceptable). Any configuration can be run via <code>dstack apply</code>.</p>"},{"location":"docs/reference/dstack.yml/volume/#examples","title":"Examples","text":""},{"location":"docs/reference/dstack.yml/volume/#new-volume","title":"Creating a new volume","text":"<pre><code>type: volume\n# The name of the volume\nname: my-new-volume\n\n# Volumes are bound to a specific backend and region\nbackend: aws\nregion: eu-central-1\n\n# The size of the volume\nsize: 100GB\n</code></pre>"},{"location":"docs/reference/dstack.yml/volume/#existing-volume","title":"Registering an existing volume","text":"<pre><code>type: volume\n# The name of the volume\nname: my-existing-volume\n\n# Volumes are bound to a specific backend and region\nbackend: aws\nregion: eu-central-1\n\n# The ID of the volume in AWS\nvolume_id: vol1235\n</code></pre>"},{"location":"docs/reference/dstack.yml/volume/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/volume/#name","title":"<code>name</code> - (Optional) The volume name.","text":""},{"location":"docs/reference/dstack.yml/volume/#backend","title":"<code>backend</code> -  The volume backend.","text":""},{"location":"docs/reference/dstack.yml/volume/#region","title":"<code>region</code> -  The volume region.","text":""},{"location":"docs/reference/dstack.yml/volume/#size","title":"<code>size</code> - (Optional) The volume size. Must be specified when creating new volumes.","text":""},{"location":"docs/reference/dstack.yml/volume/#volume_id","title":"<code>volume_id</code> - (Optional) The volume ID. Must be specified when registering external volumes.","text":""},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":"<p>The <code>~/.dstack/server/config.yml</code> file is used to configure the <code>dstack</code> server cloud accounts and other sever-level settings such as encryption.</p>"},{"location":"docs/reference/server/config.yml/#backends_1","title":"Backends","text":"<p>The <code>dstack</code> server allows you to configure backends for multiple projects. If you don't need multiple projects, use only the <code>main</code> project.</p> <p>Each cloud account must be configured under the <code>backends</code> property of the respective project. See the examples below.</p>"},{"location":"docs/reference/server/config.yml/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p> Default credentialsAccess key <p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n</code></pre> <p>Create an access key by following the this guide . Once you've downloaded the <code>.csv</code> file with your IAM user's Access key ID and Secret access key, proceed to configure the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: access_key\n          access_key: KKAAUKLIZ5EHKICAOASV\n          secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> Required permissions <p>The following AWS policy permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:AuthorizeSecurityGroupEgress\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceAttribute\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DetachVolume\",\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"servicequotas:ListServiceQuotas\",\n                \"servicequotas:GetServiceQuota\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticloadbalancing:CreateLoadBalancer\",\n                \"elasticloadbalancing:CreateTargetGroup\",\n                \"elasticloadbalancing:CreateListener\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:DeleteLoadBalancer\",\n                \"elasticloadbalancing:DeleteTargetGroup\",\n                \"elasticloadbalancing:DeleteListener\",\n                \"elasticloadbalancing:DeregisterTargets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"acm:DescribeCertificate\",\n                \"acm:ListCertificates\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>The <code>elasticloadbalancing:*</code> and <code>acm:*</code> permissions are only needed for provisioning gateways with ACM (AWS Certificate Manager) certificates.</p> VPC <p>By default, <code>dstack</code> uses the default VPC. It's possible to customize it:</p> vpc_namevpc_ids <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        vpc_name: my-vpc\n</code></pre> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        default_vpcs: true\n        vpc_ids:\n          us-east-1: vpc-0a2b3c4d5e6f7g8h\n          us-east-2: vpc-9i8h7g6f5e4d3c2b\n          us-west-1: vpc-4d3c2b1a0f9e8d7\n</code></pre> <p>For the regions without configured <code>vpc_ids</code>, enable default VPCs by setting <code>default_vpcs</code> to <code>true</code>.</p> Private subnets <p>By default, <code>dstack</code> utilizes public subnets and permits inbound SSH traffic exclusively for any provisioned instances. If you want <code>dstack</code> to use private subnets, set <code>public_ips</code> to <code>false</code>.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        public_ips: false\n</code></pre> <p>Using private subnets assumes that both the <code>dstack</code> server and users can access the configured VPC's private subnets  (e.g., through VPC peering).</p>"},{"location":"docs/reference/server/config.yml/#azure","title":"Azure","text":"<p>There are two ways to configure Azure: using a client secret or using the default credentials.</p> Default credentialsClient secret <p>If you have default credentials set up, configure the backend like this:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: azure\n        subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n        tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n        creds:\n          type: default\n</code></pre> <p>If you don't know your <code>subscription_id</code> and <code>tenant_id</code>, use Azure CLI :</p> <pre><code>az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>A client secret can be created using the Azure CLI :</p> <pre><code>SUBSCRIPTION_ID=...\naz ad sp create-for-rbac\n    --name dstack-app \\\n    --role $DSTACK_ROLE \\\n    --scopes /subscriptions/$SUBSCRIPTION_ID \\\n    --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <p>If you don't know your <code>subscription_id</code>, use Azure CLI :</p> <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre> Required permissions <p>The following Azure permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"properties\": {\n        \"roleName\": \"dstack-role\",\n        \"description\": \"Minimal required permissions for using Azure with dstack\",\n        \"assignableScopes\": [\n            \"/subscriptions/${YOUR_SUBSCRIPTION_ID}\"\n        ],\n        \"permissions\": [\n            {\n            \"actions\": [\n                \"Microsoft.Authorization/*/read\",\n                \"Microsoft.Compute/availabilitySets/*\",\n                \"Microsoft.Compute/locations/*\",\n                \"Microsoft.Compute/virtualMachines/*\",\n                \"Microsoft.Compute/virtualMachineScaleSets/*\",\n                \"Microsoft.Compute/cloudServices/*\",\n                \"Microsoft.Compute/disks/write\",\n                \"Microsoft.Compute/disks/read\",\n                \"Microsoft.Compute/disks/delete\",\n                \"Microsoft.Network/networkSecurityGroups/*\",\n                \"Microsoft.Network/locations/*\",\n                \"Microsoft.Network/virtualNetworks/*\",\n                \"Microsoft.Network/networkInterfaces/*\",\n                \"Microsoft.Network/publicIPAddresses/*\",\n                \"Microsoft.Resources/subscriptions/resourceGroups/read\",\n                \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n                \"Microsoft.Resources/subscriptions/read\"\n            ],\n            \"notActions\": [],\n            \"dataActions\": [],\n            \"notDataActions\": []\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"docs/reference/server/config.yml/#gcp","title":"GCP","text":"<p>There are two ways to configure GCP: using a service account or using the default credentials.</p> Default credentialsService account <p>Enable GCP application default credentials:</p> <pre><code>gcloud auth application-default login \n</code></pre> <p>Then configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n</code></pre> <p>To create a service account, follow this guide . After setting up the service account create a key  for it and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path.</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: service_account\n        filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <p>If you don't know your GCP project ID, use Google Cloud CLI :</p> <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre> Required permissions <p>The following GCP permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>compute.disks.create\ncompute.disks.delete\ncompute.disks.get\ncompute.disks.list\ncompute.disks.use\ncompute.firewalls.create\ncompute.images.useReadOnly\ncompute.instances.attachDisk\ncompute.instances.create\ncompute.instances.delete\ncompute.instances.detachDisk\ncompute.instances.get\ncompute.instances.setLabels\ncompute.instances.setMetadata\ncompute.instances.setTags\ncompute.networks.get\ncompute.networks.updatePolicy\ncompute.regions.get\ncompute.regions.list\ncompute.subnetworks.list\ncompute.subnetworks.use\ncompute.subnetworks.useExternalIp\ncompute.zoneOperations.get\n</code></pre> <p>If you plan to use TPUs, additional permissions are required:</p> <pre><code>tpu.nodes.create\ntpu.nodes.delete\ntpu.nodes.get\ntpu.operations.get\ntpu.operations.list\n</code></pre> <p>Also, the use of TPUs requires the <code>serviceAccountUser</code> role. For TPU VMs, dstack will use the default service account.</p> Required APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>.</p> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com\n</code></pre> VPC VPCShared VPC <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n\n      vpc_name: my-custom-vpc\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n\n      vpc_name: my-custom-vpc\n      vpc_project_id: another-project-id\n</code></pre> <p>To use a shared VPC, that VPC has to be configured with two additional firewall rules:</p> <ul> <li>Allow <code>INGRESS</code> traffic on port <code>22</code>, with the target tag <code>dstack-runner-instance</code></li> <li>Allow <code>INGRESS</code> traffic on ports <code>22</code>, <code>80</code>, <code>443</code>, with the target tag <code>dstack-gateway-instance</code></li> </ul> Private subnets <p>By default, <code>dstack</code> utilizes public subnets and permits inbound SSH traffic exclusively for any provisioned instances. If you want <code>dstack</code> to use private subnets, set <code>public_ips</code> to <code>false</code>.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: gcp\n        creds:\n          type: default\n\n        public_ips: false\n</code></pre> <p>Using private subnets assumes that both the <code>dstack</code> server and users can access the configured VPC's private subnets (e.g., through VPC peering). Additionally, Cloud NAT must be configured to provide access to external resources for provisioned instances.</p>"},{"location":"docs/reference/server/config.yml/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud  account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: lambda\n      creds:\n        type: api_key\n        api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/reference/server/config.yml/#runpod","title":"RunPod","text":"<p>Log into your RunPod  console, click Settings in the sidebar, expand the <code>API Keys</code> section, and click the button to create a Read &amp; Write key.</p> <p>Then proceed to configuring the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: runpod\n        creds:\n          type: api_key\n          api_key: US9XTPDIV8AR42MMINY8TCKRB8S4E7LNRQ6CAUQ9\n</code></pre>"},{"location":"docs/reference/server/config.yml/#ssh","title":"SSH","text":"<p>The <code>ssh</code> backend enables container orchestration across any on-prem servers. </p> <p>This backend requires no configuration. To use it, create SSH fleets.</p>"},{"location":"docs/reference/server/config.yml/#vastai","title":"Vast.ai","text":"<p>Log into your Vast.ai  account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: vastai\n      creds:\n        type: api_key\n        api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/reference/server/config.yml/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock  account, click Developers in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: tensordock\n        creds:\n          type: api_key\n          api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n          api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/reference/server/config.yml/#cudo","title":"CUDO","text":"<p>Log into your CUDO Compute  account, click API keys in the sidebar, and click the <code>Create an API key</code> button.</p> <p>Ensure you've created a project with CUDO Compute, then proceed to configuring the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: cudo\n        project_id: my-cudo-project\n        creds:\n          type: api_key\n          api_key: 7487240a466624b48de22865589\n</code></pre>"},{"location":"docs/reference/server/config.yml/#oci","title":"OCI","text":"<p>There are two ways to configure OCI: using client credentials or using the default credentials.</p> Default credentialsClient credentials <p>If you have default credentials set up in <code>~/.oci/config</code>, configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: default\n</code></pre> <p>Log into the OCI Console , go to <code>My profile</code>,  select <code>API keys</code>, and click <code>Add API key</code>.</p> <p>Once you add a key, you'll see the configuration file. Copy its values to configure the backend as follows:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: client\n      user: ocid1.user.oc1..g5vlaeqfu47akmaafq665xsgmyaqjktyfxtacfxc4ftjxuca7aohnd2ev66m\n      tenancy: ocid1.tenancy.oc1..ajqsftvk4qarcfaak3ha4ycdsaahxmaita5frdwg3tqo2bcokpd3n7oizwai\n      region: eu-frankfurt-1\n      fingerprint: 77:32:77:00:49:7c:cb:56:84:75:8e:77:96:7d:53:17\n      key_file: ~/.oci/private_key.pem\n</code></pre> <p>Make sure to include either the path to your private key via <code>key_file</code> or the contents of the key via <code>key_content</code>.</p> Required permissions <p>This is an example of a restrictive policy for a group of <code>dstack</code> users:</p> <pre><code>Allow group &lt;dstack-users&gt; to read compartments in tenancy where target.compartment.name = '&lt;dstack-compartment&gt;'\nAllow group &lt;dstack-users&gt; to read marketplace-community-listings in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage app-catalog-listing in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage instances in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage compute-capacity-reports in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage volumes in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage volume-attachments in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage virtual-network-family in compartment &lt;dstack-compartment&gt;\n</code></pre> <p>To use this policy, create a compartment for <code>dstack</code> and specify it in <code>~/.dstack/server/config.yml</code>.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: default\n    compartment_id: ocid1.compartment.oc1..aaaaaaaa\n</code></pre>"},{"location":"docs/reference/server/config.yml/#datacrunch","title":"DataCrunch","text":"<p>Log into your DataCrunch  account, click Keys in the sidebar, find <code>REST API Credentials</code> area and then click the <code>Generate Credentials</code> button.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: datacrunch\n        creds:\n          type: api_key\n          client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n          client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/reference/server/config.yml/#kubernetes","title":"Kubernetes","text":"<p><code>dstack</code> supports both self-managed, and managed Kubernetes clusters.</p> <p>To configure a Kubernetes backend, specify the path to the kubeconfig file, and the port that <code>dstack</code> can use for proxying SSH traffic. In case of a self-managed cluster, also specify the IP address of any node in the cluster.</p> Self-managedManaged <p>Here's how to configure the backend to use a self-managed cluster.</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: kubernetes\n      kubeconfig:\n        filename: ~/.kube/config\n      networking:\n        ssh_host: localhost # The external IP address of any node\n        ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> Kind <p>If you are using Kind, make sure to make  to set up <code>ssh_port</code> via <code>extraPortMappings</code> for proxying SSH traffic:</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraPortMappings:\n      - containerPort: 32000 # Must be same as `ssh_port`\n        hostPort: 32000 # Must be same as `ssh_port`\n</code></pre> <p>Go ahead and create the cluster like this: </p> <pre><code>kind create cluster --config examples/misc/kubernetes/kind-config.yml\n</code></pre> <p>Here's how to configure the backend to use a managed cluster (AWS, GCP, Azure).</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: kubernetes\n        kubeconfig:\n          filename: ~/.kube/config\n        networking:\n          ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> EKS <p>For example, if you are using EKS, make sure to add it via an ingress rule of the corresponding security group:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;cluster-security-group-id&gt; --protocol tcp --port 32000 --cidr 0.0.0.0/0\n</code></pre> NVIDIA GPU Operator <p>To use GPUs with Kubernetes, the cluster must be installed with the NVIDIA GPU Operator .</p>"},{"location":"docs/reference/server/config.yml/#encryption","title":"Encryption","text":"<p>By default, <code>dstack</code> stores data in plaintext. To enforce encryption, you  specify one or more encryption keys.</p> <p><code>dstack</code> currently supports AES and identity (plaintext) encryption keys. Support for external providers like HashiCorp Vault and AWS KMS is planned.</p> AESIdentity <p>The <code>aes</code> encryption key encrypts data using AES-256 in GCM mode. To configure the <code>aes</code> encryption, generate a random 32-byte key:</p> <pre><code>$ head -c 32 /dev/urandom | base64\n\nopmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>And specify it as <code>secret</code>:</p> <pre><code>encryption:\n  keys:\n    - type: aes\n      name: key1\n      secret: opmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>The <code>identity</code> encryption performs no encryption and stores data in plaintext. You can specify an <code>identity</code> encryption key explicitly if you want to decrypt the data:</p> <pre><code>encryption:\n  keys:\n  - type: identity\n  - type: aes\n    name: key1\n    secret: opmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>With this configuration, the <code>aes</code> key will still be used to decrypt the old data, but new writes will store the data in plaintext.</p> Key rotation <p>If multiple keys are specified, the first is used for encryption, and all are tried for decryption. This enables key rotation by specifying a new encryption key.</p> <pre><code>encryption:\n  keys:\n  - type: aes\n    name: key2\n    secret: cR2r1JmkPyL6edBQeHKz6ZBjCfS2oWk87Gc2G3wHVoA=\n\n  - type: aes\n    name: key1\n    secret: E5yzN6V3XvBq/f085ISWFCdgnOGED0kuFaAkASlmmO4=\n</code></pre> <p>Old keys may be deleted once all existing records have been updated to re-encrypt sensitive data.  Encrypted values are prefixed with key names, allowing DB admins to identify the keys used for encryption.</p> <p>See the reference table for all configurable permissions.</p>"},{"location":"docs/reference/server/config.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/server/config.yml/#_projects","title":"<code>projects</code> -  The list of projects.","text":""},{"location":"docs/reference/server/config.yml/#_encryption","title":"<code>encryption</code> - (Optional) The encryption config.","text":""},{"location":"docs/reference/server/config.yml/#_default_permissions","title":"<code>default_permissions</code> - (Optional) The default user permissions.","text":""},{"location":"docs/reference/server/config.yml/#_projects","title":"<code>projects[n]</code>","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The name of the project.","text":""},{"location":"docs/reference/server/config.yml/#backends","title":"<code>backends</code> -  The list of backends.","text":""},{"location":"docs/reference/server/config.yml/#_aws","title":"<code>projects[n].backends[type=aws]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>aws</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of AWS regions.","text":""},{"location":"docs/reference/server/config.yml/#vpc_name","title":"<code>vpc_name</code> - (Optional) The VPC name. All configured regions must have a VPC with this name.","text":""},{"location":"docs/reference/server/config.yml/#vpc_ids","title":"<code>vpc_ids</code> - (Optional) The mapping from AWS regions to VPC IDs. If <code>default_vpcs: true</code>, omitted regions will use default VPCs.","text":""},{"location":"docs/reference/server/config.yml/#default_vpcs","title":"<code>default_vpcs</code> - (Optional) A flag to enable/disable using default VPCs in regions not configured by <code>vpc_ids</code>. Set to <code>false</code> if default VPCs should never be used. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#public_ips","title":"<code>public_ips</code> - (Optional) A flag to enable/disable public IP assigning on instances. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_aws-creds","title":"<code>projects[n].backends[type=aws].creds</code>","text":"Access keyDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>access_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#access_key","title":"<code>access_key</code> -  The access key.","text":""},{"location":"docs/reference/server/config.yml/#secret_key","title":"<code>secret_key</code> -  The secret key.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#_azure","title":"<code>projects[n].backends[type=azure]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>azure</code>.","text":""},{"location":"docs/reference/server/config.yml/#tenant_id","title":"<code>tenant_id</code> -  The tenant ID.","text":""},{"location":"docs/reference/server/config.yml/#subscription_id","title":"<code>subscription_id</code> -  The subscription ID.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_azure-creds","title":"<code>projects[n].backends[type=azure].creds</code>","text":"ClientDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>client</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#_datacrunch","title":"<code>projects[n].backends[type=datacrunch]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>datacrunch</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_datacrunch-creds","title":"<code>projects[n].backends[type=datacrunch].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#_gcp","title":"<code>projects[n].backends[type=gcp]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>gcp</code>.","text":""},{"location":"docs/reference/server/config.yml/#project_id","title":"<code>project_id</code> -  The project ID.","text":""},{"location":"docs/reference/server/config.yml/#vpc_name","title":"<code>vpc_name</code> - (Optional) The VPC name.","text":""},{"location":"docs/reference/server/config.yml/#vpc_project_id","title":"<code>vpc_project_id</code> - (Optional) The shared VPC hosted project ID. Required for shared VPC only.","text":""},{"location":"docs/reference/server/config.yml/#public_ips","title":"<code>public_ips</code> - (Optional) A flag to enable/disable public IP assigning on instances. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_gcp-creds","title":"<code>projects[n].backends[type=gcp].creds</code>","text":"Service accountDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>service_account</code>.","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"<code>filename</code> -  The path to the service account file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the service account file.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#_lambda","title":"<code>projects[n].backends[type=lambda]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>lambda</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_lambda-creds","title":"<code>projects[n].backends[type=lambda].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#_oci","title":"<code>projects[n].backends[type=oci]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>oci</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) List of region names for running <code>dstack</code> jobs. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#compartment_id","title":"<code>compartment_id</code> - (Optional) Compartment where <code>dstack</code> will create all resources. Omit to instruct <code>dstack</code> to create a new compartment.","text":""},{"location":"docs/reference/server/config.yml/#_oci-creds","title":"<code>projects[n].backends[type=oci].creds</code>","text":"ClientDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>client</code>.","text":""},{"location":"docs/reference/server/config.yml/#user","title":"<code>user</code> -  User OCID.","text":""},{"location":"docs/reference/server/config.yml/#tenancy","title":"<code>tenancy</code> -  Tenancy OCID.","text":""},{"location":"docs/reference/server/config.yml/#key_file","title":"<code>key_file</code> - (Optional) Path to the user's private PEM key. Either this or <code>key_content</code> should be set.","text":""},{"location":"docs/reference/server/config.yml/#key_content","title":"<code>key_content</code> - (Optional) Content of the user's private PEM key. Either this or <code>key_file</code> should be set.","text":""},{"location":"docs/reference/server/config.yml/#pass_phrase","title":"<code>pass_phrase</code> - (Optional) Passphrase for the private PEM key if it is encrypted.","text":""},{"location":"docs/reference/server/config.yml/#fingerprint","title":"<code>fingerprint</code> -  User's public key fingerprint.","text":""},{"location":"docs/reference/server/config.yml/#region","title":"<code>region</code> -  Name or key of any region the tenancy is subscribed to.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#file","title":"<code>file</code> - (Optional) Path to the OCI CLI-compatible config file. Defaults to <code>~/.oci/config</code>.","text":""},{"location":"docs/reference/server/config.yml/#profile","title":"<code>profile</code> - (Optional) Profile to load from the config file. Defaults to <code>DEFAULT</code>.","text":""},{"location":"docs/reference/server/config.yml/#_tensordock","title":"<code>projects[n].backends[type=tensordock]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>tensordock</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_tensordock-creds","title":"<code>projects[n].backends[type=tensordock].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#api_token","title":"<code>api_token</code> -  The API token.","text":""},{"location":"docs/reference/server/config.yml/#_vastai","title":"<code>projects[n].backends[type=vastai]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>vastai</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_vastai-creds","title":"<code>projects[n].backends[type=vastai].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#_cudo","title":"<code>projects[n].backends[type=cudo]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>cudo</code>.","text":""},{"location":"docs/reference/server/config.yml/#project_id","title":"<code>project_id</code> -  The project ID.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#_cudo-creds","title":"<code>projects[n].backends[type=cudo].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#_kubernetes","title":"<code>projects[n].backends[type=kubernetes]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>kubernetes</code>.","text":""},{"location":"docs/reference/server/config.yml/#_kubeconfig","title":"<code>kubeconfig</code> -  The kubeconfig configuration.","text":""},{"location":"docs/reference/server/config.yml/#_networking","title":"<code>networking</code> - (Optional) The networking configuration.","text":""},{"location":"docs/reference/server/config.yml/#_kubeconfig","title":"<code>projects[n].backends[type=kubernetes].kubeconfig</code>","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"<code>filename</code> -  The path to the kubeconfig file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the kubeconfig file.","text":""},{"location":"docs/reference/server/config.yml/#_networking","title":"<code>projects[n].backends[type=kubernetes].networking</code>","text":""},{"location":"docs/reference/server/config.yml/#ssh_host","title":"<code>ssh_host</code> - (Optional) The external IP address of any node.","text":""},{"location":"docs/reference/server/config.yml/#ssh_port","title":"<code>ssh_port</code> - (Optional) Any port accessible outside of the cluster.","text":""},{"location":"docs/reference/server/config.yml/#_encryption","title":"<code>encryption</code>","text":""},{"location":"docs/reference/server/config.yml/#keys","title":"<code>keys</code> -  The encryption keys.","text":""},{"location":"docs/reference/server/config.yml/#_encryption-keys-identity","title":"<code>encryption.keys[n][type=identity]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the key. Must be <code>identity</code>.","text":""},{"location":"docs/reference/server/config.yml/#_encryption-keys-aes","title":"<code>encryption.keys[n][type=aes]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the key. Must be <code>aes</code>.","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The key name for key identification.","text":""},{"location":"docs/reference/server/config.yml/#secret","title":"<code>secret</code> -  Base64-encoded AES-256 key.","text":""},{"location":"docs/reference/server/config.yml/#_default-permissions","title":"<code>default_permissions</code>","text":""},{"location":"docs/reference/server/config.yml/#allow_non_admins_create_projects","title":"<code>allow_non_admins_create_projects</code> - (Optional) This flag controls whether regular users (non-global admins) can create and manage their own projects. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/server/config.yml/#allow_non_admins_manage_ssh_fleets","title":"<code>allow_non_admins_manage_ssh_fleets</code> - (Optional) This flag controls whether regular project members (i.e. Users) can add and delete SSH fleets. Defaults to <code>True</code>.","text":""},{"location":"examples/accelerators/amd/","title":"AMD","text":"<p>Since 0.18.11 , you can specify an AMD GPU under <code>resources</code>. Below are a few examples.</p> <p>AMD accelerators are currently supported only with the <code>runpod</code> backend.</p>"},{"location":"examples/accelerators/amd/#deployment","title":"Deployment","text":""},{"location":"examples/accelerators/amd/#running-as-a-service","title":"Running as a service","text":"TGI <p>Here's an example of a service that deploys Llama 3.1 70B in FP16 using TGI .</p> <p> <pre><code>type: service\nname: amd-service-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct\n  - TRUST_REMOTE_CODE=true\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n\nspot_policy: auto\n\nmodel:\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-70B-Instruct\n  format: openai\n</code></pre> <p>Docker image</p> <p>Please note that if you want to use AMD, specifying <code>image</code> is currently required. This must be an image that includes ROCm drivers.</p> <p>To request multiple GPUs, specify the quantity after the GPU name, separated by a colon, e.g., <code>MI300X:4</code>.</p> <p>AMD accelerators can also be used with other frameworks like vLLM, Ollama, etc., and we'll be adding more examples soon.</p>"},{"location":"examples/accelerators/amd/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p>"},{"location":"examples/accelerators/amd/#fleets","title":"Fleets","text":"<p>By default, <code>dstack apply</code> reuses <code>idle</code> instances from one of the existing fleets. If no <code>idle</code> instances meet the requirements, it creates a new fleet using one of the configured backends.</p> <p>Use fleets configurations to create fleets manually. This reduces startup time for dev environments, tasks, and services, and is very convenient if you want to reuse fleets across runs.</p>"},{"location":"examples/accelerators/amd/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p>"},{"location":"examples/accelerators/amd/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/tgi/amd</code> .</p>"},{"location":"examples/accelerators/amd/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks, and    services.</li> </ol>"},{"location":"examples/accelerators/tpu/","title":"TPU","text":"<p>If you're using the <code>gcp</code> backend, you can use TPUs. Just specify the TPU version and the number of cores  (separated by a dash), in the <code>gpu</code> property under <code>resources</code>. </p> <p>Currently, maximum 8 TPU cores can be specified, so the maximum supported values are <code>v2-8</code>, <code>v3-8</code>, <code>v4-8</code>, <code>v5litepod-8</code>,  and <code>v5e-8</code>. Multi-host TPU support, allowing for larger numbers of cores, is coming soon.</p> <p>Below are a few examples on using TPUs for deployment and fine-tuning.</p>"},{"location":"examples/accelerators/tpu/#deployment","title":"Deployment","text":""},{"location":"examples/accelerators/tpu/#running-as-a-service","title":"Running as a service","text":"<p>You can use any serving framework, such as vLLM, TGI. Here's an example of a service that deploys Llama 3.1 8B using  Optimum TPU  and vLLM .</p> Optimum TPUvLLM <p> <p><pre><code>type: service\nname: llama31-service-optimum-tpu\n\nimage: dstackai/optimum-tpu:llama31\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_TOTAL_TOKENS=4096\n  - MAX_BATCH_PREFILL_TOKENS=4095\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n\nspot_policy: auto\nresources:\n  gpu: v5litepod-4 \n\nmodel:\n  format: tgi\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> </p> <p>Note, for <code>Optimum TPU</code> by default <code>MAX_INPUT_TOKEN</code> is set to 4095, consequently we must set <code>MAX_BATCH_PREFILL_TOKENS</code> to 4095.</p> Docker image <p>The official Docker image <code>huggingface/optimum-tpu:latest</code> doesn\u2019t support Llama 3.1-8B.  We\u2019ve created a custom image with the fix: <code>dstackai/optimum-tpu:llama31</code>.  Once the pull request  is merged,  the official Docker image can be used.</p> <p> <p><pre><code>type: service\nname: llama31-service-vllm-tpu\n\nenv:\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - HUGGING_FACE_HUB_TOKEN\n  - DATE=20240828\n  - TORCH_VERSION=2.5.0\n  - VLLM_TARGET_DEVICE=tpu\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n  - git clone https://github.com/vllm-project/vllm.git\n  - cd vllm\n  - pip install -r requirements-tpu.txt\n  - apt-get install -y libopenblas-base libopenmpi-dev libomp-dev\n  - python setup.py develop\n  - vllm serve $MODEL_ID \n      --tensor-parallel-size 4 \n      --max-model-len $MAX_MODEL_LEN\n      --port 8000\nport:\n  - 8000\n\nspot_policy: auto\nresources:\n  gpu: v5litepod-4\n\nmodel:\n  format: openai\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> </p> <p>Note, when using Llama 3.1 8B with a <code>v5litepod</code> which has 16GB memory per core, we must limit the context size to 4096 tokens to fit the memory.</p>"},{"location":"examples/accelerators/tpu/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for serving LLMs with their corresponding TPUs. </p> Model size bfloat16 TPU int8 TPU 8B 16GB v5litepod-4 8GB v5litepod-4 70B 140GB v5litepod-16 70GB v5litepod-16 405B 810GB v5litepod-64 405GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for serving transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"examples/accelerators/tpu/#supported-frameworks","title":"Supported frameworks","text":"Framework Quantization Note TGI bfloat16 To deploy with TGI, Optimum TPU must be used. vLLM int8, bfloat16 int8 quantization still requires the same memory because the weights are first moved to the TPU in bfloat16, and then converted to int8. See the pull request  for more details."},{"location":"examples/accelerators/tpu/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p>"},{"location":"examples/accelerators/tpu/#fine-tuning-with-optimum-tpu","title":"Fine-tuning with Optimum TPU","text":"<p>Below is an example of fine-tuning Llama 3.1 8B using Optimum TPU   and the Abirate/english_quotes  dataset.</p> <pre><code>type: task\nname: optimum-tpu-llama-train\n\npython: \"3.11\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\ncommands:\n  - git clone -b add_llama_31_support https://github.com/dstackai/optimum-tpu.git\n  - mkdir -p optimum-tpu/examples/custom/\n  - cp examples/fine-tuning/optimum-tpu/llama31/train.py optimum-tpu/examples/custom/train.py\n  - cp examples/fine-tuning/optimum-tpu/llama31/config.yaml optimum-tpu/examples/custom/config.yaml\n  - cd optimum-tpu\n  - pip install -e . -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install datasets evaluate\n  - pip install accelerate -U\n  - pip install peft\n  - python examples/custom/train.py examples/custom/config.yaml\n\n\nresources:\n  gpu: v5litepod-8\n</code></pre>"},{"location":"examples/accelerators/tpu/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning LLMs with their corresponding TPUs.</p> Model size LoRA TPU 8B 16GB v5litepod-8 70B 160GB v5litepod-16 405B 950GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for fine-tuning transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"examples/accelerators/tpu/#supported-frameworks_1","title":"Supported frameworks","text":"Framework Quantization Note TRL bfloat16 To fine-tune using TRL, Optimum TPU is recommended. TRL doesn't support Llama 3.1 out of the box. Pytorch XLA bfloat16"},{"location":"examples/accelerators/tpu/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p>"},{"location":"examples/accelerators/tpu/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/optimum-tpu</code>  and <code>examples/fine-tuning/optimum-tpu</code> .</p>"},{"location":"examples/accelerators/tpu/#whats-next","title":"What's next?","text":"<ol> <li>Browse Optimum TPU ,    Optimum TPU TGI  and    vLLM .</li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol>"},{"location":"examples/fine-tuning/alignment-handbook/","title":"Alignment Handbook","text":"<p>This example shows how use Alignment Handbook  with <code>dstack</code> to  fine-tune Gemma 7B on your SFT dataset using one node or multiple nodes. </p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/fine-tuning/alignment-handbook/#training-configuration-recipe","title":"Training configuration recipe","text":"<p>Alignment Handbook's training script reads the model, LoRA, and dataset arguments, as well as trainer configuration from a YAML file. This file can be found at <code>examples/fine-tuning/alignment-handbook/config.yaml</code>. You can modify it as needed.</p> <p>Before you proceed with training, make sure to update the <code>hub_model_id</code> in <code>examples/fine-tuning/alignment-handbook/config.yaml</code>  with your HuggingFace username.</p>"},{"location":"examples/fine-tuning/alignment-handbook/#single-node-training","title":"Single-node training","text":"<p>The easiest way to run a training script with <code>dstack</code> is by creating a task configuration file. This file can be found at <code>examples/fine-tuning/alignment-handbook/train.dstack.yml</code> .  Below is its content: </p> <pre><code>type: task\nname: ah-train\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\n# Required environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - ACCELERATE_LOG_LEVEL=info\n  - WANDB_API_KEY\n# Commands of the task\ncommands:\n  - git clone https://github.com/huggingface/alignment-handbook.git\n  - cd alignment-handbook\n  - pip install .\n  - pip install flash-attn --no-build-isolation\n  - pip install wandb\n  - accelerate launch\n    --config_file recipes/accelerate_configs/multi_gpu.yaml\n    --num_processes=$DSTACK_GPUS_NUM\n    scripts/run_sft.py\n    ../examples/fine-tuning/alignment-handbook/config.yaml\n# Uncomment to access TensorBoard\n#ports:\n#  - 6006\n\nresources:\n  # Required resources\n  gpu: 24GB\n</code></pre> <p>The task clones Alignment Handbook's repo, installs the dependencies, and runs the script.</p> <p>The <code>DSTACK_GPUS_NUM</code> environment variable is automatically passed to the container according to the <code>resoruce</code> property.</p> <p>To run the task, use <code>dstack apply</code>:</p> <pre><code>$ HUGGING_FACE_HUB_TOKEN=...\n$ WANDB_API_KEY=...\n\n$ dstack apply -f examples/fine-tuning/alignment-handbook/train.dstack.yml\n</code></pre> <p>If you list <code>tensorbord</code> via <code>report_to</code> in <code>examples/fine-tuning/alignment-handbook/config.yaml</code>, you'll be able to access experiment metrics via <code>http://localhost:6006</code> (while the task is running).</p>"},{"location":"examples/fine-tuning/alignment-handbook/#multi-node-training","title":"Multi-node training","text":"<p>The multi-node training task configuration file can be found at <code>examples/fine-tuning/alignment-handbook/train-distrib.dstack.yml</code>. Below is its content:</p> <pre><code>type: task\nname: ah-train-distrib\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\n# The size of cluster\nnodes: 2\n\n# Required environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - ACCELERATE_LOG_LEVEL=info\n  - WANDB_API_KEY\n# Commands of the task (dstack runs it on each node)\ncommands:\n  - git clone https://github.com/huggingface/alignment-handbook.git\n  - cd alignment-handbook\n  - pip install .\n  - pip install flash-attn --no-build-isolation\n  - pip install wandb\n  - accelerate launch\n    --config_file ../examples/fine-tuning/alignment-handbook/fsdp_qlora_full_shard.yaml\n    --main_process_ip=$DSTACK_MASTER_NODE_IP\n    --main_process_port=8008\n    --machine_rank=$DSTACK_NODE_RANK\n    --num_processes=$DSTACK_GPUS_NUM\n    --num_machines=$DSTACK_NODES_NUM\n    scripts/run_sft.py \n    ../examples/fine-tuning/alignment-handbook/config.yaml\n# Expose 6006 to access TensorBoard\nports:\n  - 6006\n\nresources:\n  # Required resources\n  gpu: 24GB\n  # Shared memory size for inter-process communication\n  shm_size: 24GB\n</code></pre> <p>Here's how the multi-node task is different from the single-node one:</p> <ol> <li>The <code>nodes</code> property is specified with a number of required nodes (should match the fleet's nodes number).</li> <li>Under <code>resoruces</code>, <code>shm_size</code> is specified with the shared memory size used for the communication of parallel    processes within a node (in case multiple GPUs per node are used).</li> <li>Instead of Alignment Handbook's <code>recipes/accelerate_configs/multi_gpu.yaml</code>, we use <code>examples/fine-tuning/alignment-handbook/fsdp_qlora_full_shard.yaml</code> as an accelerate config.</li> <li>We use <code>DSTACK_MASTER_NODE_IP</code>, <code>DSTACK_NODE_RANK</code>, <code>DSTACK_GPUS_NUM</code>, and <code>DSTACK_NODES_NUM</code> environment variables to    configure <code>accelerate</code>. The environment variables are automatically passed    to the container for each node based on the task configuration.</li> </ol>"},{"location":"examples/fine-tuning/alignment-handbook/#fleets","title":"Fleets","text":"<p>By default, <code>dstack run</code> reuses <code>idle</code> instances from one of the existing fleets.  If no <code>idle</code> instances meet the requirements, it creates a new fleet using one of the configured backends.</p> <p>The example folder includes two cloud fleet configurations: <code>examples/fine-tuning/alignment-handbook/fleet.dstack.yml</code> (a single node with a <code>24GB</code> GPU), and a <code>examples/fine-tuning/alignment-handbook/fleet-distrib.dstack.yml</code> (a cluster of two nodes each with a <code>24GB</code> GPU).</p> <p>You can update the fleet configurations to change the vRAM size, GPU model, number of GPUs per node, or number of nodes. </p> <p>A fleet can be provisioned with <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/fine-tuning/alignment-handbook/fleet.dstack.yml\n</code></pre> <p>Once provisioned, the fleet can run dev environments and fine-tuning tasks. To delete the fleet, use <code>dstack fleet delete</code>.</p> <p>To ensure <code>dstack apply</code> always reuses an existing fleet, pass <code>--reuse</code> to <code>dstack apply</code> (or set <code>creation_policy</code> to <code>reuse</code> in the task configuration). The default policy is <code>reuse_or_create</code>.</p>"},{"location":"examples/fine-tuning/alignment-handbook/#dev-environment","title":"Dev environment","text":"<p>If you'd like to play with the example using a dev environment, run <code>.dstack.yml</code>  via <code>dstack apply</code>:</p> <pre><code>dstack apply -f examples/fine-tuning/alignment-handbook/.dstack.yaml \n</code></pre>"},{"location":"examples/fine-tuning/alignment-handbook/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in <code>examples/fine-tuning/alignment-handbook</code> .</p>"},{"location":"examples/fine-tuning/alignment-handbook/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,    services, and fleets.</li> <li>Browse Alignment Handbook .</li> <li>See other examples.</li> </ol>"},{"location":"examples/fine-tuning/axolotl/","title":"Axolotl","text":"<p>This example shows how use Axolotl   with <code>dstack</code> to fine-tune Llama3 8B using FSDP and QLoRA.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/fine-tuning/axolotl/#training-configuration-recipe","title":"Training configuration recipe","text":"<p>Axolotl reads the model, LoRA, and dataset arguments, as well as trainer configuration from a YAML file. This file can be found at <code>examples/fine-tuning/axolotl/config.yaml</code> . You can modify it as needed.</p> <p>Before you proceed with training, make sure to update the <code>hub_model_id</code> in <code>examples/fine-tuning/axolotl/config.yaml</code>  with your HuggingFace username.</p>"},{"location":"examples/fine-tuning/axolotl/#single-node-training","title":"Single-node training","text":"<p>The easiest way to run a training script with <code>dstack</code> is by creating a task configuration file. This file can be found at <code>examples/fine-tuning/axolotl/train.dstack.yml</code> . Below is its content: </p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: axolotl-train\n\n# Using the official Axolotl's Docker image\nimage: winglian/axolotl-cloud:main-20240429-py3.11-cu121-2.2.1\n\n# Required environment variables\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\n# Commands of the task\ncommands:\n  - accelerate launch -m axolotl.cli.train examples/fine-tuning/axolotl/config.yaml\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # Two or more GPU\n    count: 2..\n</code></pre> <p>The task uses Axolotl's Docker image, where Axolotl is already pre-installed.</p> <p>To run the task, use <code>dstack apply</code>:</p> <pre><code>$ HUGGING_FACE_HUB_TOKEN=...\n$ WANDB_API_KEY=...\n\n$ dstack apply -f examples/fine-tuning/axolotl/train.dstack.yml\n</code></pre>"},{"location":"examples/fine-tuning/axolotl/#fleets","title":"Fleets","text":"<p>By default, <code>dstack run</code> reuses <code>idle</code> instances from one of the existing fleets. If no <code>idle</code> instances meet the requirements, it creates a new fleet using one of the configured backends.</p> <p>The example folder includes a fleet configuration:   <code>examples/fine-tuning/axolotl/fleet.dstack.yml</code>  {:target=\"_blank\"} (a single node with a <code>24GB</code> GPU).</p> <p>You can update the fleet configuration to change the vRAM size, GPU model, number of GPUs per node, or number of nodes. </p> <p>A fleet can be provisioned with <code>dstack apply</code>:</p> <pre><code>dstack apply -f examples/fine-tuning/axolotl/fleet.dstack.yml\n</code></pre> <p>Once provisioned, the fleet can run dev environments and fine-tuning tasks. To delete the fleet, use <code>dstack fleet delete</code>.</p> <p>To ensure <code>dstack apply</code> always reuses an existing fleet, pass <code>--reuse</code> to <code>dstack apply</code> (or set <code>creation_policy</code> to <code>reuse</code> in the task configuration). The default policy is <code>reuse_or_create</code>.</p>"},{"location":"examples/fine-tuning/axolotl/#dev-environment","title":"Dev environment","text":"<p>If you'd like to play with the example using a dev environment, run <code>.dstack.yml</code>  via <code>dstack apply</code>:</p> <pre><code>dstack apply -f examples/fine-tuning/axolotl/.dstack.yaml \n</code></pre>"},{"location":"examples/fine-tuning/axolotl/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in <code>examples/fine-tuning/axolotl</code> .</p>"},{"location":"examples/fine-tuning/axolotl/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and fleets.</li> <li>Browse Axolotl .</li> </ol>"},{"location":"examples/fine-tuning/trl/","title":"TRL","text":"<p>This example walks you through how to fine-tune Llama 3.1 with <code>dstack</code>, whether in the cloud or on-prem.</p>"},{"location":"examples/fine-tuning/trl/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning Llama 3.1.</p> Model size Full fine-tuning LoRA QLoRA 8B 60GB 16GB 6GB 70B 500GB 160GB 48GB 405B 3.25TB 950GB 250GB <p>The requirements can be significantly reduced with certain optimizations.</p>"},{"location":"examples/fine-tuning/trl/#running-on-multiple-gpus","title":"Running on multiple GPUs","text":"<p>Below is an example for fine-tuning Llama 3.1 8B using the <code>OpenAssistant/oasst_top1_2023-08-25</code>  dataset:</p> <pre><code>type: task\nname: trl-train\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/multi_gpu.yaml\n    --num_processes $DSTACK_GPUS_PER_NODE \n    examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n\nresources:\ngpu:\n  # 24GB or more vRAM\n  memory: 24GB..\n  # One or more GPU\n  count: 1..\n# Shared memory (for multi-gpu)\nshm_size: 24GB\n</code></pre> <p>Change the <code>resources</code> property to specify more GPUs. </p>"},{"location":"examples/fine-tuning/trl/#deepspeed","title":"DeepSpeed","text":"<p>For more memory-efficient use of multiple GPUs, consider using DeepSpeed and ZeRO Stage 3.</p> <p>To do this, use the <code>examples/accelerate_configs/deepspeed_zero3.yaml</code> configuration file instead of  <code>examples/accelerate_configs/multi_gpu.yaml</code>.</p>"},{"location":"examples/fine-tuning/trl/#running-on-multiple-nodes","title":"Running on multiple nodes","text":"<p>In case the model doesn't feet into a single GPU, consider running a <code>dstack</code> task on multiple nodes.</p> <pre><code>type: task\nname: trl-train-distrib\n\n# Size of the cluster\nnodes: 2\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/fsdp_qlora.yaml \n    --main_process_ip=$DSTACK_MASTER_NODE_IP\n    --main_process_port=8008\n    --machine_rank=$DSTACK_NODE_RANK\n    --num_processes=$DSTACK_GPUS_NUM\n    --num_machines=$DSTACK_NODES_NUM\n      examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n    --torch_dtype bfloat16\n    --use_bnb_nested_quant\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n  # Shared memory (for multi-gpu)\n  shm_size: 24GB\n</code></pre>"},{"location":"examples/fine-tuning/trl/#fleets","title":"Fleets","text":"<p>By default, <code>dstack run</code> reuses <code>idle</code> instances from one of the existing fleets. If no <code>idle</code> instances meet the requirements, it creates a new fleet using one of the configured backends.</p> <p>Use fleets configurations to create fleets manually. This reduces startup time for dev environments, tasks, and services, and is very convenient if you want to reuse fleets across runs.</p>"},{"location":"examples/fine-tuning/trl/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p>"},{"location":"examples/fine-tuning/trl/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/llms/llama31</code>  and <code>examples/fine-tuning/trl</code> .</p>"},{"location":"examples/fine-tuning/trl/#whats-next","title":"What's next?","text":"<ol> <li>Browse the Axolotl     and Alignment Handbook examples</li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol>"},{"location":"examples/llms/llama31/","title":"Llama 3.1","text":"<p>This example walks you through how to deploy and fine-tuning Llama 3.1 with <code>dstack</code>.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/llms/llama31/#deployment","title":"Deployment","text":""},{"location":"examples/llms/llama31/#running-as-a-task","title":"Running as a task","text":"<p>If you'd like to run Llama 3.1 for development purposes, consider using <code>dstack</code> tasks.  You can use any serving framework, such as vLLM, TGI, or Ollama. Below is the configuration file for the task.</p> vLLMTGIOllama <p> <pre><code>type: task\nname: llama31-task-vllm\n\npython: \"3.10\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_MODE_LEN=4096\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL_ID\n    --tensor-parallel-size $DSTACK_GPUS_NUM\n    --max-model-len $MAX_MODEL_LEN\nports: [8000]\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  # Required resources\n  gpu: 24GB\n  # Shared memory (required by multi-gpu)\n  shm_size: 24GB\n</code></pre> <p> <pre><code>type: task\nname: llama31-task-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - NUM_SHARD=$DSTACK_GPUS_NUM text-generation-launcher\nports: [80]\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\nresources:\n  # Required resources\n  gpu: 24GB\n  # Shared memory (required by multi-gpu)\n  shm_size: 24GB\n</code></pre> <p> <pre><code>type: task\nname: llama31-task-ollama    \n\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull llama3.1\n  - fg\nport: 11434\n\nresources:\n  gpu: 24GB\n\n# Use either spot or on-demand instances\nspot_policy: auto\n\n# Required resources\nresources:\n  gpu: 24GB\n</code></pre> <p>Note, when using Llama 3.1 8B with a 24GB GPU, we must limit the context size to 4096 tokens to fit the memory.</p>"},{"location":"examples/llms/llama31/#deploying-as-a-service","title":"Deploying as a service","text":"<p>If you'd like to deploy Llama 3.1 as public auto-scalable and secure endpoint, consider using <code>dstack</code> services.</p>"},{"location":"examples/llms/llama31/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for loading the model.  This excludes memory for the model context and CUDA kernel reservations.</p> Model size FP16 FP8 INT4 8B 16GB 8GB 4GB 70B 140GB 70GB 35GB 405B 810GB 405GB 203GB <p>For example, the FP16 version of Llama 3.1 405B won't fit into a single machine with eight 80GB GPUs, so we'd need at least two nodes.</p>"},{"location":"examples/llms/llama31/#quantization","title":"Quantization","text":"<p>The INT4 version of Llama 3.1 70B, can fit into two 40GB GPUs.</p> <p>The INT4 version of Llama 3.1 405B can fit into eight 40GB GPUs.</p> <p>Useful links:</p> <ul> <li>Meta's official FP8 quantized version of Llama 3.1 405B (with minimal accuracy degradation)</li> <li>Llama 3.1 Quantized Models with quantized checkpoints</li> </ul>"},{"location":"examples/llms/llama31/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HUGGING_FACE_HUB_TOKEN=...\n\n$ dstack apply -f examples/llms/llama31/vllm/task.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB    yes   $0.12\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB    yes   $0.12\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:2  yes   $0.23\n\nSubmit the run llama31-task-vllm? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>If you run a task, <code>dstack apply</code> automatically forwards the remote ports to <code>localhost</code> for convenient access.</p> <pre><code>$ curl 127.0.0.1:8001/v1/chat/completions \\\n    -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is Deep Learning?\"\n        }\n      ],\n      \"max_tokens\": 128\n    }'\n</code></pre>"},{"location":"examples/llms/llama31/#fine-tuning-with-trl","title":"Fine-tuning with TRL","text":""},{"location":"examples/llms/llama31/#running-on-multiple-gpus","title":"Running on multiple GPUs","text":"<p>Here is the task configuration file for fine-tuning Llama 3.1 8B on the <code>OpenAssistant/oasst_top1_2023-08-25</code>  dataset.</p> <pre><code>type: task\nname: trl-train\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/multi_gpu.yaml\n    --num_processes $DSTACK_GPUS_PER_NODE \n    examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n\nresources:\ngpu:\n  # 24GB or more vRAM\n  memory: 24GB..\n  # One or more GPU\n  count: 1..\n# Shared memory (for multi-gpu)\nshm_size: 24GB\n</code></pre> <p>Change the <code>resources</code> property to specify more GPUs. </p>"},{"location":"examples/llms/llama31/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning Llama 3.1.</p> Model size Full fine-tuning LoRA QLoRA 8B 60GB 16GB 6GB 70B 500GB 160GB 48GB 405B 3.25TB 950GB 250GB <p>The requirements can be significantly reduced with certain optimizations.</p>"},{"location":"examples/llms/llama31/#deepspeed","title":"DeepSpeed","text":"<p>For more memory-efficient use of multiple GPUs, consider using DeepSpeed and ZeRO Stage 3.</p> <p>To do this, use the <code>examples/accelerate_configs/deepspeed_zero3.yaml</code> configuration file instead of  <code>examples/accelerate_configs/multi_gpu.yaml</code>.</p>"},{"location":"examples/llms/llama31/#running-on-multiple-nodes","title":"Running on multiple nodes","text":"<p>In case the model doesn't feet into a single GPU, consider running a <code>dstack</code> task on multiple nodes. Below is the corresponding task configuration file.</p> <pre><code>type: task\nname: trl-train-distrib\n\n# Size of the cluster\nnodes: 2\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/fsdp_qlora.yaml \n    --main_process_ip=$DSTACK_MASTER_NODE_IP\n    --main_process_port=8008\n    --machine_rank=$DSTACK_NODE_RANK\n    --num_processes=$DSTACK_GPUS_NUM\n    --num_machines=$DSTACK_NODES_NUM\n      examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n    --torch_dtype bfloat16\n    --use_bnb_nested_quant\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n  # Shared memory (for multi-gpu)\n  shm_size: 24GB\n</code></pre>"},{"location":"examples/llms/llama31/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/llms/llama31</code>  and <code>examples/fine-tuning/trl</code> .</p>"},{"location":"examples/llms/llama31/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and protips.</li> <li>Browse Llama 3.1 on HuggingFace ,     HuggingFace's Llama recipes ,     Meta's Llama recipes      and Llama Agentic System .</li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}