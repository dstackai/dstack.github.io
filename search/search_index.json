{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/","title":"dstack 0.9.1: Azure integration","text":"<p>The latest update introduces Azure support among other improvements.</p> <p>At <code>dstack</code>, our goal is to create a simple and unified interface for ML engineers to run dev environments, pipelines, and apps on any cloud. With the latest update, we take another significant step in this direction.</p> <p>We are thrilled to announce that the latest update introduces Azure support, among other things, making it incredibly easy to run dev environments, pipelines, and apps in Azure. Read on for more details.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#azure-support","title":"Azure support","text":"<p>Using Azure with <code>dstack</code> is very straightforward. All you need to do is create the corresponding project via the UI and  provide your Azure credentials.</p> <p></p> <p>NOTE:</p> <p>For detailed instructions on setting up <code>dstack</code> for Azure, refer to the documentation.</p> <p>Once the project is set up, you can define dev environments, pipelines, and apps as code, and easily run them with just a single command. <code>dstack</code> will automatically provision the infrastructure for you.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#logs-and-artifacts-in-ui","title":"Logs and artifacts in UI","text":"<p>Secondly, with the new update, you now have the ability to browse the logs and artifacts of any run through the user interface.</p> <p></p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#better-documentation","title":"Better documentation","text":"<p>Last but not least, with the update, we have reworked the documentation to provide a greater emphasis on specific use cases: dev environments,  tasks, and services.</p>"},{"location":"blog/2023/05/22/azure-support-better-ui-and-more/#try-it-out","title":"Try it out","text":"<p>Please note that when installing <code>dstack</code> via <code>pip</code>, you now need to specify the exact list of cloud providers you intend to use:</p> <pre><code>$ pip install \"dstack[aws,gcp,azure]\" -U\n</code></pre> <p>This requirement applies only when you start the server locally. If you connect to a server hosted elsewhere,  you can use the shorter syntax:<code>pip install dstack</code>.</p> <p>Feedback</p> <p>If you have any feedback, including issues or questions, please share them in our Discord community or file it as a GitHub issue.</p>"},{"location":"blog/2023/12/22/disk-size-cuda-12-1-mixtral-and-more/","title":"dstack 0.13.0: Disk size, CUDA 12.1, Mixtral, and more","text":"<p>The update brings configurable disk, updates CUDA drivers, and features Mixtral guide.</p> <p>As we wrap up this year, we're releasing a new update and publishing a guide on deploying Mixtral 8x7B with <code>dstack</code>.</p>"},{"location":"blog/2023/12/22/disk-size-cuda-12-1-mixtral-and-more/#configurable-disk-size","title":"Configurable disk size","text":"<p>Previously, <code>dstack</code> set the disk size to <code>100GB</code> regardless of the cloud provider. Now, to accommodate larger language models and datasets, <code>dstack</code> enables setting a custom disk size using <code>--disk</code> in <code>dstack run</code> or via the <code>disk</code> property in <code>.dstack/profiles.yml</code>.</p>"},{"location":"blog/2023/12/22/disk-size-cuda-12-1-mixtral-and-more/#default-docker-image","title":"Default Docker image","text":"<p>With <code>dstack</code>, whether you're using dev environments, tasks, or services, you can opt for a custom Docker image (for self-installed dependencies) or stick with the default Docker image (<code>dstack</code> pre-installs CUDA drivers, Conda, Python, etc.).</p> <p>We've upgraded the default Docker image's CUDA drivers to 12.1 (for better compatibility with modern libraries).</p> <p>nvcc</p> <p>If you're using the default Docker image and need the CUDA compiler (<code>nvcc</code>), you'll have to install it manually using <code>conda install cuda</code>. The image comes pre-configured with the  <code>nvidia/label/cuda-12.1.0</code> Conda channel.</p>"},{"location":"blog/2023/12/22/disk-size-cuda-12-1-mixtral-and-more/#mixtral-8x7b","title":"Mixtral 8x7B","text":"<p>Lastly, and most importantly, we've added a guide on deploying Mixtral 8x7B as a service. This guide allows you to effortlessly deploy a Mixtral endpoint on any cloud platform of your preference.</p> <p>Deploying Mixtral 8x7B is easy, especailly when using vLLM:</p> <pre><code>type: service\n\npython: \"3.11\"\n\ncommands:\n  - conda install cuda # (required by megablocks)\n  - pip install torch # (required by megablocks)\n  - pip install vllm megablocks\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size 2 # should match the number of GPUs\n\nport: 8000\n</code></pre> <p>Once the configuration is defined, goahead and run it:</p> <pre><code>$ dstack run . -f llms/mixtral.dstack.yml --gpu \"80GB:2\" --disk 200GB\n</code></pre> <p>It will deploy the endpoint at <code>https://&lt;run-name&gt;.&lt;gateway-domain&gt;</code>.</p> <p>Because vLLM provides an OpenAI-compatible endpoint, feel free to access it using various OpenAI-compatible tools like Chat UI, LangChain, Llama Index, etc.</p> <p></p> <p>Check the complete guide for more details.</p> <p>Don't forget, with <code>dstack</code>, you can use spot instances across different clouds and regions. Check out our recent guide on this topic.</p>"},{"location":"blog/2023/12/22/disk-size-cuda-12-1-mixtral-and-more/#feedback-and-support","title":"Feedback and support","text":"<p>That's all! Feel free to try out the update and the new guide, and share your feedback with us.</p> <p>For updates or assistance, join our Discord.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/","title":"GCP support just landed","text":"<p>The 0.2 update adds support for Google Cloud Platform (GCP).</p> <p>With the release of version 0.2 of <code>dstack</code>, it is now possible to configure GCP as a remote. All features that were previously available for AWS, except real-time artifacts, are now available for GCP as well.</p> <p>This means that you can define your ML workflows in code and easily run them locally or remotely in your GCP account.</p> <p><code>dstack</code> automatically creates and deletes cloud instances as needed, and assists in setting up the environment, including pipeline dependencies, and saving/loading artifacts. </p> <p>No code changes are required since ML workflows are described in YAML. You won't need to deal with Docker, Kubernetes, or stateful UI.</p> <p>This article will explain how to use <code>dstack</code> to run remote ML workflows on GCP.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the latest version of <code>dstack</code> before proceeding.</p> <pre><code>$ pip install dstack --upgrade\n</code></pre> <p>By default, workflows run locally. To run workflows remotely, e.g. on a GCP account), you must configure a remote using the <code>dstack config</code> command. Follow the steps below to do so.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#1-create-a-project","title":"1. Create a project","text":"<p>First you have to create a project in your GCP account, link a billing to it, and make sure that the required APIs and enabled for it.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#2-create-a-storage-bucket","title":"2. Create a storage bucket","text":"<p>Once the project is set up, you can proceed and create a storage bucket. This bucket will be used to store workflow artifacts and metadata.</p> <p>NOTE:</p> <p>Make sure to create the bucket in the sane location where you'd like to run your workflows.</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#3-create-a-service-account","title":"3. Create a service account","text":"<p>The next step is to create a service account in the created project and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p> <p>Once the service account is set up, create a key for it and download the corresponding JSON file to your local machine (e.g. to <code>~/Downloads/my-awesome-project-d7735ca1dd53.json</code>).</p>"},{"location":"blog/2023/03/13/gcp-support-just-landed/#4-configure-the-cli","title":"4. Configure the CLI","text":"<p>Once the service account key JSON file is on your machine, you can configure the CLI using the <code>dstack config</code> command.</p> <p>The command will ask you for a path to the key, GCP region and zone, and storage bucket name.</p> <pre><code>$ dstack config\n\n? Choose backend: gcp\n? Enter path to credentials file: ~/Downloads/dstack-d7735ca1dd53.json\n? Choose GCP geographic area: North America\n? Choose GCP region: us-west1\n? Choose GCP zone: us-west1-b\n? Choose storage bucket: dstack-dstack-us-west1\n? Choose VPC subnet: no preference\n</code></pre> <p>That's it! Now you can run remote workflows on GCP.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/","title":"dstack 0.7.0: Introducing dstack server","text":"<p>The latest update introduces the server with UI, team management, and more.</p> <p>Last October, we open-sourced the <code>dstack</code> CLI for defining ML workflows as code and running them easily on any cloud or locally. The tool abstracts ML engineers from vendor APIs and infrastructure, making it convenient to run scripts, development environments, and applications.</p> <p>Today, we are excited to announce a preview of <code>Hub</code>, a new way to use dstack for teams to manage their model development workflows effectively on any cloud platform.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#how-does-it-work","title":"How does it work?","text":"<p>Previously, the <code>dstack</code> CLI configured a cloud account as a remote to use local cloud credentials for direct requests to the cloud. Now, the CLI allows configuration of Hub as a remote, enabling requests to the cloud using user credentials stored in Hub.</p> <pre><code>sequenceDiagram\n  autonumber\n  participant CLI\n  participant Hub\n  participant Cloud\n  %  Note right of Cloud: AWS, GCP, etc\n  CLI-&gt;&gt;Hub: Run a workflow\n  activate Hub\n      Hub--&gt;&gt;Hub: User authentication\n      loop Workflow provider\n        Hub--&gt;&gt;Cloud: Submit workflow jobs\n      end\n  Hub--&gt;&gt;CLI: Return the workflow status\n  deactivate Hub\n  loop Workflow scheduler\n    Hub--&gt;&gt;Cloud: Re-submit workflow jobs\n  end</code></pre> <p>The Hub not only provides basic features such as authentication and credential storage, but it also has built-in workflow scheduling capabilities. For instance, it can monitor the availability of spot instances and automatically resubmit jobs.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#why-does-it-matter","title":"Why does it matter?","text":"<p>As you start developing models more regularly, you'll encounter the challenge of automating your ML workflows to reduce time spent on infrastructure and manual work.</p> <p>While many cloud vendors offer tools to automate ML workflows, they do so through opinionated UIs and APIs, leading to a suboptimal developer experience and vendor lock-in.</p> <p>In contrast, <code>dstack</code> aims to provide a non-opinionated and developer-friendly interface that can work across any  vendor.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#try-the-preview","title":"Try the preview","text":"<p>Here's a quick guide to get started with Hub:</p> <ol> <li>Start the Hub application</li> <li>Visit the URL provided in the output to log in as an administrator</li> <li>Create a project and configure its backend (AWS or GCP)</li> <li>Configure the CLI to use the project as a remote</li> </ol> <p>For more details, visit the Hub documentation. </p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#whats-next","title":"What's next?","text":"<p>Currently, the only way to run or manage workflows is through the <code>dstack</code> CLI. There are scenarios when you'd prefer to run workflows other ways, e.g. from Python code or programmatically via API. To support these scenarios, we plan to release soon Python SDK and REST API.</p> <p>The built-in scheduler currently monitors spot instance availability and automatically resubmits jobs. Our plan is to enhance this feature and include additional capabilities. Users will be able to track cloud compute usage, and manage quotes per team via the user interface.</p> <p>Lastly, and of utmost importance, we plan to extend support to other cloud platforms, not limiting ourselves to AWS, GCP, and Azure.</p>"},{"location":"blog/2023/04/11/introducing-dstack-hub/#contribution","title":"Contribution","text":"<p>You are encouraged to report any bugs, suggest new features, and provide feedback to improve Hub through GitHub issues.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/","title":"dstack 0.10.5: Lambda integration, Docker support, and more","text":"<p>The latest release improves Lambda Cloud integration and adds support for Docker.</p> <p>In the previous update, we added initial integration with Lambda Cloud. With today's release, this integration has significantly improved and finally goes generally available. Additionally, the latest release adds support for custom Docker images.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#lambda-cloud","title":"Lambda Cloud","text":"<p>In this update, we've added a possibility to create Lambda Cloud projects via the user interface.</p> <p></p> <p>All you need to do is provide your Lambda Cloud API key, and specify an S3 bucket and AWS credentials  for storing state and artifacts.</p> <p>Learn more \u2192</p> <p>Once the project is configured, feel free to run dev environments and tasks in Lambda Cloud using the <code>dstack</code> CLI.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#custom-docker-images","title":"Custom Docker images","text":"<p>By default, <code>dstack</code> uses its own base Docker images to run  dev environments and tasks. These base images come pre-configured with Python, Conda, and essential CUDA drivers.  However, there may be times when you need additional dependencies that you don't want to install every time you run your dev environment or task.</p> <p>To address this, <code>dstack</code> now allows specifying custom Docker images. Here's an example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nenv:\n  - MODEL_ID=tiiuae/falcon-7b\n\nports:\n - 3000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 3000 --trust-remote-code\n</code></pre> Existing limitations <p>Dev environments require the Docker image to have <code>openssh-server</code> pre-installed. If you want to use a custom Docker image with a dev environment and it does not include <code>openssh-server</code>, you can install it using the following  method:</p> <pre><code>type: dev-environment\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nbuild:\n  - apt-get update\n  - DEBIAN_FRONTEND=noninteractive apt-get install -y openssh-server\n  - rm -rf /var/lib/apt/lists/*\n\nide: vscode\n</code></pre> <p>The documentation and examples are updated to reflect the changes in the release.</p>"},{"location":"blog/2023/07/14/lambda-cloud-ga-and-docker-support/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"blog/2023/08/22/multiple-clouds/","title":"dstack 0.11.0: Multi-cloud and multi-region projects","text":"<p>The latest update now automatically finds the cheapest GPU across clouds and regions.</p> <p>The latest release of <code>dstack</code> enables the automatic discovery of the best GPU price and availability across multiple configured cloud providers and regions.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#multiple-backends-per-project","title":"Multiple backends per project","text":"<p>Now, <code>dstack</code> leverages price data from multiple configured cloud providers and regions to automatically suggest the most cost-effective options.</p> <pre><code>$ dstack run . -f llama-2/train.dstack.yml --gpu A100\n\n Configuration       llama-2/train.dstack.yml\n Min resources       2xCPUs, 8GB, 1xA100\n Max price           no\n Spot policy         auto\n Max duration        72h\n\n #  BACKEND  RESOURCES                      SPOT  PRICE\n 2  lambda   30xCPUs, 200GB, 1xA100 (80GB)  yes   $1.1\n 3  gcp      12xCPUs, 85GB, 1xA100 (40GB)   yes   $1.20582\n 1  azure    24xCPUs, 220GB, 1xA100 (80GB)  yes   $1.6469\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>The default behavior of <code>dstack</code> is to first attempt the most cost-effective options, provided they are available. You have the option to set a maximum price limit either through <code>max_price</code> in <code>.dstack/profiles.yml</code> or by using <code>--max-price</code> in the <code>dstack run</code> command.</p> <p>To implement this change, we have modified the way projects are configured. You can now configure multiple clouds and regions within a single project.</p> <p></p> <p>Why this matter?</p> <p>The ability to run LLM workloads across multiple cloud GPU providers allows for a significant reduction in costs and an increase in availability, while also remaining independent of any particular cloud vendor.</p> <p>We hope that the value of <code>dstack</code> will continue to grow as we expand our support for additional cloud GPU providers. If you're interested in a specific provider, please message us on Discord.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#custom-domains-and-https","title":"Custom domains and HTTPS","text":"<p>In other news, it is now possible to deploy services using HTTPS. All you need to do is configure a wildcard domain (e.g., <code>*.mydomain.com</code>), point it to the gateway IP address, and then pass the subdomain you want to use (e.g., <code>myservice.mydomain.com</code>) to the <code>gateway</code> property in YAML (instead of the gateway IP address).</p>"},{"location":"blog/2023/08/22/multiple-clouds/#other-changes","title":"Other changes","text":""},{"location":"blog/2023/08/22/multiple-clouds/#dstackprofilesyml","title":".dstack/profiles.yml","text":"<ul> <li>The <code>project</code> property is no longer supported.</li> <li>You can now use <code>max_price</code> to set the maximum price per hour in dollars.</li> </ul>"},{"location":"blog/2023/08/22/multiple-clouds/#dstack-run","title":"dstack run","text":"<p>Using the dstack run command, you are now able to utilize options such as <code>--gpu</code>, <code>--memory</code>, <code>--env</code>, <code>--max-price</code>, and several other arguments to override the profile settings.</p> <p>Lastly, the local backend is no longer supported. Now, you can run everything using only a cloud backend.</p> <p>The documentation is updated to reflect the changes in the release.</p> <p>Migration to 0.11</p> <p>The <code>dstack</code> version 0.11 update brings significant changes that break backward compatibility. If you used prior <code>dstack</code> versions, after updating to <code>dstack==0.11</code>, you'll need to log in to the UI and reconfigure clouds. </p> <p>We apologize for any inconvenience and aim to ensure future updates maintain backward compatibility.</p>"},{"location":"blog/2023/08/22/multiple-clouds/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Why managed notebooks are losing ground to cloud dev environments.</p> <p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/2023/06/29/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/2023/08/07/services-preview/","title":"dstack 0.10.7: Introducing services to simplify deployment","text":"<p>The latest update introduces services, a new configuration type for easier deployment.</p> <p>Until now, <code>dstack</code> has supported <code>dev-environment</code> and <code>task</code> as configuration types. Even though <code>task</code>  may be used for basic serving use cases, it lacks crucial serving features. With the new update, we introduce <code>service</code>, a dedicated configuration type for serving.</p> <p>Consider the following example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nports: \n  - 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>When running it, the <code>dstack</code> CLI forwards traffic to <code>127.0.0.1:8000</code>. This is convenient for development but unsuitable for production.</p> <p>In production, you need your endpoint available on the external network, preferably behind authentication  and a load balancer. </p> <p>This is why we introduce the <code>service</code> configuration type.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>As you see, there are two differences compared to <code>task</code>.</p> <ol> <li>The <code>gateway</code> property: the address of a special cloud instance that wraps the running service with a public    endpoint. Currently, you must specify it manually. In the future, <code>dstack</code> will assign it automatically.</li> <li>The <code>port</code> property: A service must always configure one port on which it's running.</li> </ol> <p>When running, <code>dstack</code> forwards the traffic to the gateway, providing you with a public endpoint that you can use to access the running service.</p> Existing limitations <ol> <li>Currently, you must create a gateway manually using the <code>dstack gateway</code> command  and specify its address via YAML (e.g. using secrets). In the future, <code>dstack</code> will assign it automatically.</li> <li>Gateways do not support HTTPS yet. When you run a service, its endpoint URL is <code>&lt;the address of the gateway&gt;:80</code>.  The port can be overridden via the port property: instead of <code>8000</code>, specify <code>&lt;gateway port&gt;:8000</code>.</li> <li>Gateways do not provide authorization and auto-scaling. In the future, <code>dstack</code> will support them as well.</li> </ol> <p>This initial support for services is the first step towards providing multi-cloud and cost-effective inference.</p> <p>Give it a try and share feedback</p> <p>Even though the current support is limited in many ways, we encourage you to give it a try and share your feedback with us!</p> <p>More details on how to use services can be found in a dedicated guide in our docs.  Questions and requests for help are very much welcome in our Discord server.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/","title":"dstack 0.12.0: Simplified cloud setup, and refined API","text":"<p>The latest update simplifies cloud configuration and enhances the Python API.</p> <p>For the past six weeks, we've been diligently overhauling <code>dstack</code> with the aim of significantly simplifying the process of configuring clouds and enhancing the functionality of the API. Please take note of the breaking changes, as they necessitate careful migration.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#cloud-setup","title":"Cloud setup","text":"<p>Previously, the only way to configure clouds for a project was through the UI. Additionally, you had to specify not only the credentials but also set up a storage bucket for each cloud to store metadata.</p> <p>Now, you can configure clouds for a project via <code>~/.dstack/server/config.yml</code>. Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Regions and other settings are optional. Learn more on what credential types are supported  via Clouds.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#enhanced-api","title":"Enhanced API","text":"<p>The earlier introduced Python API is now greatly refined.</p> <p>Creating a <code>dstack</code> client is as easy as this: </p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError:\n    print(\"Can't connect to the server\")\n</code></pre> <p>Now, you can submit a task or a service:</p> <pre><code>from dstack.api import Task, Resources, GPU\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",\n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre> <p>The <code>dstack.api.Run</code> instance provides methods for various operations including attaching to the run,  forwarding ports to <code>localhost</code>, retrieving status, stopping, and accessing logs. For more details, refer to  the example and reference.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#other-changes","title":"Other changes","text":"<ul> <li>Because we've prioritized CLI and API UX over the UI, the UI is no longer bundled.  Please inform us if you experience any significant inconvenience related to this.</li> <li>Gateways should now be configured using the <code>dstack gateway</code> command, and their usage requires you to specify a domain.   Learn more about how to set up a gateway.</li> <li>The <code>dstack start</code> command is now <code>dstack server</code>.</li> <li>The Python API classes were moved from the <code>dstack</code> package to <code>dstack.api</code>.</li> </ul>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#migration","title":"Migration","text":"<p>Unfortunately, when upgrading to 0.12.0, there is no automatic migration for data. This means you'll need to delete <code>~/.dstack</code> and configure <code>dstack</code> from scratch.</p> <ol> <li><code>pip install \"dstack[all]==0.12.0\"</code></li> <li>Delete <code>~/.dstack</code></li> <li>Configure clouds via <code>~/.dstack/server/config.yml</code> (see the new guide)</li> <li>Run <code>dstack server</code></li> </ol> <p>The documentation and examples are updated.</p>"},{"location":"blog/2023/10/18/simplified-cloud-setup/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n</code></pre> <p>Feedback and support</p> <p>Questions and requests for help are very much welcome in our  Discord server.</p>"},{"location":"blog/2023/10/31/tensordock/","title":"dstack 0.12.2: TensorDock integration","text":"<p>With dstack 0.12.2, enjoy highly competitive pricing for cloud GPU with TensorDock.</p> <p>At <code>dstack</code>, we remain committed to our mission of building the most convenient tool for orchestrating generative AI workloads in the cloud. In today's release, we have added support for TensorDock, making it easier for you to leverage cloud GPUs at highly competitive prices.</p> <p>Configuring your TensorDock account with <code>dstack</code> is very easy. Simply generate an authorization key in your TensorDock API settings and set it up in <code>~/.dstack/server/config.yml</code>:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>Now you can restart the server and proceed to using the CLI or API for running development environments, tasks, and services.</p> <pre><code>$ dstack run . -f .dstack.yml --gpu 40GB\n\n Min resources  1xGPU (40GB)\n Max price      -\n Max duration   6h\n Retry policy   no\n\n #  REGION        INSTANCE  RESOURCES                     SPOT  PRICE\n 1  unitedstates  ef483076  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6235\n 2  canada        0ca177e7  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n 3  canada        45d0cabd  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>TensorDock offers cloud GPUs on top of servers from dozens of independent hosts, providing some of the most affordable GPU pricing you can find on the internet.</p> <p>With <code>dstack</code>, you can now utilize TensorDock's GPUs through a highly convenient interface, which includes the developer-friendly CLI and API.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p>"},{"location":"blog/2023/11/21/vastai/","title":"dstack 0.12.3: Vast.ai integration","text":"<p>With dstack 0.12.3, you can now use Vast.ai's GPU marketplace as a cloud provider.</p> <p><code>dstack</code> simplifies gen AI model development and deployment through its developer-friendly CLI and API.  It eliminates cloud infrastructure hassles while supporting top cloud providers (such as AWS, GCP, Azure, among others).</p> <p>While <code>dstack</code> streamlines infrastructure challenges, GPU costs can still hinder development. To address this,  we've integrated <code>dstack</code> with Vast.ai, a marketplace providing GPUs from independent hosts at  notably lower prices compared to other providers.</p> <p>With the <code>dstack</code> 0.12.3 release, it's now possible use Vast.ai alongside other cloud providers.</p> <pre><code>$ dstack run . --gpu 24GB --backend vastai --max-price 0.4\n\n #  REGION            INSTANCE  RESOURCES                       PRICE\n 1  pl-greaterpoland  6244171   16xCPU, 32GB, 1xRTX3090 (24GB)  $0.18478\n 2  ee-harjumaa       6648481   16xCPU, 64GB, 1xA5000 (24GB)    $0.29583\n 3  pl-greaterpoland  6244172   32xCPU, 64GB, 2xRTX3090 (24GB)  $0.36678\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>By default, it suggests GPU instances based on their quality score. If you want to, you can control the maximum price.</p> <p>Configuring Vast.ai for use with <code>dstack</code> is easy. Log into your Vast AI account, click <code>Account</code> in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>Now you can restart the server and proceed to using <code>dstack</code>'s CLI and API.</p> <p>If you want an easy way to  develop, train and deploy gen AI models using affordable cloud GPUs,  give <code>dstack</code> with Vast.ai a try.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p>"},{"location":"docs/","title":"What is dstack?","text":"<p><code>dstack</code> is an open-source toolkit and orchestration engine for running GPU workloads.  It works seamlessly with top cloud GPU providers (AWS, GCP, Azure, Lambda, TensorDock, Vast.ai, etc.)</p>"},{"location":"docs/#why-use-dstack","title":"Why use dstack?","text":"<ol> <li>Designed for development, training, and deployment of gen AI models.</li> <li>Efficiently utilizes GPUs across regions and cloud providers.</li> <li>Compatible with any frameworks.</li> <li>100% open-source.</li> </ol>"},{"location":"docs/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Install the open-source <code>dstack</code> server and configure cloud credentials (or opt for the cloud version.) </li> <li>Define run configurations such as dev environments, tasks, and services.</li> <li>Execute configurations via the CLI or API. The <code>dstack</code> server automatically provisions cloud resources, handles     containers, logs, network, and everything else.</li> </ol>"},{"location":"docs/#where-do-i-start","title":"Where do I start?","text":"<ol> <li>Install the server (or sign up with the cloud version)</li> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord</li> </ol>"},{"location":"docs/quickstart/","title":"Quickstart","text":""},{"location":"docs/quickstart/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> Dev environmentTaskService <p> <pre><code>type: dev-environment\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\nide: vscode\n</code></pre> <p>A dev environments is a perfect tool for interactive experimentation with your IDE. It allows to pre-configure the Python version or a Docker image, etc. Go to Dev environments to learn more.</p> <p> <pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py\n</code></pre> <p>A task may run training scripts, batch jobs, or web apps. It allows to specify the commands, ports,  and pre-configure the Python version or a Docker image, etc. Go to Tasks to learn more.</p> <p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv: \n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ \n\nport: 80\n\ncommands:\n  - text-generation-launcher --hostname 0.0.0.0 --port 80 --trust-remote-code\n</code></pre> <p>A service makes it very easy to deploy models or web apps. It allows to specify the commands,  and the Python version or a Docker image, etc. Go to Services to learn more.</p>"},{"location":"docs/quickstart/#run-configuration","title":"Run configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>No need to worry about copying code, setting up environment, IDE, etc. <code>dstack</code> handles it all  automatically.</p>"},{"location":"docs/quickstart/#whats-next","title":"What's next?","text":"<ol> <li>Learn more about dev environments, tasks,      and services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"docs/concepts/dev-environments/","title":"Dev environments","text":"<p>Before submitting a long-running task or deploying a model, you may want to experiment  interactively using your IDE, terminal, or Jupyter notebooks.</p> <p>With <code>dstack</code>, you can provision a dev environment with the required cloud resources,  code, and environment via a single command.</p>"},{"location":"docs/concepts/dev-environments/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\nide: vscode\n</code></pre> <p>Configuration options</p> <p>You can specify your own Docker image, configure environment variables, etc. If no image is specified, <code>dstack</code> uses its own Docker image (pre-configured with Python, Conda, and essential CUDA drivers). For more details, refer to the Reference.</p>"},{"location":"docs/concepts/dev-environments/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f .dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+fast-moth-1/workflow\n</code></pre> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, disk size, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p> <p>Once the dev environment is provisioned, click the link to open the environment in your desktop IDE.</p> <p></p> <p>Port forwarding</p> <p>When running a dev environment, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access.</p> <p>No need to worry about copying code, setting up environment, IDE, etc. <code>dstack</code> handles it all  automatically.</p> .gitignore <p>When running a dev environment, <code>dstack</code> uses the exact version of code from your project directory. </p> <p>If there are large files, consider creating a <code>.gitignore</code> file to exclude them for better performance.</p>"},{"location":"docs/concepts/dev-environments/#whats-next","title":"What's next?","text":"<ol> <li>Browse examples</li> <li>Check the reference</li> </ol>"},{"location":"docs/concepts/services/","title":"Services","text":"<p>With <code>dstack</code>, you can use the CLI or API to deploy models or web apps. Provide the commands, port, and choose the Python version or a Docker image.</p> <p><code>dstack</code> handles the deployment on configured cloud GPU provider(s) with the necessary resources.</p> Prerequisites <p>If you're using the open-source server, you first have to set up a gateway.</p> <p>If you're using the cloud version of <code>dstack</code>, the gateway is set up for you.</p>"},{"location":"docs/concepts/services/#set-up-a-gateway","title":"Set up a gateway","text":"<p>For example, if your domain is <code>example.com</code>, go ahead and run the  <code>dstack gateway create</code> command:</p> <pre><code>$ dstack gateway create --domain example.com --region eu-west-1 --backend aws\n\nCreating gateway...\n---&gt; 100%\n\n BACKEND  REGION     NAME          ADDRESS        DOMAIN       DEFAULT\n aws      eu-west-1  sour-fireant  52.148.254.14  example.com  \u2713\n</code></pre> <p>Afterward, in your domain's DNS settings, add an <code>A</code> DNS record for <code>*.example.com</code>  pointing to the IP address of the gateway.</p> <p>This way, if you run a service, <code>dstack</code> will make its endpoint available at  <code>https://&lt;run-name&gt;.example.com</code>.</p>"},{"location":"docs/concepts/services/#using-the-cli","title":"Using the CLI","text":""},{"location":"docs/concepts/services/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv: \n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ \n\nport: 80\n\ncommands:\n  - text-generation-launcher --hostname 0.0.0.0 --port 80 --trust-remote-code\n</code></pre> <p>By default, <code>dstack</code> uses its own Docker images to run dev environments,  which are pre-configured with Python, Conda, and essential CUDA drivers.</p> <p>Configuration options</p> <p>Configuration file allows you to specify a custom Docker image, environment variables, and many other  options. For more details, refer to the Reference.</p>"},{"location":"docs/concepts/services/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f serve.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nServing HTTP on https://yellow-cat-1.example.com ...\n</code></pre> <p>Once the service is deployed, its endpoint will be available at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, disk size, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p> <p>What's next?</p> <ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about dev environments      and tasks</li> <li>Browse examples</li> <li>Check the reference</li> </ol>"},{"location":"docs/concepts/tasks/","title":"Tasks","text":"<p>With <code>dstack</code>, you can use the CLI or API to run tasks like training scripts, batch jobs, or web apps.  Provide the commands, ports, and choose the Python version or a Docker image.</p> <p><code>dstack</code> handles the execution on configured cloud GPU provider(s) with the necessary resources.</p>"},{"location":"docs/concepts/tasks/#using-the-cli","title":"Using the CLI","text":""},{"location":"docs/concepts/tasks/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py\n</code></pre> <p>A task can configure ports:</p> <pre><code>type: task\n\nports:\n  - 7860\n\npython: \"3.11\" # (Optional) If not specified, your local version is used.\n\ncommands:\n  - pip install -r requirements.txt\n  - gradio app.py\n</code></pre> <p>When running a task, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access.</p> <p>By default, <code>dstack</code> uses its own Docker images to run dev environments,  which are pre-configured with Python, Conda, and essential CUDA drivers.</p> <p>Configuration options</p> <p>Configuration file allows you to specify a custom Docker image, ports, environment variables, and many other  options. For more details, refer to the Reference.</p>"},{"location":"docs/concepts/tasks/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options (e.g., for requesting hardware resources).</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>Run options</p> <p>The <code>dstack run</code> command allows you to use <code>--gpu</code> to request GPUs (e.g. <code>--gpu A100</code> or <code>--gpu 80GB</code> or <code>--gpu A100:4</code>, etc.), and many other options (incl. spot instances, disk size, max price, max duration, retry policy, etc.). For more details, refer to the Reference.</p> Port mapping <p>When running a task, <code>dstack</code> forwards the remote ports to <code>localhost</code> for secure  and convenient access. You can override local ports via <code>--port</code>:</p> <pre><code>$ dstack run . -f serve.dstack.yml --port 8080:7860\n</code></pre> <p>This will forward the task's port <code>7860</code> to <code>localhost:8080</code>.</p>"},{"location":"docs/concepts/tasks/#parametrize-tasks","title":"Parametrize tasks","text":"<p>You can parameterize tasks with user arguments using <code>${{ run.args }}</code> in the configuration.</p> <p>Example:</p> <pre><code>type: task\n\npython: \"3.11\" # (Optional) If not specified, your local version is used\n\ncommands:\n  - pip install -r requirements.txt\n  - python train.py ${{ run.args }}\n</code></pre> <p>Now, you can pass your arguments to the <code>dstack run</code> command:</p> <pre><code>$ dstack run . -f train.dstack.yml --gpu A100 --train_batch_size=1 --num_train_epochs=100\n</code></pre> <p>The <code>dstack run</code> command will pass <code>--train_batch_size=1</code> and <code>--num_train_epochs=100</code> as arguments to <code>train.py</code>.</p>"},{"location":"docs/concepts/tasks/#configure-retry-limit","title":"Configure retry limit","text":"<p>By default, if <code>dstack</code> is unable to find capacity, <code>dstack run</code> will fail. However, you may pass the <code>--retry-limit</code> option to <code>dstack run</code> to specify the timeframe in which <code>dstack</code> should search for capacity and automatically resubmit the run.</p> <p>Example:</p> <pre><code>$ dstack run . -f train.dstack.yml --retry-limit 3h\n</code></pre> <p>For more details on the <code>dstack run</code> command, refer to the Reference.</p>"},{"location":"docs/concepts/tasks/#whats-next","title":"What's next?","text":"<ol> <li>Check the QLoRA example</li> <li>Read about dev environments      and services</li> <li>Browse examples</li> <li>Check the reference</li> </ol>"},{"location":"docs/guides/fine-tuning/","title":"Fine-tuning","text":"<p>For fine-tuning an LLM with <code>dstack</code>'s API, specify a model, dataset, training parameters, and required compute resources. The API takes care of everything else.</p> <p>The API currently supports only supervised fine-tuning (SFT). Support for DPO and RLHF is coming soon.</p>"},{"location":"docs/guides/fine-tuning/#how-does-it-work","title":"How does it work?","text":"<p><code>dstack</code> loads a base model and dataset from Hugging Face, schedules the fine-tuning task to run on a configured cloud, and reports metrics to a tracker of your choice.</p> <p></p> <p>Once the model is fine-tuned, it's pushed to Hugging Face and is ready for deployment.</p>"},{"location":"docs/guides/fine-tuning/#prepare-a-dataset","title":"Prepare a dataset","text":"<p>The dataset should contain a <code>\"text\"</code> column with completions following the prompt format of the corresponding model. Check the example (for fine-tuning Llama 2).</p> <p>Once the dataset is prepared, it must be uploaded to Hugging Face.</p> Uploading a dataset <p>Here's an example of how to upload a dataset programmatically:</p> <pre><code>import pandas as pd\nfrom datasets import Dataset\n\ndf = pd.read_json(\"samsum.jsonl\", lines=True)\ndataset = Dataset.from_pandas(df)\ndataset.push_to_hub(\"peterschmidt85/samsum\")\n</code></pre>"},{"location":"docs/guides/fine-tuning/#create-a-client","title":"Create a client","text":"<p>First, you connect to <code>dstack</code>:</p> <pre><code>from dstack.api import Client\n\nclient = Client.from_config()\n</code></pre>"},{"location":"docs/guides/fine-tuning/#create-a-task","title":"Create a task","text":"<p>Then, you create a fine-tuning task, specifying the model and dataset,  and various training parameters.</p> <pre><code>from dstack.api import FineTuningTask\n\ntask = FineTuningTask(\n    model_name=\"NousResearch/Llama-2-13b-hf\",\n    dataset_name=\"peterschmidt85/samsum\",\n    env={\n        \"HUGGING_FACE_HUB_TOKEN\": \"...\",\n    },\n    num_train_epochs=2,\n    max_seq_length=1024,\n    per_device_train_batch_size=2,\n)\n</code></pre>"},{"location":"docs/guides/fine-tuning/#run-the-task","title":"Run the task","text":"<p>When running a task, you can configure resources, and many other options.</p> <pre><code>from dstack.api import Resources, GPU\n\nrun = client.runs.submit(\n    run_name=\"Llama-2-13b-samsum\", # (Optional) If unset, its chosen randomly\n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre> <p>GPU memory</p> <p>The API defaults to using QLoRA based on the provided  training parameters. When specifying GPU memory, consider both the model size and the specified batch size. After a few attempts, you'll discover the best configuration.</p> <p>When the training is done, the API pushes the final model to the Hugging Face hub.</p> <p></p>"},{"location":"docs/guides/fine-tuning/#manage-runs","title":"Manage runs","text":"<p>You can manage runs using API, the CLI, or the user interface.</p>"},{"location":"docs/guides/fine-tuning/#track-metrics","title":"Track metrics","text":"<p>To track experiment metrics, specify <code>report_to</code> and related authentication environment variables.</p> <pre><code>task = FineTuningTask(\n    model_name=\"NousResearch/Llama-2-13b-hf\",\n    dataset_name=\"peterschmidt85/samsum\",\n    report_to=\"wandb\",\n    env={\n        \"HUGGING_FACE_HUB_TOKEN\": \"...\",\n        \"WANDB_API_KEY\": \"...\",\n    },\n    num_train_epochs=2\n)\n</code></pre> <p>Currently, the API supports <code>\"tensorboard\"</code> and <code>\"wandb\"</code>.</p> <p></p>"},{"location":"docs/guides/fine-tuning/#whats-next","title":"What's next?","text":"<ul> <li>Once the model is trained, proceed to deploy it as an endpoint.   The deployed endpoint can be used from your apps directly or via LangChain.</li> <li>The source code of the fine-tuning task is available   at GitHub.   If you prefer using a custom script, feel free to do so using dev environments and    tasks.</li> </ul>"},{"location":"docs/guides/text-generation/","title":"Text generation","text":"<p>The easiest way to deploy a text generation model is by using <code>dstack</code> API. You only need to specify a model, quantization parameters,  and required compute resources.</p> Prerequisites <p>If you're using the cloud version of <code>dstack</code>, no prerequisites are required.</p> <p>However, if you're using the open-source server, you need to  set up a gateway before running models as public endpoints.  Not required for private endpoints.</p>"},{"location":"docs/guides/text-generation/#create-a-client","title":"Create a client","text":"<p>First, you connect to <code>dstack</code>:</p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError:\n    print(\"Can't connect to the server\")\n</code></pre>"},{"location":"docs/guides/text-generation/#create-a-configuration","title":"Create a configuration","text":"<p><code>dstack</code> allows to run a model either as a public endpoint or as a private endpoint.</p> Public endpointPrivate endpoint <pre><code>from dstack.api import CompletionService\n\nconfiguration = CompletionService(\n    model_name=\"TheBloke/CodeLlama-34B-GPTQ\",\n    quantize=\"gptq\"\n)\n</code></pre> <p>When you run a model as a public endpoint, <code>dstack</code> makes it available at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <pre><code>from dstack.api import CompletionTask\n\nconfiguration = CompletionTask(\n    model_name=\"TheBloke/CodeLlama-34B-GPTQ\",\n    quantize=\"gptq\",\n    local_port=\"8080,\n)\n</code></pre> <p>When you run a model as a private endpoint, <code>dstack</code> makes it available only at <code>http://localhost:&lt;local_port&gt;</code> (even though the model itself is running in the cloud).  This is convenient if you intend to access the endpoint solely from your local machine.</p>"},{"location":"docs/guides/text-generation/#run-the-configuration","title":"Run the configuration","text":"<p>When running a service, you can configure resources, and many other options.</p> <pre><code>from dstack.api import Resources, GPU\n\nrun = client.runs.submit(\n    run_name=\"codellama-34b-gptq\", # (Optional) If unset, its chosen randomly\n    configuration=configuration,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre>"},{"location":"docs/guides/text-generation/#access-the-endpoint","title":"Access the endpoint","text":"Public endpointPrivate endpoint <pre><code>$ curl https://&amp;lt;run-name&amp;gt;.&amp;lt;domain-name&amp;gt;/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\": 20}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The OpenAPI documentation on the endpoint can be found at <code>https://&lt;run-name&gt;.&lt;domain-name&gt;/docs</code>.</p> <pre><code>$ curl http://localhost:&amp;lt;local-port&amp;gt;/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\": 20}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The OpenAPI documentation on the endpoint can be found at <code>http://localhost:&lt;local-port&gt;/docs</code>.</p> <p>Both public and private endpoint support streaming, continuous batching, tensor parallelism, etc.</p>"},{"location":"docs/guides/text-generation/#manage-runs","title":"Manage runs","text":"<p>You can use the instance of <code>dstack.api.Client</code> to manage your runs,  including getting a list of runs, stopping a given run, etc.</p>"},{"location":"docs/installation/","title":"Installation","text":"<p>Follow this guide to install the open-source version of <code>dstack</code> server.</p>"},{"location":"docs/installation/#set-up-the-server","title":"Set up the server `","text":"pipDocker <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n\nApplying configuration from ~/.dstack/server/config.yml...\n\nThe server is running at http://127.0.0.1:3000/\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\n</code></pre> <pre><code>$ docker run -p 3000:3000 -v $HOME/.dstack/server/:/root/.dstack/server dstackai/dstack\n\nApplying configuration from ~/.dstack/server/config.yml...\n\nThe server is running at http://127.0.0.1:3000/.\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\n</code></pre> <p>NOTE:</p> <p>For flexibility, <code>dstack</code> server allows you to configure multiple project. The default project is <code>main</code>.</p>"},{"location":"docs/installation/#configure-credentials","title":"Configure credentials","text":"<p>To let <code>dstack</code> run workloads in your cloud account(s), you need to configure cloud credentials  in <code>~/.dstack/server/config.yml</code> under the <code>backends</code> property of the respective project.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Default credentials</p> <p>If you have default AWS, GCP, or Azure credentials on your machine, the <code>dstack</code> server will pick them up automatically. Otherwise, you have to configure them manually.</p>"},{"location":"docs/installation/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p> Access keyDefault credentials <p>Create an access key by following the this guide. Once you've downloaded the <code>.csv</code> file with your IAM user's Access key ID and Secret access key, proceed to  configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: KKAAUKLIZ5EHKICAOASV\n      secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> <p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: default\n</code></pre> Required AWS permissions <p>The following AWS policy permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"servicequotas:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetRole\",\n                \"iam:CreateRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:TagRole\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreatePolicy\",\n                \"iam:TagPolicy\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetInstanceProfile\",\n                \"iam:CreateInstanceProfile\",\n                \"iam:AddRoleToInstanceProfile\",\n                \"iam:TagInstanceProfile\",\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"docs/installation/#azure","title":"Azure","text":"<p>There are two ways to configure Azure: using a client secret or using the default credentials.</p> Client secretDefault credentials <p>A client secret can be created using the Azure CLI:</p> <pre><code>SUBSCRIPTION_ID=...\naz ad sp create-for-rbac \n    --name dstack-app \\\n    --role Owner \\ \n    --scopes /subscriptions/$SUBSCRIPTION_ID \\ \n    --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <p>Obtain the <code>subscription_id</code> and <code>tenant_id</code> via the Azure CLI:</p> <pre><code>az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>Then proceed to configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your <code>subscription_id</code>, run</p> <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre> Required Azure permissions <p>You must have the <code>Owner</code> permission for the Azure subscription. Please, let us know if your use case requires more granular Azure permissions.</p>"},{"location":"docs/installation/#gcp","title":"GCP","text":"Enable APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>.</p> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com \n</code></pre> <p>There are two ways to configure GCP: using a service account or using the default credentials.</p> Service accountDefault credentials <p>To create a service account, follow this guide. Make sure to grant it the <code>Service Account User</code> and <code>Compute Admin</code> roles.</p> <p>After setting up the service account create a key for it  and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: service_account\n      filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your GCP project ID, run </p> <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre> Required GCP permissions <p>The <code>Service Account User</code> and <code>Compute Admin</code> roles are sufficient for <code>dstack</code> to work.</p>"},{"location":"docs/installation/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: lambda\n    creds:\n      type: api_key\n      api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/installation/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock account, click API in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>NOTE:</p> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#vast-ai","title":"Vast AI","text":"<p>Log into your Vast AI account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>NOTE:</p> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#datacrunch","title":"DataCrunch","text":"<p>Log into your DataCrunch account, click Account Settings in the sidebar, find <code>REST API Credentials</code> area and then click the <code>Generate Credentials</code> button.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: datacrunch\n    creds:\n      type: api_key\n      client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n      client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/installation/#configure-regions","title":"Configure regions","text":"<p>In addition to credentials, each cloud (except TensorDock, Vast AI, and DataCrunch) optionally allows for region configuration.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    regions: [us-west-2, eu-west-1]\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>If regions aren't specified, <code>dstack</code> will use all available regions.</p> <p>After you update <code>~/.dstack/server/config.yml</code>, make sure to restart the server.</p>"},{"location":"docs/installation/#set-up-the-client","title":"Set up the client","text":"<p>The client is configured via <code>~/.dstack/config.yml</code> with the server address, user token, and the project name. </p> <p>If you run <code>dstack server</code> on the same machine, it automatically updates the client configuration for the default project (<code>main</code>).</p> <p>To configure the client on a different machine or for other projects, use <code>dstack config</code>.</p> <pre><code>$ dstack config --server &amp;lt;your server adddress&amp;gt; --project &amp;lt;your project name&amp;gt; --token &amp;lt;your user token&amp;gt;\n\nConfigurated is updated at ~/.dstack/config.yml\n</code></pre>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<p>With <code>dstack</code>, you can define what you want to run as YAML configuration files  and run them using the <code>dstack run</code> command. </p> <p>Filename</p> <p>Configuration files must have a name ending with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable). Configuration files can be placed either in the project's root directory or in any nested folder. </p> <p>Configurations can be of three types: <code>dev-environment</code>, <code>task</code>, and <code>service</code>.</p> <p>Below, you'll find the complete reference detailing all available properties for each type of configuration.</p>"},{"location":"docs/reference/dstack.yml/#dev-environment","title":"dev-environment","text":"<p>This configuration type allows you to provision a dev environment with the required cloud resources,  code, and environment.</p> Property Description Type Default value <code>type</code> <code>Literal['dev-environment']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>ide</code> The IDE to run <code>Literal['vscode']</code> required <code>version</code> The version of the IDE <code>Optional[str]</code> <code>None</code> <code>init</code> The bash commands to run <code>List[str]</code> <code>[]</code>"},{"location":"docs/reference/dstack.yml/#task","title":"task","text":"<p>This configuration type allows you to run tasks like training scripts, batch jobs, or web apps.</p> Property Description Type Default value <code>type</code> <code>Literal['task']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>commands</code> The bash commands to run <code>List[str]</code> required"},{"location":"docs/reference/dstack.yml/#service","title":"service","text":"<p>This configuration type allows you to deploy models or web apps as services.</p> Property Description Type Default value <code>type</code> <code>Literal['service']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, str]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>commands</code> The bash commands to run <code>List[str]</code> required <code>port</code> The port, that application listens to or the mapping <code>Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]</code> required <code>model</code> The model info for OpenAI interface <code>Optional[ModelInfo]</code> <code>None</code>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Instead of configuring resources and other run options through<code>dstack run</code>,  you can do so via <code>.dstack/profiles.yml</code> in the root folder of the project. </p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: large\n\n    resources:\n      memory: 24GB  # (Optional) The minimum amount of RAM memory\n      gpu:\n        name: A100 # (Optional) The name of the GPU\n        memory: 40GB # (Optional) The minimum amount of GPU memory \n      shm_size: 8GB # (Optional) The size of shared memory\n\n    spot_policy: auto # (Optional) The spot policy. Supports `spot`, `on-demand, and `auto`.\n\n    max_price: 1.5 # (Optional) The maximum price per instance per hour\n\n    max_duration: 1d # (Optional) The maximum duration of the run.\n\n    retry:\n      retry-limit: 3h # (Optional) To wait for capacity\n\n    backends: [azure, lambda]  # (Optional) Use only listed backends \n\n    default: true # (Optional) Activate the profile by default\n</code></pre> <p>You can mark any profile as default or pass its name via <code>--profile</code> to <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/profiles.yml/#profile","title":"Profile","text":"Property Description Type Default value <code>name</code> The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code> <code>str</code> required <code>backends</code> The backends to consider for provisionig (e.g., \"[aws, gcp]\") <code>Optional[List[BackendType]]</code> <code>None</code> <code>resources</code> The minimum resources of the instance to be provisioned <code>ProfileResources</code> <code>cpu=2 memory=8192 gpu=None shm_size=None disk=ProfileDisk(size=102400)</code> <code>spot_policy</code> The policy for provisioning spot or on-demand instances: spot, on-demand, or auto <code>Optional[SpotPolicy]</code> <code>None</code> <code>retry_policy</code> The policy for re-submitting the run <code>Optional[ProfileRetryPolicy]</code> <code>None</code> <code>max_duration</code> The maximum duration of a run (e.g., 2h, 1d, etc). After it elapses, the run is forced to stop. <code>Union[Literal['off'],str,int,NoneType]</code> <code>None</code> <code>max_price</code> The maximum price per hour, in dollars <code>Optional[float]</code> <code>None</code> <code>default</code> If set to true, <code>dstack run</code> will use this profile by default. <code>bool</code> <code>False</code>"},{"location":"docs/reference/profiles.yml/#profileresources","title":"ProfileResources","text":"Property Description Type Default value <code>cpu</code> The minimum number of CPUs <code>Optional[int]</code> <code>2</code> <code>memory</code> The minimum size of RAM memory (e.g., \"16GB\") <code>Union[int,str,NoneType]</code> <code>8GB</code> <code>gpu</code> The minimum number of GPUs or a GPU spec <code>Union[int,ProfileGPU,NoneType]</code> <code>None</code> <code>shm_size</code> The size of shared memory (e.g., \"8GB\"). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this. <code>Union[int,str,NoneType]</code> <code>None</code> <code>disk</code> The minimum size of disk or a disk spec <code>Union[int,str,ProfileDisk,NoneType]</code> <code>size=102400</code>"},{"location":"docs/reference/profiles.yml/#profilegpu","title":"ProfileGPU","text":"Property Description Type Default value <code>name</code> The name of the GPU (e.g., \"A100\" or \"H100\") <code>Optional[str]</code> <code>None</code> <code>count</code> The minimum number of GPUs <code>int</code> <code>1</code> <code>memory</code> The minimum size of a single GPU memory (e.g., \"16GB\") <code>Union[int,str,NoneType]</code> <code>None</code> <code>total_memory</code> The minimum total size of all GPUs memory (e.g., \"32GB\") <code>Union[int,str,NoneType]</code> <code>None</code> <code>compute_capability</code> The minimum compute capability of the GPU (e.g., 7.5) <code>Union[float,str,Tuple,NoneType]</code> <code>None</code>"},{"location":"docs/reference/profiles.yml/#profileretrypolicy","title":"ProfileRetryPolicy","text":"Property Description Type Default value <code>retry</code> Whether to retry the run on failure or not <code>bool</code> <code>False</code> <code>limit</code> The maximum period of retrying the run, e.g., 4h or 1d <code>Union[int,str,NoneType]</code> <code>None</code>"},{"location":"docs/reference/api/python/","title":"API","text":"<p>The Python API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and displaying its logs.</p> <pre><code>import sys\n\nfrom dstack.api import Task, GPU, Client, Resources\n\nclient = Client.from_config()\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",  # If not specified, a random name is assigned \n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n    repo=None, # Specify to mount additional files\n)\n\nrun.attach()\n\ntry:\n    for log in run.logs():\n        sys.stdout.buffer.write(log)\n        sys.stdout.buffer.flush()\nexcept KeyboardInterrupt:\n    run.stop(abort=True)\nfinally:\n    run.detach()\n</code></pre> <p>NOTE:</p> <ol> <li>The <code>configuration</code> argument in the <code>submit</code> method can be either <code>dstack.api.Task</code> or <code>dstack.api.Service</code>. </li> <li>If you create <code>dstack.api.Task</code> or <code>dstack.api.Service</code>, you may specify the <code>image</code> argument. If <code>image</code> isn't    specified, the default image will be used. For a private Docker registry, ensure you also pass the <code>registry_auth</code> argument.</li> <li>The <code>repo</code> argument in the <code>submit</code> method allows the mounting of a local folder, a remote repo, or a    programmatically created repo. In this case, the <code>commands</code> argument can refer to the files within this repo.</li> <li>The <code>attach</code> method waits for the run to start and, for <code>dstack.api.Task</code> sets up an SSH tunnel and forwards configured <code>ports</code> to <code>localhost</code>.</li> </ol>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>runs</code> <code>RunCollection</code> <p>Operations with runs.</p> <code>repos</code> <code>RepoCollection</code> <p>Operations with repositories.</p> <code>backends</code> <code>BackendCollection</code> <p>Operations with backends.</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using the default configuration from <code>~/.dstack/config.yml</code> if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>The dstack server URL (e.g. <code>http://localhost:3000/</code> or <code>https://cloud.dstack.ai</code>)</p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>The dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for SSH tunneling</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>A client instance</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.RunCollection</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>Union[Task, Service]</code> <p>A run configuration.</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>The path to the configuration file, relative to the root directory of the repo.</p> <code>None</code> <code>repo</code> <code>Union[LocalRepo, RemoteRepo, VirtualRepo]</code> <p>A repo to mount to the run.</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>A list of allowed backend for provisioning.</p> <code>None</code> <code>resources</code> <code>Resources</code> <p>The minimal required resources for provisioning.</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>A spot policy for provisioning.</p> <code>None</code> <code>retry_policy</code> <code>RetryPolicy</code> <p>A retry policy.</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>The max instance running duration in seconds.</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>The max instance price in dollars per hour for provisioning.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>A working directory relative to the repo root directory</p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>A desired name of the run. Must be unique in the project. If not specified, a random name is assigned.</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>Whether local ports should be reserved in advance.</p> <code>True</code> <p>Returns:</p> Type Description <code>Run</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.RepoCollection</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Initializes the repo and configures its credentials in the project. Must be invoked before mounting the repo to a run.</p> <p>Example:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\",\n)\nclient.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Once the repo is initialized, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The repo to initialize.</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for accessing the remote repo.</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>The GitHub OAuth token to access the remote repo.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>ports</code> <code>List[PortMapping]</code> <p>Port numbers/mapping to expose</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p>"},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>port</code> <code>PortMapping</code> <p>The port, that application listens to or the mapping</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p>"},{"location":"docs/reference/api/python/##dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>JobStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/##dstack.api.Resources","title":"<code>dstack.api.Resources</code>","text":"<p>The minimum resources requirements for the run.</p> <p>Attributes:</p> Name Type Description <code>cpu</code> <code>Optional[int]</code> <p>The minimum number of CPUs</p> <code>memory</code> <code>Optional[str]</code> <p>The minimum size of RAM memory (e.g., <code>\"16GB\"</code>)</p> <code>gpu</code> <code>Optional[GPU]</code> <p>The GPU spec</p> <code>shm_size</code> <code>Optional[str]</code> <p>The size of shared memory (e.g., <code>\"8GB\"</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.</p> <code>disk</code> <code>Optional[Disk]</code> <p>The disk spec</p>"},{"location":"docs/reference/api/python/##dstack.api.GPU","title":"<code>dstack.api.GPU</code>","text":"<p>The GPU spec</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <p>The name of the GPU (e.g., <code>\"A100\"</code> or <code>\"H100\"</code>)</p> <code>count</code> <code>int</code> <p>The minimum number of GPUs</p> <code>memory</code> <code>Optional[str]</code> <p>The minimum size of a single GPU memory (e.g., <code>\"16GB\"</code>)</p> <code>total_memory</code> <code>Optional[str]</code> <p>The minimum total size of all GPUs memory (e.g., <code>\"32GB\"</code>)</p> <code>compute_capability</code> <code>float</code> <p>The minimum compute capability of the GPU (e.g., <code>7.5</code>)</p>"},{"location":"docs/reference/api/python/##dstack.api.Disk","title":"<code>dstack.api.Disk</code>","text":"<p>The disk spec</p> <p>Attributes:</p> Name Type Description <code>size</code> <code>str</code> <p>The minimum size of the disk (e.g., <code>\"100GB\"</code>)</p>"},{"location":"docs/reference/api/python/##dstack.api.LocalRepo","title":"<code>dstack.api.LocalRepo</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Example:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=LocalRepo.from_dir(\".\"), # Mount the current folder to the run\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.LocalRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>LocalRepo</code> <p>A local repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.RemoteRepo","title":"<code>dstack.api.RemoteRepo</code>","text":"<p>Creates an instance of a remote Git repo for mounting to a submitted run.</p> <p>Using a locally checked-out remote Git repo:</p> <pre><code>repo=RemoteRepo.from_dir(repo_dir=\".\")\n</code></pre> <p>Using a remote Git repo by a URL:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\"\n)\n</code></pre> <p>Initialize the repo before mounting it.</p> <pre><code>client.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Finally, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_url","title":"<code>from_url(repo_url, repo_branch=None, repo_hash=None)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>The URL of a remote Git repo</p> required <code>repo_branch</code> <code>Optional[str]</code> <p>The name of the remote branch. Must be specified if <code>hash</code> is not specified.</p> <code>None</code> <code>repo_hash</code> <code>Optional[str]</code> <p>The hash of the revision. Must be specified if <code>branch</code> is not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.VirtualRepo","title":"<code>dstack.api.VirtualRepo</code>","text":"<p>Allows mounting a repo created programmatically.</p> <p>Example:</p> <pre><code>virtual_repo = VirtualRepo(repo_id=\"some-unique-repo-id\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"requirements.txt\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"train.py\")\n\nrun = client.runs.submit(\n    configuration=...,\n    repo=virtual_repo,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>repo_id</code> <p>A unique identifier of the repo</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file","title":"<code>add_file(path, content)</code>","text":"<p>Adds a given file to the repo.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path inside the repo to add the file.</p> <code>content</code> <code>bytes</code> <p>The contents of the file.</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file_from_package","title":"<code>add_file_from_package(package, path)</code>","text":"<p>Includes a file from a given package to the repo.</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>Union[ModuleType, str]</code> <p>A package to include the file from.</p> <code>path</code> <code>str</code> <p>The path to the file to include to the repo. Must be relative to the package directory.</p>"},{"location":"docs/reference/api/python/##dstack.api.RegistryAuth","title":"<code>dstack.api.RegistryAuth</code>","text":"<p>Credentials for pulling a private Docker image.</p> <p>Attributes:</p> Name Type Description <code>username</code> <code>str</code> <p>The username</p> <code>password</code> <code>str</code> <p>The password or access token</p>"},{"location":"docs/reference/api/python/#dstack.api.BackendType","title":"<code>dstack.api.BackendType</code>","text":"<p>Attributes:</p> Name Type Description <code>AWS</code> <code>BackendType</code> <p>Amazon Web Services</p> <code>AZURE</code> <code>BackendType</code> <p>Microsoft Azure</p> <code>DSTACK</code> <code>BackendType</code> <p>dstack Cloud</p> <code>GCP</code> <code>BackendType</code> <p>Google Cloud Platform</p> <code>DATACRUNCH</code> <code>BackendType</code> <p>DataCrunch</p> <code>LAMBDA</code> <code>BackendType</code> <p>Lambda Cloud</p> <code>TENSORDOCK</code> <code>BackendType</code> <p>TensorDock Marketplace</p> <code>VASTAI</code> <code>BackendType</code> <p>Vast.ai Marketplace</p>"},{"location":"docs/reference/api/python/_artifacts_archived/","title":"Python API","text":""},{"location":"docs/reference/api/python/_artifacts_archived/#artifacts","title":"Artifacts","text":"<p>This API allows you to save and load data and models from the artifact storage.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#dstackartifactsupload","title":"dstack.artifacts.upload","text":"<pre><code>def upload(local_path: str, artifact_path: Optional[str] = None,\n           tag: Optional[str] = None)\n</code></pre> <p>Uploads the files located under the <code>local_path</code> folder as artifacts.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#argument-reference","title":"Argument reference","text":"<ul> <li><code>tag</code> \u2013 (Optional) If <code>tag</code> is not specified, artifacts are automatically attached to the current run.     If <code>tag is specified</code>, artifacts are attached to the given tag name.</li> <li><code>local_path</code> \u2013 (Optional) The path to a local folder with the files to upload.</li> <li><code>artifact_path</code> \u2013 (Optional) The path under which the files will be stored.</li> </ul>"},{"location":"docs/reference/api/python/_artifacts_archived/#usage-example","title":"Usage example:","text":"<pre><code>from dstack import artifacts\n\n# Uploads files under \"datasets/dataset1\" as artifacts and attach them to the current run \nartifacts.upload(\"datasets/dataset1\")\n\n# Uploads files under \"datasets/dataset1\" as artifacts and creates a tag \"my_tag\"\nartifacts.upload(\"datasets/dataset1\", tag=\"my_tag\")\n</code></pre>"},{"location":"docs/reference/api/python/_artifacts_archived/#dstackartifactsdownload","title":"dstack.artifacts.download","text":"<pre><code>def download(run: Optional[str] = None, tag: Optional[str] = None,\n             artifact_path: Optional[str] = None, local_path: Optional[str] = None)\n</code></pre> <p>Downloads artifact files of a given run or a tag.</p>"},{"location":"docs/reference/api/python/_artifacts_archived/#argument-reference_1","title":"Argument reference","text":"<p>One of the following arguments is required:</p> <ul> <li><code>run</code> \u2013 The run to download the artifacts from</li> <li><code>tag</code> \u2013 The tag to download the artifacts from</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>artifact_path</code> \u2013 (Optional) The path to the artifact files to download.      If not specified, all artifact files are downloaded.</li> <li><code>local_path</code> \u2013 (Optional) The local path to save the files to.     If not specified, files are downloaded to the current directory.</li> </ul>"},{"location":"docs/reference/api/python/_artifacts_archived/#usage-example_1","title":"Usage example","text":"<pre><code>from dstack import artifacts\n\n# Downloads all artifact files of a run to the current directory\nartifacts.download(run=\"sharp-shrimp-1\")\n\n# Downloads all artifact files of the \"my_tag\" tag and saves them to \"my_model\"\nartifacts.download(tag=\"my_tag\", local_path=\"my_model\")\n</code></pre> <p>NOTE:</p> <p>Currently, the Python API can only be used from dev environments and tasks.</p>"},{"location":"docs/reference/api/python/server/","title":"Low-level API client","text":""},{"location":"docs/reference/api/python/server/#dstack.api.server.APIClient","title":"<code>dstack.api.server.APIClient</code>","text":"<p>Low-level API client for interacting with dstack server. Implements all API endpoints</p> <p>Attributes:</p> Name Type Description <code>users</code> <code>UsersAPIClient</code> <p>operations with users</p> <code>projects</code> <code>ProjectsAPIClient</code> <p>operations with projects</p> <code>backends</code> <code>BackendsAPIClient</code> <p>operations with backends</p> <code>runs</code> <code>RunsAPIClient</code> <p>operations with runs</p> <code>logs</code> <code>LogsAPIClient</code> <p>operations with logs</p> <code>gateways</code> <code>GatewaysAPIClient</code> <p>operations with gateways</p>"},{"location":"docs/reference/api/python/server/#dstack.api.server.APIClient.__init__","title":"<code>__init__(base_url, token)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>API endpoints prefix, e.g. <code>http://127.0.0.1:3000/</code></p> required <code>token</code> <code>str</code> <p>API token</p> required"},{"location":"docs/reference/backends/aws/","title":"AWS","text":"<p>The <code>AWS</code> backend type allows provisioning infrastructure and storing artifacts in an AWS account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/aws/#create-an-s3-bucket","title":"Create an S3 bucket","text":"<p>First, you need to create an S3 bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/aws/#create-an-iam-user","title":"Create an IAM user","text":"<p>The next step is to create an IAM user and  grant this user permissions to perform actions on the <code>s3</code>, <code>logs</code>, <code>secretsmanager</code>, <code>ec2</code>, and <code>iam</code> services.</p> IAM policy template <p>If you'd like to limit the permissions to the most narrow scope, feel free to use the IAM policy template below.</p> <p>Replace <code>{bucket_name}</code> and <code>{bucket_name_under_score}</code> variables in the template below with the values that correspond to your S3 bucket.</p> <p>For <code>{bucket_name}</code>, use the name of the S3 bucket.  For <code>{bucket_name_under_score}</code>, use the same but with dash characters replaced to underscores  (e.g. if <code>{bucket_name}</code> is <code>my-awesome-project</code>, then  <code>{bucket_name_under_score}</code>  must be <code>my_awesome_project</code>.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\",\n        \"s3:GetLifecycleConfiguration\",\n        \"s3:PutLifecycleConfiguration\",\n        \"s3:PutObjectTagging\",\n        \"s3:GetObjectTagging\",\n        \"s3:DeleteObjectTagging\",\n        \"s3:GetBucketAcl\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::{bucket_name}\",\n        \"arn:aws:s3:::{bucket_name}/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:DescribeLogGroups\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:*:*:log-group:*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:FilterLogEvents\",\n        \"logs:TagLogGroup\",\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:*:*:log-group:/dstack/jobs/{bucket_name}*:*\",\n        \"arn:aws:logs:*:*:log-group:/dstack/runners/{bucket_name}*:*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:UpdateSecret\",\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:CreateSecret\",\n        \"secretsmanager:PutSecretValue\",\n        \"secretsmanager:PutResourcePolicy\",\n        \"secretsmanager:TagResource\",\n        \"secretsmanager:DeleteSecret\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/credentials/*\",\n        \"arn:aws:secretsmanager:*:*:secret:/dstack/{bucket_name}/secrets/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeSpotPriceHistory\",\n        \"ec2:RunInstances\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:AuthorizeSecurityGroupEgress\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CancelSpotInstanceRequests\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:ResourceTag/dstack_bucket\": \"{bucket_name}\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetRole\",\n        \"iam:CreateRole\",\n        \"iam:AttachRolePolicy\",\n        \"iam:TagRole\"\n      ],\n      \"Resource\": \"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:CreatePolicy\",\n        \"iam:TagPolicy\"\n      ],\n      \"Resource\": \"arn:aws:iam::*:policy/dstack_policy_{bucket_name_under_score}*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetInstanceProfile\",\n        \"iam:CreateInstanceProfile\",\n        \"iam:AddRoleToInstanceProfile\",\n        \"iam:TagInstanceProfile\",\n        \"iam:PassRole\"\n      ],\n      \"Resource\": [\n        \"arn:aws:iam::*:instance-profile/dstack_role_{bucket_name_under_score}*\",\n        \"arn:aws:iam::*:role/dstack_role_{bucket_name_under_score}*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/reference/backends/aws/#set-up-aws-credentials","title":"Set up AWS credentials","text":"<p><code>dstack</code> support two methods to authenticate with AWS: Default credentials and Access key.</p>"},{"location":"docs/reference/backends/aws/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up AWS credentials set up on your machine (e.g. credentials stored as AWS profiles or environment variables). You can use default credentials if you don't want to enter and store AWS credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/aws/#access-key","title":"Access key","text":"<p><code>dstack</code> also support authentication using an access key. To create an access key, follow this guide. Once the access key is created, make sure to download the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>.</p>"},{"location":"docs/reference/backends/aws/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>AWS</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/aws/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Region</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The S3 bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Access key ID</code> - (Optional) The Access key ID to authenticate <code>dstack</code> </li> <li><code>Secret access key</code> - (Optional) The Secret access key to authenticate <code>dstack</code></li> <li><code>Subnet</code> - (Optional) The EC2 subnet is required to provision infrastructure using a non-default VPC and subnet. If   not specified, dstack will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/azure/","title":"Azure","text":"<p>The <code>Azure</code> backend type allows provisioning infrastructure and storing artifacts in an Azure account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/azure/#create-a-resource-group","title":"Create a resource group","text":"<p>First, create a new Azure resource group. All resource created by <code>dstack</code> will belong to this group.</p>"},{"location":"docs/reference/backends/azure/#create-a-storage-account","title":"Create a storage account","text":"<p>Next, create an Azure storage account in the newly created resource group. <code>dstack</code> will use this storage account to store metadata and artifacts.</p> <p>NOTE:</p> <p>Make sure that the storage account is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/azure/#set-up-azure-credentials","title":"Set up Azure credentials","text":"<p><code>dstack</code> support two methods to authenticate with Azure: Default credentials and Client secret.</p>"},{"location":"docs/reference/backends/azure/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up Azure default credentials set up on your machine. You can use default credentials if you don't want to enter and store Azure credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/azure/#client-secret","title":"Client secret","text":"<p><code>dstack</code> also supports an Azure Active Directory app credentials to authenticate with your Azure account. If the app is <code>Owner</code> of the subscription, <code>dstack</code> will automatically set up all the resources required to run workflows. It will also create a separate managed identity with fine-grained permissions to authenticate with your Azure account when running workflows. </p> <p>To create new application credentials using the Azure CLI, run:</p> <pre><code>az ad sp create-for-rbac --name dstack-app --role Owner --scopes /subscriptions/$SUBSCRIPTION_ID --query \"{ client_id: appId, client_secret: password, tenant_id: tenant }\"\n</code></pre>"},{"location":"docs/reference/backends/azure/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>Azure</code> in the <code>Type</code> field.</p> <p>It may take up to a minute to set up Azure resource after saving the backend settings.</p>"},{"location":"docs/reference/backends/azure/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Tenant ID</code> - (Required) The Azure Tenant ID</li> <li><code>Subscription ID</code> - (Required) The Azure Subscription ID</li> <li><code>Location</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Storage account</code> - (Required) The Storage account to store state and artifacts (must be in the same region)</li> </ul> <p>The following fields are optional:</p> <ul> <li><code>Client ID</code> - (Optional) The Client ID to authenticate <code>dstack</code></li> <li><code>Client Secret</code> - (Optional) The Client secret to authenticate <code>dstack</code></li> </ul>"},{"location":"docs/reference/backends/gcp/","title":"GCP","text":"<p>The GCP backend type allows provisioning infrastructure and storing artifacts in a GCP account.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/gcp/#enable-apis","title":"Enable APIs","text":"<p>First, ensure that the required APIs are enabled in your GCP project.</p> Required APIs <p>Here's the list of APIs that have to be enabled for the project.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"docs/reference/backends/gcp/#create-a-storage-bucket","title":"Create a storage bucket","text":"<p>Once the APIs are enabled, proceed and create a storage bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p> <p>NOTE:</p> <p>Make sure that the bucket is created in the same region where you plan to provision infrastructure.</p>"},{"location":"docs/reference/backends/gcp/#set-up-gcp-credentials","title":"Set up GCP credentials","text":"<p><code>dstack</code> support two methods to authenticate with GCP: Default credentials and Service account key.</p>"},{"location":"docs/reference/backends/gcp/#default-credentials","title":"Default credentials","text":"<p><code>dstack</code> can automatically pick up GCP default credentials set up on your machine. You can use default credentials if you don't want to enter and store GCP credentials in <code>dstack</code>.</p>"},{"location":"docs/reference/backends/gcp/#service-account-key","title":"Service account key","text":"<p><code>dstack</code> also support authentication using a service account key. Follow this guide to create a service account and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p> <p>Once the service account is set up, create a key for it and download the corresponding JSON file.</p>"},{"location":"docs/reference/backends/gcp/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>GCP</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/gcp/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>Location</code> - (Required) The location where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Region</code> - (Required) The region where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Zone</code> - (Required) The zone where <code>dstack</code> will provision infrastructure and store state and artifacts</li> <li><code>Bucket</code> - (Required) The storage bucket to store state and artifacts (must be in the same region)</li> </ul> <p>The following arguments are optional:</p> <ul> <li><code>Service account</code> - (Optional) The JSON file of the service account key to authenticate <code>dstack</code> </li> <li><code>Subnet</code> - (Optional) The VPC subnet where <code>dstack</code> will provision infrastructure. If   not specified, <code>dstack</code> will use the default VPC and subnet.</li> </ul>"},{"location":"docs/reference/backends/lambda/","title":"Lambda Cloud","text":"<p>The <code>Lambda</code> backend allows provisioning infrastructure in Lambda Cloud while storing  artifacts in an S3 bucket.</p> <p>Follow the step-by-step guide below to configure this backend in your project.</p>"},{"location":"docs/reference/backends/lambda/#set-up-storage","title":"Set up storage","text":"<p>As Lambda Cloud doesn't have its own object storage, <code>dstack</code> requires you to specify an S3 bucket,  along with AWS credentials, for storing state and artifacts.</p>"},{"location":"docs/reference/backends/lambda/#create-an-s3-bucket","title":"Create an S3 bucket","text":"<p>First, you need to create an S3 bucket. <code>dstack</code> will use this bucket to store state and artifacts.</p>"},{"location":"docs/reference/backends/lambda/#create-an-iam-user","title":"Create an IAM user","text":"<p>The next step is to create an IAM user and grant this user permissions to perform actions on the <code>s3</code> service.</p> Logs and secrets <p>If you want <code>dstack</code> to also store logs and secrets, you can optionally grant permissions  to the <code>logs</code> and <code>secretsmanager</code> services.</p>"},{"location":"docs/reference/backends/lambda/#create-an-access-key","title":"Create an access key","text":"<p>To create an access key, follow this guide. Once the access key is created, make sure to download the <code>.csv</code> file containing your IAM user's <code>Access key ID</code> and <code>Secret access key</code>.</p>"},{"location":"docs/reference/backends/lambda/#set-up-api-key","title":"Set up API key","text":"<p>Then, you'll need a Lambda Cloud API key. Log into your Lambda Cloud account, click <code>API keys</code> in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p></p>"},{"location":"docs/reference/backends/lambda/#configure-the-backend","title":"Configure the backend","text":"<p>Now, log in to the UI, open the project's settings, click <code>Edit</code>, then click <code>Add backend</code>, and select <code>Lambda</code> in the <code>Type</code> field.</p>"},{"location":"docs/reference/backends/lambda/#fields-reference","title":"Fields reference","text":"<p>The following fields are required:</p> <ul> <li><code>API key</code> - (Required) The [API key] to authenticate <code>dstack</code> with Lambda Cloud</li> <li><code>Regions</code> - (Required) The list of regions where <code>dstack</code> may provision infrastructure. It is recommended to select as many regions as possible to maximize availability.</li> <li><code>Storage</code> - (Required) The storage provider that <code>dstack</code> will use to store the state and artifacts. Currently, only <code>AWS</code> is supported.</li> <li><code>Access key ID</code> - (Required) The Access key ID to authenticate <code>dstack</code> with AWS</li> <li><code>Secret access key</code> - (Required) The Secret access key to authenticate <code>dstack</code> with AWS</li> <li><code>Bucket</code> - (Required) The S3 bucket to store state and artifacts</li> </ul>"},{"location":"docs/reference/cli/","title":"CLI","text":""},{"location":"docs/reference/cli/#commands","title":"Commands","text":""},{"location":"docs/reference/cli/#dstack-server","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p> <pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL] [--default]\n                     [--no-default] [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to WARNING.\n  --default             Update the default project configuration\n  --no-default          Do not update the default project configuration\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/#dstack-init","title":"dstack init","text":"<p>This command initializes the current folder as a repo.</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access the remote repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use git\n</code></pre> Git credentials <p>If the current folder is a Git repo, the command authorizes <code>dstack</code> to access it. By default, the command uses the default Git credentials configured for the repo.  You can override these credentials via <code>--token</code> (OAuth token) or <code>--git-identity</code>.</p> Custom SSH key <p>By default, this command generates an SSH key that will be used for port forwarding and SSH access to running workloads.  You can override this key via <code>--ssh-identity</code>.</p>"},{"location":"docs/reference/cli/#dstack-run","title":"dstack run","text":"<p>This command runs a given configuration.</p> <pre><code>$ dstack run . --help\nUsage: dstack run [--project NAME] [-h [TYPE]] [-f FILE] [-n RUN_NAME] [-d]\n                  [-y] [--max-offers MAX_OFFERS] [--profile NAME] [--gpu SPEC]\n                  [--disk SIZE] [--max-price PRICE] [--max-duration DURATION]\n                  [-b NAME]\n                  [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                  [--retry | --no-retry | --retry-limit DURATION]\n                  [-e KEY=VALUE]\n                  working_dir\n\nPositional Arguments:\n  working_dir\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit. TYPE is one of task,\n                        dev-environment, service\n  -f, --file FILE       The path to the run configuration file. Defaults to\n                        WORKING_DIR/.dstack.yml\n  -n, --name RUN_NAME   The name of the run. If not specified, a random name\n                        is assigned\n  -d, --detach          Do not poll logs and run status\n  -y, --yes             Do not ask for plan confirmation\n  --max-offers MAX_OFFERS\n                        Number of offers to show in the run plan\n  -e, --env KEY=VALUE   Environment variables\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --gpu SPEC            Request a GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk SIZE           Request the minimum size of disk for the run. Example\n                        --disk 100GB.\n  --max-price PRICE     The maximum price per hour, in dollars\n  --max-duration DURATION\n  -b, --backend NAME    The backends that will be tried for provisioning\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-limit DURATION\n</code></pre> .gitignore <p>When running anything via CLI, <code>dstack</code> uses the exact version of code from your project directory.</p> <pre><code>If there are large files, consider creating a `.gitignore` file to exclude them for better performance.\n</code></pre>"},{"location":"docs/reference/cli/#dstack-ps","title":"dstack ps","text":"<p>This command shows the status of runs.</p> <pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-stop","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p> <pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/#dstack-logs","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p> <pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose\n  -a, --attach          Set up an SSH tunnel, and print logs as they follow.\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n</code></pre>"},{"location":"docs/reference/cli/#dstack-config","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p> <pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN]\n                     [--default] [--remove] [--no-default]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  --default          Set the project as default. It will be used when\n                     --project is omitted in commands.\n  --remove           Delete project configuration\n  --no-default       Do not prompt to set the project as default\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway","title":"dstack gateway","text":"<p>A gateway is required for running services.</p>"},{"location":"docs/reference/cli/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p> <pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-create","title":"dstack gateway create","text":"<p>The <code>dstack gateway create</code> command creates a new gateway instance in the project.</p> <pre><code>$ dstack gateway create --help\nUsage: dstack gateway create [-h] --backend {aws,gcp,azure} --region REGION\n                             [--set-default] [--name NAME] --domain DOMAIN\n\nOptions:\n  -h, --help            show this help message and exit\n  --backend {aws,gcp,azure}\n  --region REGION\n  --set-default         Set as default gateway for the project\n  --name NAME           Set a custom name for the gateway\n  --domain DOMAIN       Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p> <pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-update","title":"dstack gateway update","text":"<p>The <code>dstack gateway update</code> command updates the specified gateway.</p> <pre><code>$ dstack gateway update --help\nUsage: dstack gateway update [-h] [--set-default] [--domain DOMAIN] name\n\nPositional Arguments:\n  name             The name of the gateway\n\nOptions:\n  -h, --help       show this help message and exit\n  --set-default    Set it the default gateway for the project\n  --domain DOMAIN  Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#environment-variables","title":"Environment variables","text":"Name Description Default <code>DSTACK_CLI_LOG_LEVEL</code> Configures CLI logging level <code>CRITICAL</code> <code>DSTACK_PROFILE</code> Has the same effect as <code>--profile</code> <code>None</code> <code>DSTACK_PROJECT</code> Has the same effect as <code>--project</code> <code>None</code> <code>DSTACK_DEFAULT_CREDS_DISABLED</code> Disables default credentials detection if set <code>None</code> <code>DSTACK_LOCAL_BACKEND_ENABLED</code> Enables local backend for debug if set <code>None</code> <code>DSTACK_RUNNER_VERSION</code> Sets exact runner version for debug <code>latest</code> <code>DSTACK_SERVER_ADMIN_TOKEN</code> Has the same effect as <code>--token</code> <code>None</code> <code>DSTACK_SERVER_DIR</code> Sets path to store data and server configs <code>~/.dstack/server</code> <code>DSTACK_SERVER_HOST</code> Has the same effect as <code>--host</code> <code>127.0.0.1</code> <code>DSTACK_SERVER_LOG_LEVEL</code> Has the same effect as <code>--log-level</code> <code>WARNING</code> <code>DSTACK_SERVER_PORT</code> Has the same effect as <code>--port</code> <code>3000</code> <code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> Sets root logger log level <code>ERROR</code> <code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> Sets uvicorn logger log level <code>ERROR</code>"},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":""},{"location":"docs/reference/server/config.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/server/config.yml/#serverconfig","title":"ServerConfig","text":"Property Description Type Default value <code>projects</code> <code>List[ProjectConfig]</code> required"},{"location":"docs/reference/server/config.yml/#projectconfig","title":"ProjectConfig","text":"Property Description Type Default value <code>name</code> <code>str</code> required <code>backends</code> <code>Union[AWSConfigInfoWithCreds, AzureConfigInfoWithCreds, GCPConfigInfoWithCreds, LambdaConfigInfoWithCreds, TensorDockConfigInfoWithCreds, VastAIConfigInfoWithCreds]</code> required"},{"location":"docs/reference/server/config.yml/#awsconfig","title":"AWSConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['aws']</code> <code>aws</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[AWSAccessKeyCreds,AWSDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#awsdefaultcreds","title":"AWSDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#awsaccesskeycreds","title":"AWSAccessKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['access_key']</code> <code>access_key</code> <code>access_key</code> <code>str</code> required <code>secret_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#azureconfig","title":"AzureConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['azure']</code> <code>azure</code> <code>tenant_id</code> <code>str</code> required <code>subscription_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[AzureClientCreds,AzureDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#azuredefaultcreds","title":"AzureDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#azureclientcreds","title":"AzureClientCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['client']</code> <code>client</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required <code>tenant_id</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#datacrunchconfig","title":"DataCrunchConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['datacrunch']</code> <code>datacrunch</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>DataCrunchAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#datacrunchapikeycreds","title":"DataCrunchAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#gcpconfig","title":"GCPConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['gcp']</code> <code>gcp</code> <code>project_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[GCPServiceAccountCreds,GCPDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#gcpdefaultcreds","title":"GCPDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#gcpserviceaccountcreds","title":"GCPServiceAccountCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['service_account']</code> <code>service_account</code> <code>filename</code> <code>str</code> required <code>data</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#lambdaconfig","title":"LambdaConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['lambda']</code> <code>lambda</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>LambdaAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#lambdaapikeycreds","title":"LambdaAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#tensordockconfig","title":"TensorDockConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['tensordock']</code> <code>tensordock</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>TensorDockAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#tensordockapikeycreds","title":"TensorDockAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required <code>api_token</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#vastaiconfig","title":"VastAIConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['vastai']</code> <code>vastai</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>VastAIAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#vastaiapikeycreds","title":"VastAIAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"examples/deploy-python/","title":"Serving LLMs using Python API","text":"<p>The Python API of <code>dstack</code> can be used to run tasks and services programmatically.</p> <p>To demonstrate how it works, we've created a simple Streamlit app that uses <code>dstack</code>'s API to deploy a quantized  version of Llama 2 to your cloud with a click of a button.</p> <p></p>"},{"location":"examples/deploy-python/#prerequisites","title":"Prerequisites","text":"<p>Before you can use <code>dstack</code> Python API, ensure you have set up the server.</p>"},{"location":"examples/deploy-python/#how-does-it-work","title":"How does it work?","text":""},{"location":"examples/deploy-python/#create-a-client","title":"Create a client","text":"<p>If you're familiar with Docker's Python SDK, you'll find <code>dstack</code>'s Python API  quite similar, except that it runs your workload in the cloud.</p> <p>To get started, create an instance of <code>dstack.Client</code> and use its methods to submit and manage runs.</p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError as e:\n    print(e)\n</code></pre>"},{"location":"examples/deploy-python/#create-a-task","title":"Create a task","text":"<p>NOTE:</p> <p>With <code>dstack.Client</code>, you can run tasks and services. Running a task allows you to programmatically access its ports and forward traffic to your local machine. For example, if you run an LLM as a task, you can access it on <code>localhost</code>. Services on the other hand allow deploying applications as public endpoints.</p> <p>In our example, we'll deploy an LLM as a task. To do this, we'll create a <code>dstack.Task</code> instance that configures how the LLM should be run.</p> <pre><code>from dstack.api import Task\n\nconfiguration = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": model_id},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"8080:80\"],  # LLM runs on port 80, forwarded to localhost:8080\n)\n</code></pre>"},{"location":"examples/deploy-python/#create-resources","title":"Create resources","text":"<p>Then, we'll need to specify the resources our LLM will require. To do this, we'll create a <code>dstack.Resources</code> instance:</p> <pre><code>from dstack.api import Resources, GPU\n\nif model_id == \"TheBloke/Llama-2-13B-chat-GPTQ\":\n    gpu_memory = \"20GB\"\nelif model_id == \"TheBloke/Llama-2-70B-chat-GPTQ\":\n    gpu_memory = \"40GB\"\n\nresources = Resources(gpu=GPU(memory=gpu_memory))\n</code></pre>"},{"location":"examples/deploy-python/#submit-the-run","title":"Submit the run","text":"<p>To deploy the LLM, we submit the task using <code>runs.submit()</code> in <code>dstack.Client</code>.</p> <pre><code>run_name = \"deploy-python\"\n\nrun = client.runs.submit(configuration=configuration, run_name=run_name, resources=resources)\n</code></pre>"},{"location":"examples/deploy-python/#attach-to-the-run","title":"Attach to the run","text":"<p>Then, we use the <code>attach()</code> method on <code>dstack.Run</code>. This method waits for the task to start,  and forwards the configured ports to <code>localhost</code>.</p> <pre><code>run.attach()\n</code></pre>"},{"location":"examples/deploy-python/#wait-for-the-endpoint-to-start","title":"Wait for the endpoint to start","text":"<p>Finally, we wait until <code>http://localhost:8080/health</code> returns <code>200</code>, which indicates that the LLM is deployed and ready to handle requests.</p> <pre><code>import time\nimport requests\n\nwhile True:\n    time.sleep(0.5)\n    try:\n        r = requests.get(\"http://localhost:8080/health\")\n        if r.status_code == 200:\n            break\n    except Exception:\n        pass\n</code></pre>"},{"location":"examples/deploy-python/#stop-the-run","title":"Stop the run","text":"<p>To undeploy the model, we can use the <code>stop()</code> method on <code>dstack.Run</code>.</p> <pre><code>run.stop()\n</code></pre>"},{"location":"examples/deploy-python/#retrieve-the-status-of-a-run","title":"Retrieve the status of a run","text":"<p>Note: If you'd like to retrieve the <code>dstack.Run</code> instance by the name of the run, you can use the <code>runs.get()</code> method on <code>dstack.Client</code>.</p> <pre><code>run = client.runs.get(run_name)\n</code></pre> <p>The <code>status</code> property on <code>dstack.Run</code> provides the status of the run.</p> <pre><code>if run:\n    print(run.status)\n</code></pre> <p>To get the latest state of the run, you can use the <code>run.refresh()</code> method:</p> <pre><code>run.refresh()\n</code></pre>"},{"location":"examples/deploy-python/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p> <pre><code>git clone https://github.com/dstackai/dstack-examples\ncd dstack-examples\n</code></pre> <p>Once the repository is cloned, feel free to install the requirements and run the app:</p> <pre><code>pip install -r deploy-python/requirements.txt\nstreamlit run deploy-python/app.py\n</code></pre>"},{"location":"examples/llama-index/","title":"Llama Index","text":"<p>RAG, or retrieval-augmented generation, empowers LLMs by providing them with access to your data.</p> <p>Here's an example of how to apply this technique using the Llama Index framework  and Weaviate vector database.</p>"},{"location":"examples/llama-index/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Llama Index loads data from local files, structures it into chunks, and ingests it into Weaviate (an open-source vector database).    We set up Llama Index to use local embeddings through the SentenceTransformers library.</li> <li><code>dstack</code> allows us to deploy LLMs to any cloud provider, e.g. via Services using TGI or vLLM.</li> <li>Llama Index allows us to prompt the LLM automatically incorporating the context from Weaviate. </li> </ol>"},{"location":"examples/llama-index/#requirements","title":"Requirements","text":"<p>Here's the list of Python libraries that we'll use:</p> <pre><code>weaviate-client\nllama-index\nsentence-transformers\ntext_generation\n</code></pre>"},{"location":"examples/llama-index/#load-data-to-weaviate","title":"Load data to Weaviate","text":"<p>The first thing we do is load the data from local files and ingest it into Weaviate.</p> <p>NOTE:</p> <p>To use Weaviate, you need to either install  it on-premises or sign up for their managed service.</p> <p>Since we're going to load data into or from Weaviate, we'll need a <code>weaviate.Client</code>:</p> <pre><code>import os\n\nimport weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_TOKEN\"))\n\nclient = weaviate.Client(url=os.getenv(\"WEAVIATE_URL\"), auth_client_secret=auth_config)\n\nclient.schema.delete_class(\"DstackExample\")\n</code></pre> <p>Next, prepare the Llama Index classes: <code>llama_index.ServiceContext</code> (for indexing and querying) and <code>llama_index.StorageContext</code> (for loading and storing). </p> <p>Embeddings</p> <p>Note that we're using <code>langchain.embeddings.huggingface.HuggingFaceEmbeddings</code> for local embeddings instead of OpenAI.</p> <pre><code>from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index import (\n    LangchainEmbedding,\n    ServiceContext,\n    StorageContext,\n)\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n</code></pre> <p>Once the utility classes are configured, we can load the data from local files and pass it to <code>llama_index.VectorStoreIndex</code>. Using its <code>from_documents</code> method will then store the data in the vector database.</p> <pre><code>from pathlib import Path\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\n\ndocuments = SimpleDirectoryReader(Path(__file__).parent / \"data\").load_data()\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    service_context=service_context,\n    storage_context=storage_context,\n)\n</code></pre> <p>The data is in the vector database! Now we can proceed with the part where we invoke an LLM using this data as context.</p>"},{"location":"examples/llama-index/#deploy-an-llm","title":"Deploy an LLM","text":"<p>This example assumes we're using an LLM deployed using TGI.</p> <p>Once you deployed the model, make sure to set the <code>TGI_ENDPOINT_URL</code> environment variable  to its URL, e.g. <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (or <code>http://localhost:&lt;port&gt;</code> if it's deployed  as a task). We'll use this environment variable below.</p> <pre><code>$ curl -X POST --location $TGI_ENDPOINT_URL/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n          \"inputs\": \"What is Deep Learning?\",\n          \"parameters\": {\n            \"max_new_tokens\": 20\n          }\n        }'\n</code></pre>"},{"location":"examples/llama-index/#generate-response","title":"Generate response","text":"<p>Once the LLM endpoint is up, we can prompt it through Llama Index to automatically incorporate context from Weaviate.</p> <p>Since we'll invoke the actual LLM, when configuring <code>llama_index.ServiceContext</code>, we must include the LLM configuration.</p> <pre><code>import os\n\nfrom llama_index import (\n    LangchainEmbedding,\n    PromptHelper,\n    ServiceContext,\n    VectorStoreIndex,\n)\n\nfrom langchain import HuggingFaceTextGenInference\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index.llm_predictor import LLMPredictor\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nllm_predictor = LLMPredictor(\n    llm=HuggingFaceTextGenInference(\n        inference_server_url=os.getenv(\"TGI_ENDPOINT_URL\"),\n        max_new_tokens=512,\n        streaming=True,\n    ),\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model,\n    llm_predictor=llm_predictor,\n    prompt_helper=PromptHelper(context_window=1024),\n)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nindex = VectorStoreIndex.from_vector_store(\n    vector_store, service_context=service_context\n)\n</code></pre> <p>Once <code>llama_index.VectorStoreIndex</code> is ready, we can proceed with querying it.</p> <p>Prompt format</p> <p>If we're deploying Llama 2, we have to ensure that the prompt format is correct.</p> <pre><code>from llama_index import (QuestionAnswerPrompt, RefinePrompt)\n\ntext_qa_template = QuestionAnswerPrompt(\n        \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nWe have provided context information below. \n\n{context_str}\n\nGiven this information, please answer the question.\n&lt;&lt;/SYS&gt;&gt;\n\n{query_str} [/INST]\"\"\"\n    )\n\nrefine_template = RefinePrompt(\n    \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nThe original query is as follows: \n\n{query_str}\n\nWe have provided an existing answer:\n\n{existing_answer}\n\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\n\n{context_msg}\n&lt;&lt;/SYS&gt;&gt;\n\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. [/INST]\"\"\"\n)\n\nquery_engine = index.as_query_engine(\n    text_qa_template=text_qa_template,\n    refine_template=refine_template,\n    streaming=True,\n)\n\nresponse = query_engine.query(\"Make a bullet-point timeline of the authors biography?\")\nresponse.print_response_stream()\n</code></pre> <p>That's it! This basic example shows how straightforward it is to use Llama Index and Weaviate with the LLMs deployed using <code>dstack</code>. For more in-depth information, we encourage you to explore the documentation for each tool.</p>"},{"location":"examples/llama-index/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/llama-index/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/llmchat/","title":"LLM as Chatbot","text":"<p>This example is built by Chansung Park. It can run any open-source LLM either as a Gradio chat app or as a Discord bot. With <code>dstack</code>, you can run this Gradio chat app or Discord bot in any cloud with a single command. To try this example with <code>dstack</code>, follow the instructions below.</p>"},{"location":"examples/llmchat/#define-a-profile","title":"Define a profile","text":"<p>Each LLM model requires specific resources. To inform <code>dstack</code> about the required resources, you need to  define a profile via the <code>.dstack/profiles.yaml</code> file within your project.</p> <p>Below is a profile that will provision a cloud instance with <code>24GB</code> of memory and a <code>T4</code> GPU in the <code>gcp</code> project.</p> <pre><code>profiles:\n  - name: t4-serve\n\n    resources:\n      memory: 24GB\n      gpu:\n        name: T4\n\n    spot_policy: auto # (Optional) Use spot instances if available\n\n    default: true\n</code></pre>"},{"location":"examples/llmchat/#run-the-app","title":"Run the app","text":"<p>Here's the configuration that runs the Gradio app:</p> <pre><code>type: task\n\nenv:\n  # (Optional) Specify your Hugging Face token\n  - HUGGING_FACE_HUB_TOKEN=\n  # (Optional) Specify your Serper API Key\n  - LLMCHAT_SERPER_API_KEY=\n\nports:\n  - 6006\n\ncommands:\n  - pip install -r requirements.txt\n  - LLMCHAT_APP_MODE=GRADIO python entry_point.py\n</code></pre> <p>Here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f gradio.dstack.yml\n</code></pre> <p><code>dstack</code> will provision the cloud instance, run the task, and forward the defined ports to your local machine for secure and convenient access.</p> <p></p>"},{"location":"examples/llmchat/#run-the-discord-bot","title":"Run the Discord bot","text":"<p>Here's the configuration that runs the Gradio app:</p> <pre><code>type: task\n\nenv:\n  # (Required) Specify your Discord bot token.\n  - DISCORD_BOT_TOKEN=\n  # (Required) Specify the name of the model. See `README.md`` for supported models.\n  - DISCORD_BOT_MODEL_NAME=alpaca-lora-7b\n  # (Optional) Specify your Hugging Face token\n  - HUGGING_FACE_HUB_TOKEN=\n  # (Optional) Specify your Serper API Key to enable Internet search support.\n  - LLMCHAT_SERPER_API_KEY=\n\ncommands:\n  - pip install -r requirements.txt --progress-bar off\n  - LLMCHAT_APP_MODE=DISCORD python entry_point.py\n</code></pre> How to acquire a Discord bot token <p>Before running, ensure you have specified your Discord bot token, which you can obtain from the Discord Developer Portal. If you haven't set up a Discord Bot on the portal yet,  follow the How to Create a Discord Bot Account  section of the tutorial from freeCodeCamp.</p> <p>Finally, here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f discord.dstack.yml\n</code></pre> <p>Once you confirm, <code>dstack</code> will provision the cloud instance and run the task. Once it's up, you can freely send messages to your bot via Discord.</p> <p></p> <p>For advanced commands supported by the bot, check the README file.</p> <p>Source code</p>"},{"location":"examples/mixtral/","title":"Mixtral 8x7B","text":"<p>This example demonstrates how to deploy Mixtral with <code>dstack</code>'s services.</p>"},{"location":"examples/mixtral/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy Mixtral as a service, you have to define the corresponding configuration file. Below are multiple variants: via vLLM (<code>fp16</code>), TGI (<code>fp16</code>), or TGI (<code>int4</code>).</p> vLLM <code>fp16</code>TGI <code>fp16</code>TGI <code>int4</code> <p> <pre><code>type: service\n# This configuration deploys Mixtral in fp16 using vLLM\n\npython: \"3.11\"\n\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size 2 # Should match the number of GPUs\n\nport: 8000\n</code></pre> <p> <pre><code>type: service\n# This configuration deploys Mixtral in fp16 using TGI\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=mistralai/Mixtral-8x7B-Instruct-v0.1\n\ncommands:\n  - text-generation-launcher \n    --hostname 0.0.0.0 \n    --port 8000 \n    --trust-remote-code\n    --num-shard 2 # Should match the number of GPUs\n\nport: 8000\n</code></pre> <p> <pre><code>type: service\n# This configuration deploys Mixtral in int4 using TGI\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n\ncommands:\n  - text-generation-launcher \n    --hostname 0.0.0.0 \n    --port 8000 \n    --trust-remote-code \n    --quantize gptq\n\nport: 8000\n</code></pre> <p>vLLM's support for quantized Mixtral is not yet stable. </p>"},{"location":"examples/mixtral/#run-the-configuration","title":"Run the configuration","text":"<p>Prerequisites</p> <p>Before running a service, make sure to set up a gateway. However, it's not required when using dstack Cloud, as it's set up automatically.</p> <p>Resources</p> <p>For <code>fp16</code>, deployment of Mixtral, ensure a minimum total GPU memory of <code>100GB</code> and disk size of <code>200GB</code>. Also, make sure to adjust the <code>--tensor-parallel-size</code> and <code>--num-shard</code> parameters in the YAML configuration to align with the number of GPUs used. For <code>int4</code>, request at least <code>25GB</code> of GPU memory.</p> vLLM <code>fp16</code>TGI <code>fp16</code>TGI <code>int4</code> <pre><code>$ dstack run . -f llms/mixtral/vllm.dstack.yml --gpu \"80GB:2\" --disk 200GB\n</code></pre> <pre><code>$ dstack run . -f llms/mixtral/tgi.dstack.yml --gpu \"80GB:2\" --disk 200GB\n</code></pre> <pre><code>$ dstack run . -f llms/mixtral/tgi-gptq.dstack.yml --gpu 25GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> Hugging Face Hub token <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p>"},{"location":"examples/mixtral/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/mixtral/#whats-next","title":"What's next?","text":"<ol> <li>Check the vLLM and Text Generation Inference examples</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/qlora/","title":"QLoRA","text":"<p>This example demonstrates how to fine-tune <code>llama-2-7b-chat-hf</code>, with QLoRA and your own script, using Tasks.</p>"},{"location":"examples/qlora/#prepare-a-dataset","title":"Prepare a dataset","text":"<p>When selecting a dataset, make sure that it is pre-processed to match the prompt format of Llama 2:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\n</code></pre> <p>In our example, we'll use the <code>mlabonne/guanaco-llama2-1k</code> dataset. It is a 1K sample from the <code>timdettmers/openassistant-guanaco</code> dataset converted to Llama 2's format.</p>"},{"location":"examples/qlora/#define-the-training-script","title":"Define the training script","text":""},{"location":"examples/qlora/#requirements","title":"Requirements","text":"<p>The most notable libraries that we'll use are <code>peft</code> (required for using the QLoRA technique), <code>bitsandbytes</code> (required for using the quantization technique), and <code>trl</code> (required for supervised fine-tuning).</p> <pre><code>accelerate==0.21.0\npeft==0.4.0\nbitsandbytes==0.40.2\ntransformers==4.31.0\ntrl==0.4.7\nscipy\ntensorboard\nsentencepiece\nhf-transfer\n</code></pre>"},{"location":"examples/qlora/#load-the-base-model","title":"Load the base model","text":"<p>In the first part of our script, we prepare the <code>bitsandbytes</code> config and load the base model along with its tokenizer, based on the script arguments.</p> <pre><code>from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\ndef create_and_prepare_model(args):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=args.use_4bit,\n        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n        bnb_4bit_use_double_quant=args.use_nested_quant,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n\n    model.config.use_cache = False\n    model.config.pretraining_tp\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    return model, tokenizer\n</code></pre>"},{"location":"examples/qlora/#create-a-trainer-instance","title":"Create a trainer instance","text":"<p>In the second part of our script, we prepare the <code>peft</code> config and create the trainer based on the script arguments.</p> <pre><code>from peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ndef create_and_prepare_trainer(model, tokenizer, dataset, args):\n    training_arguments = TrainingArguments(\n        output_dir=args.output_dir,\n        num_train_epochs=args.num_train_epochs,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        optim=args.optim,\n        save_steps=args.save_steps,\n        logging_steps=args.logging_steps,\n        learning_rate=args.learning_rate,\n        weight_decay=args.weight_decay,\n        fp16=args.fp16,\n        bf16=args.bf16,\n        max_grad_norm=args.max_grad_norm,\n        max_steps=args.max_steps,\n        warmup_ratio=args.warmup_ratio,\n        group_by_length=args.group_by_length,\n        lr_scheduler_type=args.lr_scheduler_type,\n        report_to=\"tensorboard\",\n    )\n\n    peft_config = LoraConfig(\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        r=args.lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=args.max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=args.packing,\n    )\n\n    return trainer\n</code></pre>"},{"location":"examples/qlora/#publish-the-fine-tuned-model","title":"Publish the fine-tuned model","text":"<p>In the third part of the script, we merge the base model with the fine-tuned model and push it to the Hugging Face Hub.</p> <pre><code>from peft import PeftModel\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer\n)\n\ndef merge_and_push(args):\n    # Reload model in FP16 and merge it with LoRA weights\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, args.new_model_name)\n    model = model.merge_and_unload()\n\n    # Reload the new tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name, trust_remote_code=True\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    # Publish the new model to Hugging Face Hub\n    model.push_to_hub(args.new_model_name, use_temp_dir=False)\n    tokenizer.push_to_hub(args.new_model_name, use_temp_dir=False)\n</code></pre>"},{"location":"examples/qlora/#put-it-all-together","title":"Put it all together","text":"<p>Finally, in the main part of the script, we put it all together.</p> <pre><code>from dataclasses import dataclass\nfrom datasets import load_dataset\nfrom transformers import HfArgumentParser\n\n@dataclass\nclass ScriptArguments:\n    # ...\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n\n    dataset = load_dataset(args.dataset_name, split=\"train\")\n\n    model, tokenizer = create_and_prepare_model(args)\n\n    trainer = create_and_prepare_trainer(model, tokenizer, dataset, args)\n\n    trainer.train()\n    trainer.model.save_pretrained(args.new_model_name)\n\n    if args.merge_and_push:\n        merge_and_push(args)\n</code></pre>"},{"location":"examples/qlora/#define-the-configuration","title":"Define the configuration","text":"<p>Here's the configuration that runs the training task via <code>dstack</code>:</p> <pre><code>type: task\n\n# (Optional) When not specified, your local Python version is used\npython: \"3.11\"\n\nenv: \n  - HF_HUB_ENABLE_HF_TRANSFER=1\n  # (Required) Specify your Hugging Face token to publish the fine-tuned model\n  - HUGGING_FACE_HUB_TOKEN=\n\nports:\n  - 6006\n\ncommands:\n  - echo \"Installing requirements...\"\n  - pip -q install -r llama-2/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python llama-2/train.py --merge_and_push ${{ run.args }}\n</code></pre>"},{"location":"examples/qlora/#run-the-configuration","title":"Run the configuration","text":"<p>Here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f llama-2/train.dstack.yml --gpu 16GB --num_train_epochs 10\n\nInstalling requirements...\nTensorBoard 2.14.0 at http://127.0.0.1:6006/ (Press CTRL+C to quit)\n{'loss': 1.3491, 'learning_rate': 0.0002, 'epoch': 0.1}\n{'loss': 1.6299, 'learning_rate': 0.0002, 'epoch': 0.2}\n{'loss': 1.2071, 'learning_rate': 0.0002, 'epoch': 0.3}\n</code></pre> <p><code>dstack</code> will provision the cloud instance corresponding to the configured project and profile, run the training, and tear down the cloud instance once the training is complete.</p> Tensorboard <p>Since we've executed <code>tensorboard</code> within our task and configured its port using <code>ports</code>, you can access it using the URL provided in the output. <code>dstack</code> automatically forwards the configured port to your local machine.</p> <p></p>"},{"location":"examples/qlora/#source-code","title":"Source code","text":"<p>The complete and ready-to-run code for the example is available in our GitHub repo.</p>"},{"location":"examples/qlora/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about tasks</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/sdxl/","title":"SDXL","text":"<p>Stable Diffusion XL (SDXL) 1.0 is the latest version of the open-source model that is capable  of generating high-quality images from text.</p> <p>The example below demonstrates how to use <code>dstack</code> to serve SDXL as a REST endpoint in a cloud of your choice for image generation and refinement.</p>"},{"location":"examples/sdxl/#define-endpoints","title":"Define endpoints","text":""},{"location":"examples/sdxl/#requirements","title":"Requirements","text":"<p>Here's the list of libraries that our example will require:</p> <pre><code>transformers\naccelerate\nsafetensors\ndiffusers\ninvisible-watermark&gt;=0.2.0\nopencv-python-headless\nfastapi\nuvicorn\n</code></pre> <p>Let's walk through the code of the example.</p>"},{"location":"examples/sdxl/#load-the-model","title":"Load the model","text":"<p>First of all, let's load the base SDXL model using the <code>diffusers</code> library.</p> <pre><code>from diffusers import StableDiffusionXLPipeline\nimport torch\n\n\nbase = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n)\nbase.to(\"cuda\")\n</code></pre>"},{"location":"examples/sdxl/#define-the-generate-endpoint","title":"Define the generate endpoint","text":"<p>Now that the model is loaded, let's define the FastAPI app and the <code>/generate</code> REST endpoint that will accept a prompt and generate an image.</p> <pre><code>import uuid\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = None\n    width: Optional[int] = None\n    height: Optional[str] = None\n\n\nclass ImageResponse(BaseModel):\n    id: str\n\n\nimages_dir = Path(\"images\")\nimages_dir.mkdir(exist_ok=True)\n\n\n@app.post(\"/generate\")\nasync def generate(request: GenerateRequest):\n    image = base(\n        prompt=request.prompt,\n        negative_prompt=request.negative_prompt,\n        width=request.width,\n        height=request.height,\n    ).images[0]\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre>"},{"location":"examples/sdxl/#define-the-download-endpoint","title":"Define the download endpoint","text":"<p>Notice that the endpoint only returns the ID of the image. To download images by ID, we'll define another endpoint:</p> <pre><code>from fastapi.responses import FileResponse\n\n\n@app.get(\"/download/{id}\")\ndef download(id: str):\n    filename = f\"{id}.png\"\n    return FileResponse(\n        images_dir / filename, media_type=\"image/png\", filename=filename\n    )\n</code></pre> <p>That's it. Once we run the application, we can already utilize the <code>/generate</code> and <code>/download</code> endpoints.</p>"},{"location":"examples/sdxl/#define-the-refine-endpoint","title":"Define the refine endpoint","text":"<p>Since SDXL allows refining images, let's define the refine endpoint to accept the image ID and the refinement prompt.</p> <pre><code>import asyncio\n\nimport PIL\n\n\nclass RefineRequest(BaseModel):\n    id: str\n    prompt: str\n\n\nrefiner = None\nrefiner_lock = asyncio.Lock()\n\n\n@app.post(\"/refine\")\nasync def refine(request: RefineRequest):\n    await refiner_lock.acquire()\n    global refiner\n    if refiner is None:\n        refiner = DiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n            text_encoder_2=base.text_encoder_2,\n            vae=base.vae,\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n            variant=\"fp16\",\n        )\n        refiner.to(\"cuda\")\n    refiner_lock.release()\n\n    image = refiner(\n        prompt=request.prompt,\n        image=PIL.Image.open(images_dir / f\"{request.id}.png\"),\n    ).images[0]\n\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre> <p>The code for the endpoints is ready. Now, let's explore how to use <code>dstack</code> to serve it on a cloud account of your choice.</p>"},{"location":"examples/sdxl/#define-the-configuration","title":"Define the configuration","text":"Tasks <p>If you want to serve an application for development purposes only, you can use  tasks.  In this scenario, while the application runs in the cloud,  it is accessible from your local machine only.</p> <p>For production purposes, the optimal approach to serve an application is by using  services. In this case, the application can be accessed through a public endpoint.</p> <p>Here's the configuration that uses services:</p> <pre><code>type: service\n\n# (Optional) If not specified, it will use your local version\npython: \"3.11\"\n\nport: 8000\n\ncommands: \n  - apt-get update \n  - apt-get install libgl1 -y\n  - pip install -r stable-diffusion-xl/requirements.txt\n  - uvicorn stable-diffusion-xl.main:app --port 8000\n</code></pre>"},{"location":"examples/sdxl/#run-the-configuration","title":"Run the configuration","text":"<p>NOTE:</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Cloud, the default gateway is configured automatically for you.</p> <p>After the gateway is configured, go ahead run the service.</p> <pre><code>$ dstack run . -f stable-diffusion-xl/api.dstack.yml\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query the endpoint:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{ \"prompt\": \"A cat in a hat\" }'\n</code></pre>"},{"location":"examples/sdxl/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/sdxl/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/spot/","title":"Spot instances","text":"<p>Cloud instances come in three types: <code>reserved</code> (for long-term commitments at a cheaper rate), <code>on-demand</code> (used as needed but more expensive), and <code>spot</code> (cheapest, provided when available, but can be taken away when requested by someone else).</p> <p>There are three cloud providers that offer spot instances: AWS, GCP, and Azure.  Once you've configured any of these, you can use spot instances  for dev environments, tasks, and  services.</p> <p>NOTE:</p> <p>Before you can use spot instances with AWS, GCP, and Azure, ensure you request the necessary quota  in the corresponding regions via a support ticket.</p>"},{"location":"examples/spot/#setting-a-spot-policy","title":"Setting a spot policy","text":"<p>By default, for dev environments, <code>dstack</code> doesn't use spot instances. For tasks and services, <code>dstack</code> uses spot instances only if they are available (falling back to on-demand otherwise). This default behavior can be overriden. </p> <p>To use only spot instances, pass <code>--spot</code> to <code>dstack run</code>.  To use spot instances only if they are available (and fallback to on-demand instances otherwise), pass <code>--spot-auto</code>:</p> <pre><code>$ dstack run . --gpu 24GB --spot-auto\n Max price      -\n Max duration   6h\n Spot policy    auto\n Retry policy   no\n\n #  BACKEND  REGION       RESOURCES               SPOT  PRICE\n 1  gcp      us-central1  4xCPU, 16GB, L4 (24GB)  yes   $0.223804\n 2  gcp      us-east1     4xCPU, 16GB, L4 (24GB)  yes   $0.223804\n 3  gcp      us-west1     4xCPU, 16GB, L4 (24GB)  yes   $0.223804\n    ...\n\nContinue? [y/n]:\n</code></pre>"},{"location":"examples/spot/#setting-a-retry-policy","title":"Setting a retry policy","text":"<p>If the requested instance is unavailable, the <code>dstack run</code> command will fail \u2013 unless you specify a retry policy. This can be done via <code>--retry-limit</code>:</p> <pre><code>$ dstack run . --gpu 24GB --spot --retry-limit 1h\n</code></pre> <p>In this case, <code>dstack</code> will retry to find spot instances within one hour. All that time, the run will be marked as pending.</p> <p>NOTE:</p> <p>If you've set the retry duration and the spot instance is taken while your run was not  finished, <code>dstack</code> will restart it from scratch.</p> <p>If you run a service using spot instances, the default retry duration is set to infinity.  </p>"},{"location":"examples/spot/#tips-and-tricks","title":"Tips and tricks","text":"<ol> <li>The <code>--spot-auto</code> policy allows for the automatic use of spot instances when available, seamlessly reverting to    on-demand instances if spots aren't accessible. You can enable it via <code>dstack run</code> or     via <code>profiles.yml</code>.</li> <li>You can use multiple cloud providers (incl. AWS, GCP, and Azure) and regions to increase the likelihood of    obtaining a spot instance. However, in doing so, beware of data transfer costs if large volumes of data    need to be loaded.</li> <li>When using spot instances for training, ensure you save checkpoints regularly and load them if the run is restarted    due to interruption.</li> </ol>"},{"location":"examples/tei/","title":"Text Embeddings Inference","text":"<p>This example demonstrates how to use TEI with <code>dstack</code>'s services to deploy embeddings.</p>"},{"location":"examples/tei/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy a text embeddings model as a service using TEI, define the following configuration file:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-embeddings-inference:latest\n\nenv:\n  - MODEL_ID=thenlper/gte-base\n\nport: 8000\n\ncommands: \n  - text-embeddings-router --hostname 0.0.0.0 --port 8000\n</code></pre>"},{"location":"examples/tei/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Cloud, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f text-embeddings-inference/embeddings.dstack.yml --gpu 24GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query it:</p> <pre><code>$ curl https://yellow-cat-1.mydomain.com \\\n    -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"inputs\":\"What is Deep Learning?\"}'\n\n[[0.010704354,-0.033910684,0.004793657,-0.0042832214,0.07551489,0.028702762,0.03985837,0.021956133,...]]\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f text-embeddings-inference/serve.dstack.yml \\ \n    --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; \\\n    --gpu 24GB\n</code></pre> </p> <p>Source code</p> <p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/tgi/","title":"Text Generation Inference","text":"<p>This example demonstrates how to use TGI with <code>dstack</code>'s services to deploy LLMs.</p>"},{"location":"examples/tgi/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using TGI, you have to define the following configuration file:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=NousResearch/Llama-2-7b-hf\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre>"},{"location":"examples/tgi/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Cloud, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f text-generation-inference/serve.dstack.yml --gpu 24GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query it:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n          \"inputs\": \"What is Deep Learning?\",\n          \"parameters\": {\n            \"max_new_tokens\": 20\n          }\n        }'\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f text-generation-inference/serve.dstack.yml \\\n    --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; \\\n    --gpu 24GB\n</code></pre> </p>"},{"location":"examples/tgi/#quantization","title":"Quantization","text":"<p>An LLM typically requires twice the GPU memory compared to its parameter count. For instance, a model with <code>13B</code> parameters needs around <code>26GB</code> of GPU memory. To decrease memory usage and fit the model on a smaller GPU, consider using quantization, which TGI offers as <code>bitsandbytes</code> and <code>gptq</code> methods. </p> <p>Here's an example of the Llama 2 13B model tailored for a <code>24GB</code> GPU (A10 or L4):</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-GPTQ\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code --quantize gptq\n</code></pre> <p>A similar approach allows running the Llama 2 70B model on an <code>40GB</code> GPU (A100).</p> <p>To calculate the exact GPU memory required for a specific model with different quantization methods, you can use the hf-accelerate/memory-model-usage Space.</p>"},{"location":"examples/tgi/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/tgi/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Embeddings Inference and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/vllm/","title":"vLLM","text":"<p>This example demonstrates how to use vLLM with <code>dstack</code>'s services to deploy LLMs.</p>"},{"location":"examples/vllm/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using vLLM, you have to define the following configuration file:</p> <pre><code>type: service\n\npython: \"3.11\"\n\nenv:\n  - MODEL=NousResearch/Llama-2-7b-hf\n\nport: 8000\n\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\n</code></pre>"},{"location":"examples/vllm/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Cloud, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f vllm/serve.dstack.yml --gpu 24GB\n</code></pre> <p>Endpoint URL</p> <p>Once the service is deployed, its endpoint will be available at  <code>https://&lt;run-name&gt;.&lt;domain-name&gt;</code> (using the domain set up for the gateway).</p> <p>If you wish to customize the run name, you can use the <code>-n</code> argument with the <code>dstack run</code> command.</p> <p>Once the service is up, you can query it:</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"model\": \"NousResearch/Llama-2-7b-hf\",\n          \"prompt\": \"San Francisco is a\",\n          \"max_tokens\": 7,\n          \"temperature\": 0\n        }'\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f vllm/serve.dstack.yml --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; --gpu 24GB\n</code></pre> </p>"},{"location":"examples/vllm/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in dstackai/dstack-examples.</p>"},{"location":"examples/vllm/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference example</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/releases/","title":"Releases","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""}]}