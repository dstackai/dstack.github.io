{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"community/","title":"Community","text":""},{"location":"community/#help-and-feedback","title":"Help and feedback","text":"<p>Connect with the <code>dstack</code> team and community through Discord or share feedback on GitHub.</p>              Discord chat                       Issue tracker"},{"location":"community/#ambassador-program","title":"Ambassador program","text":"<p>Love AI infrastructure and open-source? Become a <code>dstack</code> ambassador!</p> <p>As an ambassador, you\u2019ll play a key role in growing our community by:</p> <ul> <li>Sharing your expertise through blogs, talks, and tutorials</li> <li>Organizing meetups and community events</li> <li>Advocating for open-source AI container orchestration</li> </ul> <p>     Get involved </p> <p>We support ambassadors with recognition, wider exposure, and cloud GPU credits.</p>"},{"location":"community/#contributing-to-dstack","title":"Contributing to dstack","text":"<p>Join the development of <code>dstack</code> by contributing bug fixes,  new features, and cloud integrations via custom backends.</p>              Contributing guide                       Adding new backends"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#deployment","title":"Deployment","text":"vLLM         <p>             Deploy Llama 3.1 with vLLM         </p>             TGI         <p>             Deploy Llama 3.1 with TGI         </p>             NIM         <p>             Deploy Llama 3.1 with NIM         </p>"},{"location":"examples/#fine-tuning","title":"Fine-tuning","text":"Axolotl          <p>             Fine-tune Llama 3 on a custom dataset using Axolotl.         </p>              TRL          <p>             Fine-tune Llama 3.1 8B on a custom dataset using TRL.         </p>"},{"location":"examples/#accelerators","title":"Accelerators","text":"AMD          <p>             Deploy and fine-tune LLMs on AMD         </p>              TPU          <p>             Deploy and fine-tune LLMs on TPU         </p>"},{"location":"examples/#llms","title":"LLMs","text":"Llama 3.1          <p>             Deploy and fine-tune Llama 3.1         </p>              Llama 3.2          <p>             Deploy Llama 3.2 vision models         </p>"},{"location":"examples/#misc","title":"Misc","text":"Docker Compose          <p>             Use Docker and Docker Compose inside runs         </p>"},{"location":"partners/","title":"Partners","text":"<p><code>dstack</code> sets a new standard for managing AI infrastructure by building an open-source, streamlined alternative to Kubernetes and Slurm, aimed at accelerating and democratizing AI development across any cloud and hardware providers.</p>"},{"location":"partners/#cloud-providers","title":"Cloud providers","text":"<p>Here are some cloud providers <code>dstack</code> integrates with and collaborates:</p>              Amazon Web Services                       Microsoft Azure                       Google Cloud Platform                       Lambda                       RunPod                       Vultr                       Vast.ai                       OCI                       CUDO                       TensorDock                       DataCrunch"},{"location":"partners/#bare-metal-providers","title":"Bare metal providers","text":"<p>Here are some of the bare metal providers <code>dstack</code> collaborates with:</p>              Hot Aisle"},{"location":"partners/#become-a-partner","title":"Become a partner","text":"<p>Are you interested in collaborating or integrating with us to become part of the ecosystem?</p> <p>     Talk to us </p>"},{"location":"terms/","title":"Terms of service","text":""},{"location":"terms/#agreement-to-terms","title":"Agreement to terms","text":"<p>We are dstack GmbH (\"Company,\" \"we,\" \"us,\" \"our\"), a company registered in Germany at Franz-Joseph-Stra\u00dfe, 11, Munich, Bayern 80801.</p> <p>These Legal Terms constitute a legally binding agreement made between you, whether personally or on behalf of an entity (\"you\"), and dstack GmbH, concerning your access to and use of the Services. You agree that by accessing the Services, you have read, understood, and agreed to be bound by all of these Legal Terms. IF YOU DO NOT AGREE WITH ALL OF THESE LEGAL TERMS, THEN YOU ARE EXPRESSLY PROHIBITED FROM USING THE SERVICES AND YOU MUST DISCONTINUE USE IMMEDIATELY.</p> <p>Supplemental terms and conditions or documents that may be posted on the Services from time to time are hereby expressly incorporated herein by reference. We reserve the right, in our sole discretion, to make changes or modifications to these Legal Terms from time to time. We will alert you about any changes by updating the \"Last updated\" date of these Legal Terms, and you waive any right to receive specific notice of each such change. It is your responsibility to periodically review these Legal Terms to stay informed of updates. You will be subject to, and will be deemed to have been made aware of and to have accepted, the changes in any revised Legal Terms by your continued use of the Services after the date such revised Legal Terms are posted.</p>"},{"location":"terms/#1-our-services","title":"1. Our services","text":"<p>The information provided when using the Services is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject us to any registration requirement within such jurisdiction or country. Accordingly, those persons who choose to access the Services from other locations do so on their own initiative and are solely responsible for compliance with local laws, if and to the extent local laws are applicable.</p> <p>The Services are not tailored to comply with industry-specific regulations (Health Insurance Portability and Accountability Act (HIPAA), Federal Information Security Management Act (FISMA), etc.), so if your interactions would be subjected to such laws, you may not use the Services. You may not use the Services in a way that would violate the Gramm-Leach-Bliley Act (GLBA).</p>"},{"location":"terms/#2-intelliectual-property-rights","title":"2. Intelliectual property rights","text":"<p>Our intellectual property</p> <p>We are the owner or the licensee of all intellectual property rights in our Services, including all source code, databases, functionality, software, website designs, audio, video, text, photographs, and graphics in the Services ( collectively, the \"Content\"), as well as the trademarks, service marks, and logos contained therein (the \"Marks\").</p> <p>Our Content and Marks are protected by copyright and trademark laws (and various other intellectual property rights and unfair competition laws) and treaties in the United States and around the world.</p> <p>The Content and Marks are provided in or through the Services \"AS IS\" for your personal, non-commercial use or internal business purpose only.</p> <p>Your use of our Services</p> <p>Subject to your compliance with these Legal Terms, including the \"Prohibited activities\" section below, we grant you a non-exclusive, non-transferable, revocable license to:</p> <ul> <li>access the Services; and</li> <li>download or print a copy of any portion of the Content to which you have properly gained access. solely for your personal, non-commercial use or internal business purpose.</li> </ul> <p>Except as set out in this section or elsewhere in our Legal Terms, no part of the Services and no Content or Marks may be copied, reproduced, aggregated, republished, uploaded, posted, publicly displayed, encoded, translated, transmitted, distributed, sold, licensed, or otherwise exploited for any commercial purpose whatsoever, without our express prior written permission.</p> <p>If you wish to make any use of the Services, Content, or Marks other than as set out in this section or elsewhere in our Legal Terms, please address your request to: hello@dstack.ai. If we ever grant you the permission to post, reproduce, or publicly display any part of our Services or Content, you must identify us as the owners or licensors of the Services, Content, or Marks and ensure that any copyright or proprietary notice appears or is visible on posting, reproducing, or displaying our Content.</p> <p>We reserve all rights not expressly granted to you in and to the Services, Content, and Marks.</p> <p>Any breach of these Intellectual Property Rights will constitute a material breach of our Legal Terms and your right to use our Services will terminate immediately.</p> <p>Your submissions</p> <p>Please review this section and the \"Prohibited activities\" section carefully prior to using our Services to understand the (a) rights you give us and (b) obligations you have when you post or upload any content through the Services.</p> <p>Submissions: By directly sending us any question, comment, suggestion, idea, feedback, or other information about the Services (\"Submissions\"), you agree to assign to us all intellectual property rights in such Submission. You agree that we shall own this Submission and be entitled to its unrestricted use and dissemination for any lawful purpose, commercial or otherwise, without acknowledgment or compensation to you.</p> <p>You are responsible for what you post or upload: By sending us Submissions through any part of the Services you: * confirm that you have read and agree with our \"Prohibited activities\" and will not post, send, publish, upload, or * transmit through the Services any Submission that is illegal, harassing, hateful, harmful, defamatory, obscene, * bullying, abusive, discriminatory, threatening to any person or group, sexually explicit, false, inaccurate, deceitful,   or misleading; * to the extent permissible by applicable law, waive any and all moral rights to any such Submission; * warrant that any such Submission are original to you or that you have the necessary rights and licenses to submit such    Submissions and that you have full authority to grant us the above-mentioned rights in relation to your Submissions; and * warrant and represent that your Submissions do not constitute confidential information.</p> <p>You are solely responsible for your Submissions and you expressly agree to reimburse us for any and all losses that we may suffer because of your breach of (a) this section, (b) any third party\u2019s intellectual property rights, or (c) applicable law.</p>"},{"location":"terms/#3-user-representations","title":"3. User representations","text":"<p>By using the Services, you represent and warrant that: (1) all registration information you submit will be true, accurate, current, and complete; (2) you will maintain the accuracy of such information and promptly update such registration information as necessary; (3) you have the legal capacity and you agree to comply with these Legal Terms; ( 4) you are not a minor in the jurisdiction in which you reside; (5) you will not access the Services through automated or non-human means, whether through a bot, script or otherwise; (6) you will not use the Services for any illegal or unauthorized purpose; and (7) your use of the Services will not violate any applicable law or regulation.</p> <p>If you provide any information that is untrue, inaccurate, not current, or incomplete, we have the right to suspend or terminate your account and refuse any and all current or future use of the Services (or any portion thereof).</p>"},{"location":"terms/#4-user-registration","title":"4. User registration","text":"<p>You may be required to register to use the Services. You agree to keep your password confidential and will be responsible for all use of your account and password. We reserve the right to remove, reclaim, or change a username you select if we determine, in our sole discretion, that such username is inappropriate, obscene, or otherwise objectionable.</p>"},{"location":"terms/#5-purchases-and-payment","title":"5. Purchases and payment","text":"<p>We accept the following forms of payment:</p> <ul> <li>Visa</li> <li>Mastercard</li> </ul> <p>You agree to provide current, complete, and accurate purchase and account information for all purchases made via the Services. You further agree to promptly update account and payment information, including email address, payment method, and payment card expiration date, so that we can complete your transactions and contact you as needed. Sales tax will be added to the price of purchases as deemed required by us. We may change prices at any time. All payments shall be in US dollars.</p> <p>You agree to pay all charges at the prices then in effect for your purchases and any applicable shipping fees, and you authorize us to charge your chosen payment provider for any such amounts upon placing your order. We reserve the right to correct any errors or mistakes in pricing, even if we have already requested or received payment.</p> <p>We reserve the right to refuse any order placed through the Services. We may, in our sole discretion, limit or cancel quantities purchased per person, per household, or per order. These restrictions may include orders placed by or under the same customer account, the same payment method, and/or orders that use the same billing or shipping address. We reserve the right to limit or prohibit orders that, in our sole judgment, appear to be placed by dealers, resellers, or distributors.</p>"},{"location":"terms/#6-subscriptions","title":"6. Subscriptions","text":"<p>Billing and Renewal</p> <p>e.g. by topping up their balance manually using their credit card.</p> <p>Cancellation</p> <p>You can cancel your subscription at any time by contacting us using the contact information provided below. Your cancellation will take effect at the end of the current paid term. If you have any questions or are unsatisfied with our Services, please email us at hello@dstack.ai .</p> <p>Fee Changes</p> <p>We may, from time to time, make changes to the subscription fee and will communicate any price changes to you in accordance with applicable law.</p>"},{"location":"terms/#7-software","title":"7. Software","text":"<p>We may include software for use in connection with our Services. If such software is accompanied by an end user license agreement (\"EULA\"), the terms of the EULA will govern your use of the software. If such software is not accompanied by a EULA, then we grant to you a non-exclusive, revocable, personal, and non-transferable license to use such software solely in connection with our services and in accordance with these Legal Terms. Any software and any related documentation is provided \"AS IS\" without warranty of any kind, either express or implied, including, without limitation, the implied warranties of merchantability, fitness for a particular purpose, or non-infringement. You accept any and all risk arising out of use or performance of any software. You may not reproduce or redistribute any software except in accordance with the EULA or these Legal Terms.</p>"},{"location":"terms/#8-prohibited-activities","title":"8. Prohibited activities","text":"<p>You may not access or use the Services for any purpose other than that for which we make the Services available. The Services may not be used in connection with any commercial endeavors except those that are specifically endorsed or approved by us.</p> <p>As a user of the Services, you agree not to:</p> <ul> <li>Systematically retrieve data or other content from the Services to create or compile, directly or indirectly, a   collection, compilation, database, or directory without written permission from us.</li> <li>Trick, defraud, or mislead us and other users, especially in any attempt to learn sensitive account information such   as user passwords.</li> <li>Circumvent, disable, or otherwise interfere with security-related features of the Services, including features that   prevent or restrict the use or copying of any Content or enforce limitations on the use of the Services and/or the   Content contained therein.</li> <li>Disparage, tarnish, or otherwise harm, in our opinion, us and/or the Services.</li> <li>Use any information obtained from the Services in order to harass, abuse, or harm another person.</li> <li>Make improper use of our support services or submit false reports of abuse or misconduct.</li> <li>Use the Services in a manner inconsistent with any applicable laws or regulations.</li> <li>Engage in unauthorized framing of or linking to the Services.</li> <li>Upload or transmit (or attempt to upload or to transmit) viruses, Trojan horses, or other material, including   excessive use of capital letters and spamming (continuous posting of repetitive text), that interferes with any   party\u2019s uninterrupted use and enjoyment of the Services or modifies, impairs, disrupts, alters, or interferes with the   use, features, functions, operation, or maintenance of the Services.</li> <li>Engage in any automated use of the system, such as using scripts to send comments or messages, or using any data   mining, robots, or similar data gathering and extraction tools.</li> <li>Delete the copyright or other proprietary rights notice from any Content.</li> <li>Attempt to impersonate another user or person or use the username of another user.</li> <li>Upload or transmit (or attempt to upload or to transmit) any material that acts as a passive or active information   collection or transmission mechanism, including without limitation, clear graphics interchange formats (\"gifs\"), 1\u00d71   pixels, web bugs, cookies, or other similar devices (sometimes referred to as \"spyware\" or \"passive collection   mechanisms\" or \"pcms\").</li> <li>Interfere with, disrupt, or create an undue burden on the Services or the networks or services connected to the   Services.</li> <li>Harass, annoy, intimidate, or threaten any of our employees or agents engaged in providing any portion of the Services   to you.</li> <li>Attempt to bypass any measures of the Services designed to prevent or restrict access to the Services, or any portion   of the Services.</li> <li>Copy or adapt the Services' software, including but not limited to Flash, PHP, HTML, JavaScript, or other code.</li> <li>Except as permitted by applicable law, decipher, decompile, disassemble, or reverse engineer any of the software   comprising or in any way making up a part of the Services.</li> <li>Except as may be the result of standard search engine or Internet browser usage, use, launch, develop, or distribute   any automated system, including without limitation, any spider, robot, cheat utility, scraper, or offline reader that   accesses the Services, or use or launch any unauthorized script or other software.</li> <li>Use a buying agent or purchasing agent to make purchases on the Services.</li> <li>Make any unauthorized use of the Services, including collecting usernames and/or email addresses of users by   electronic or other means for the purpose of sending unsolicited email, or creating user accounts by automated means   or under false pretenses.</li> <li>Use the Services as part of any effort to compete with us or otherwise use the Services and/or the Content for any   revenue-generating endeavor or commercial enterprise.</li> </ul>"},{"location":"terms/#9-user-generated-contributions","title":"9. User generated contributions","text":"<p>The Services does not offer users to submit or post content.</p>"},{"location":"terms/#10-contribution-license","title":"10. Contribution license","text":"<p>You and Services agree that we may access, store, process, and use any information and personal data that you provide following the terms of the Privacy Policy and your choices (including settings).</p> <p>By submitting suggestions or other feedback regarding the Services, you agree that we can use and share such feedback for any purpose without compensation to you.</p>"},{"location":"terms/#11-social-media","title":"11. Social media","text":"<p>As part of the functionality of the Services, you may link your account with online accounts you have with third-party service providers (each such account, a \"Third-Party Account\") by either: (1) providing your Third-Party Account login information through the Services; or (2) allowing us to access your Third-Party Account, as is permitted under the applicable terms and conditions that govern your use of each Third-Party Account. You represent and warrant that you are entitled to disclose your Third-Party Account login information to us and/or grant us access to your Third-Party Account, without breach by you of any of the terms and conditions that govern your use of the applicable Third-Party Account, and without obligating us to pay any fees or making us subject to any usage limitations imposed by the third-party service provider of the Third-Party Account. By granting us access to any Third-Party Accounts, you understand that (1) we may access, make available, and store (if applicable) any content that you have provided to and stored in your Third-Party Account (the \"Social Network Content\") so that it is available on and through the Services via your account, including without limitation any friend lists and (2) we may submit to and receive from your Third-Party Account additional information to the extent you are notified when you link your account with the Third-Party Account. Depending on the Third-Party Accounts you choose and subject to the privacy settings that you have set in such Third-Party Accounts, personally identifiable information that you post to your Third-Party Accounts may be available on and through your account on the Services. Please note that if a Third-Party Account or associated service becomes unavailable or our access to such Third-Party Account is terminated by the third-party service provider, then Social Network Content may no longer be available on and through the Services. You will have the ability to disable the connection between your account on the Services and your Third-Party Accounts at any time. PLEASE NOTE THAT YOUR RELATIONSHIP WITH THE THIRD-PARTY SERVICE PROVIDERS ASSOCIATED WITH YOUR THIRD-PARTY ACCOUNTS IS GOVERNED SOLELY BY YOUR AGREEMENT(S) WITH SUCH THIRD-PARTY SERVICE PROVIDERS. We make no effort to review any Social Network Content for any purpose, including but not limited to, for accuracy, legality, or non-infringement, and we are not responsible for any Social Network Content. You acknowledge and agree that we may access your email address book associated with a Third-Party Account and your contacts list stored on your mobile device or tablet computer solely for purposes of identifying and informing you of those contacts who have also registered to use the Services. You can deactivate the connection between the Services and your Third-Party Account by contacting us using the contact information below or through your account settings (if applicable). We will attempt to delete any information stored on our servers that was obtained through such Third-Party Account, except the username and profile picture that become associated with your account.</p>"},{"location":"terms/#12-third-party-websites-and-content","title":"12. Third-party websites and content","text":"<p>The Services may contain (or you may be sent via the Site) links to other websites (\"Third-Party Websites\") as well as articles, photographs, text, graphics, pictures, designs, music, sound, video, information, applications, software, and other content or items belonging to or originating from third parties (\"Third-Party Content\"). Such Third-Party Websites and Third-Party Content are not investigated, monitored, or checked for accuracy, appropriateness, or completeness by us, and we are not responsible for any Third-Party Websites accessed through the Services or any Third-Party Content posted on, available through, or installed from the Services, including the content, accuracy, offensiveness, opinions, reliability, privacy practices, or other policies of or contained in the Third-Party Websites or the Third-Party Content. Inclusion of, linking to, or permitting the use or installation of any Third-Party Websites or any Third-Party Content does not imply approval or endorsement thereof by us. If you decide to leave the Services and access the Third-Party Websites or to use or install any Third-Party Content, you do so at your own risk, and you should be aware these Legal Terms no longer govern. You should review the applicable terms and policies, including privacy and data gathering practices, of any website to which you navigate from the Services or relating to any applications you use or install from the Services. Any purchases you make through Third-Party Websites will be through other websites and from other companies, and we take no responsibility whatsoever in relation to such purchases which are exclusively between you and the applicable third party. You agree and acknowledge that we do not endorse the products or services offered on Third-Party Websites and you shall hold us blameless from any harm caused by your purchase of such products or services. Additionally, you shall hold us blameless from any losses sustained by you or harm caused to you relating to or resulting in any way from any Third-Party Content or any contact with Third-Party Websites.</p>"},{"location":"terms/#13-services-management","title":"13. Services management","text":"<p>We reserve the right, but not the obligation, to: (1) monitor the Services for violations of these Legal Terms; (2) take appropriate legal action against anyone who, in our sole discretion, violates the law or these Legal Terms, including without limitation, reporting such user to law enforcement authorities; (3) in our sole discretion and without limitation, refuse, restrict access to, limit the availability of, or disable (to the extent technologically feasible) any of your Contributions or any portion thereof; (4) in our sole discretion and without limitation, notice, or liability, to remove from the Services or otherwise disable all files and content that are excessive in size or are in any way burdensome to our systems; and (5) otherwise manage the Services in a manner designed to protect our rights and property and to facilitate the proper functioning of the Services.</p>"},{"location":"terms/#14-privacy-policy","title":"14. Privacy policy","text":"<p>We care about data privacy and security. Please review our Privacy Policy. By using the Services, you agree to be bound by our Privacy Policy, which is incorporated into these Legal Terms. Please be advised the Services are hosted in Germany and United States. If you access the Services from any other region of the world with laws or other requirements governing personal data collection, use, or disclosure that differ from applicable laws in Germany and United States, then through your continued use of the Services, you are transferring your data to Germany and United States, and you expressly consent to have your data transferred to and processed in Germany and United States.</p>"},{"location":"terms/#15-term-and-termination","title":"15. Term and termination","text":"<p>These Legal Terms shall remain in full force and effect while you use the Services. WITHOUT LIMITING ANY OTHER PROVISION OF THESE LEGAL TERMS, WE RESERVE THE RIGHT TO, IN OUR SOLE DISCRETION AND WITHOUT NOTICE OR LIABILITY, DENY ACCESS TO AND USE OF THE SERVICES (INCLUDING BLOCKING CERTAIN IP ADDRESSES), TO ANY PERSON FOR ANY REASON OR FOR NO REASON, INCLUDING WITHOUT LIMITATION FOR BREACH OF ANY REPRESENTATION, WARRANTY, OR COVENANT CONTAINED IN THESE LEGAL TERMS OR OF ANY APPLICABLE LAW OR REGULATION. WE MAY TERMINATE YOUR USE OR PARTICIPATION IN THE SERVICES OR DELETE YOUR ACCOUNT AND ANY CONTENT OR INFORMATION THAT YOU POSTED AT ANY TIME, WITHOUT WARNING, IN OUR SOLE DISCRETION.</p> <p>If we terminate or suspend your account for any reason, you are prohibited from registering and creating a new account under your name, a fake or borrowed name, or the name of any third party, even if you may be acting on behalf of the third party. In addition to terminating or suspending your account, we reserve the right to take appropriate legal action, including without limitation pursuing civil, criminal, and injunctive redress.</p>"},{"location":"terms/#16-modifications-and-interruptions","title":"16. Modifications and interruptions","text":"<p>We reserve the right to change, modify, or remove the contents of the Services at any time or for any reason at our sole discretion without notice. However, we have no obligation to update any information on our Services. We will not be liable to you or any third party for any modification, price change, suspension, or discontinuance of the Services.</p> <p>We cannot guarantee the Services will be available at all times. We may experience hardware, software, or other problems or need to perform maintenance related to the Services, resulting in interruptions, delays, or errors. We reserve the right to change, revise, update, suspend, discontinue, or otherwise modify the Services at any time or for any reason without notice to you. You agree that we have no liability whatsoever for any loss, damage, or inconvenience caused by your inability to access or use the Services during any downtime or discontinuance of the Services. Nothing in these Legal Terms will be construed to obligate us to maintain and support the Services or to supply any corrections, updates, or releases in connection therewith.</p>"},{"location":"terms/#17-governing-law","title":"17. Governing law","text":"<p>These Legal Terms are governed by and interpreted following the laws of Germany, and the use of the United Nations Convention of Contracts for the International Sales of Goods is expressly excluded. If your habitual residence is in the EU, and you are a consumer, you additionally possess the protection provided to you by obligatory provisions of the law in your country to residence. dstack GmbH and yourself both agree to submit to the non-exclusive jurisdiction of the courts of Bayern, which means that you may make a claim to defend your consumer protection rights in regards to these Legal Terms in Germany, or in the EU country in which you reside.</p>"},{"location":"terms/#18-dispute-resolution","title":"18. Dispute resolution","text":"<p>Informal Negotiations</p> <p>To expedite resolution and control the cost of any dispute, controversy, or claim related to these Legal Terms (each a \" Dispute\" and collectively, the \"Disputes\") brought by either you or us (individually, a \"Party\" and collectively, the \" Parties\"), the Parties agree to first attempt to negotiate any Dispute (except those Disputes expressly provided below) informally for at least thirty (30) days before initiating arbitration. Such informal negotiations commence upon written notice from one Party to the other Party.</p> <p>Binding Arbitration</p> <p>Any dispute arising from the relationships between the Parties to these Legal Terms shall be determined by one arbitrator who will be chosen in accordance with the Arbitration and Internal Rules of the European Court of Arbitration being part of the European Centre of Arbitration having its seat in Strasbourg, and which are in force at the time the application for arbitration is filed, and of which adoption of this clause constitutes acceptance. The seat of arbitration shall be Munich , Germany . The language of the proceedings shall be German . Applicable rules of substantive law shall be the law of Germany .</p> <p>Restrictions</p> <p>The Parties agree that any arbitration shall be limited to the Dispute between the Parties individually. To the full extent permitted by law, (a) no arbitration shall be joined with any other proceeding; (b) there is no right or authority for any Dispute to be arbitrated on a class-action basis or to utilize class action procedures; and (c) there is no right or authority for any Dispute to be brought in a purported representative capacity on behalf of the general public or any other persons.</p> <p>Exceptions to Informal Negotiations and Arbitration</p> <p>The Parties agree that the following Disputes are not subject to the above provisions concerning informal negotiations binding arbitration: (a) any Disputes seeking to enforce or protect, or concerning the validity of, any of the intellectual property rights of a Party; (b) any Dispute related to, or arising from, allegations of theft, piracy, invasion of privacy, or unauthorized use; and (c) any claim for injunctive relief. If this provision is found to be illegal or unenforceable, then neither Party will elect to arbitrate any Dispute falling within that portion of this provision found to be illegal or unenforceable and such Dispute shall be decided by a court of competent jurisdiction within the courts listed for jurisdiction above, and the Parties agree to submit to the personal jurisdiction of that court.</p>"},{"location":"terms/#19-corrections","title":"19. Corrections","text":"<p>There may be information on the Services that contains typographical errors, inaccuracies, or omissions, including descriptions, pricing, availability, and various other information. We reserve the right to correct any errors, inaccuracies, or omissions and to change or update the information on the Services at any time, without prior notice.</p>"},{"location":"terms/#20-disclaimer","title":"20. Disclaimer","text":"<p>THE SERVICES ARE PROVIDED ON AN AS-IS AND AS-AVAILABLE BASIS. YOU AGREE THAT YOUR USE OF THE SERVICES WILL BE AT YOUR SOLE RISK. TO THE FULLEST EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED, IN CONNECTION WITH THE SERVICES AND YOUR USE THEREOF, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. WE MAKE NO WARRANTIES OR REPRESENTATIONS ABOUT THE ACCURACY OR COMPLETENESS OF THE SERVICES' CONTENT OR THE CONTENT OF ANY WEBSITES OR MOBILE APPLICATIONS LINKED TO THE SERVICES AND WE WILL ASSUME NO LIABILITY OR RESPONSIBILITY FOR ANY (1) ERRORS, MISTAKES, OR INACCURACIES OF CONTENT AND MATERIALS, (2) PERSONAL INJURY OR PROPERTY DAMAGE, OF ANY NATURE WHATSOEVER, RESULTING FROM YOUR ACCESS TO AND USE OF THE SERVICES, (3) ANY UNAUTHORIZED ACCESS TO OR USE OF OUR SECURE SERVERS AND/OR ANY AND ALL PERSONAL INFORMATION AND/OR FINANCIAL INFORMATION STORED THEREIN, (4) ANY INTERRUPTION OR CESSATION OF TRANSMISSION TO OR FROM THE SERVICES, (5) ANY BUGS, VIRUSES, TROJAN HORSES, OR THE LIKE WHICH MAY BE TRANSMITTED TO OR THROUGH THE SERVICES BY ANY THIRD PARTY, AND/OR (6) ANY ERRORS OR OMISSIONS IN ANY CONTENT AND MATERIALS OR FOR ANY LOSS OR DAMAGE OF ANY KIND INCURRED AS A RESULT OF THE USE OF ANY CONTENT POSTED, TRANSMITTED, OR OTHERWISE MADE AVAILABLE VIA THE SERVICES. WE DO NOT WARRANT, ENDORSE, GUARANTEE, OR ASSUME RESPONSIBILITY FOR ANY PRODUCT OR SERVICE ADVERTISED OR OFFERED BY A THIRD PARTY THROUGH THE SERVICES, ANY HYPERLINKED WEBSITE, OR ANY WEBSITE OR MOBILE APPLICATION FEATURED IN ANY BANNER OR OTHER ADVERTISING, AND WE WILL NOT BE A PARTY TO OR IN ANY WAY BE RESPONSIBLE FOR MONITORING ANY TRANSACTION BETWEEN YOU AND ANY THIRD-PARTY PROVIDERS OF PRODUCTS OR SERVICES. AS WITH THE PURCHASE OF A PRODUCT OR SERVICE THROUGH ANY MEDIUM OR IN ANY ENVIRONMENT, YOU SHOULD USE YOUR BEST JUDGMENT AND EXERCISE CAUTION WHERE APPROPRIATE.</p>"},{"location":"terms/#21-limitations-of-liability","title":"21. Limitations of liability","text":"<p>IN NO EVENT WILL WE OR OUR DIRECTORS, EMPLOYEES, OR AGENTS BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY DIRECT, INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL, OR PUNITIVE DAMAGES, INCLUDING LOST PROFIT, LOST REVENUE, LOSS OF DATA, OR OTHER DAMAGES ARISING FROM YOUR USE OF THE SERVICES, EVEN IF WE HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, OUR LIABILITY TO YOU FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM OF THE ACTION, WILL AT ALL TIMES BE LIMITED TO THE LESSER OF THE AMOUNT PAID, IF ANY, BY YOU TO US DURING THE zero (0) MONTH PERIOD PRIOR TO ANY CAUSE OF ACTION ARISING OR $0.00 USD. CERTAIN US STATE LAWS AND INTERNATIONAL LAWS DO NOT ALLOW LIMITATIONS ON IMPLIED WARRANTIES OR THE EXCLUSION OR LIMITATION OF CERTAIN DAMAGES. IF THESE LAWS APPLY TO YOU, SOME OR ALL OF THE ABOVE DISCLAIMERS OR LIMITATIONS MAY NOT APPLY TO YOU, AND YOU MAY HAVE ADDITIONAL RIGHTS.</p>"},{"location":"terms/#22-indemnification","title":"22. Indemnification","text":"<p>You agree to defend, indemnify, and hold us harmless, including our subsidiaries, affiliates, and all of our respective officers, agents, partners, and employees, from and against any loss, damage, liability, claim, or demand, including reasonable attorneys\u2019 fees and expenses, made by any third party due to or arising out of: (1) use of the Services; (2) breach of these Legal Terms; (3) any breach of your representations and warranties set forth in these Legal Terms; (4) your violation of the rights of a third party, including but not limited to intellectual property rights; or (5) any overt harmful act toward any other user of the Services with whom you connected via the Services. Notwithstanding the foregoing, we reserve the right, at your expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate, at your expense, with our defense of such claims. We will use reasonable efforts to notify you of any such claim, action, or proceeding which is subject to this indemnification upon becoming aware of it.</p>"},{"location":"terms/#23-user-data","title":"23. User data","text":"<p>We will maintain certain data that you transmit to the Services for the purpose of managing the performance of the Services, as well as data relating to your use of the Services. Although we perform regular routine backups of data, you are solely responsible for all data that you transmit or that relates to any activity you have undertaken using the Services. You agree that we shall have no liability to you for any loss or corruption of any such data, and you hereby waive any right of action against us arising from any such loss or corruption of such data.</p>"},{"location":"terms/#24-electronic-communications-transactions-and-signatures","title":"24. Electronic communications, transactions, and signatures","text":"<p>Visiting the Services, sending us emails, and completing online forms constitute electronic communications. You consent to receive electronic communications, and you agree that all agreements, notices, disclosures, and other communications we provide to you electronically, via email and on the Services, satisfy any legal requirement that such communication be in writing. YOU HEREBY AGREE TO THE USE OF ELECTRONIC SIGNATURES, CONTRACTS, ORDERS, AND OTHER RECORDS, AND TO ELECTRONIC DELIVERY OF NOTICES, POLICIES, AND RECORDS OF TRANSACTIONS INITIATED OR COMPLETED BY US OR VIA THE SERVICES. You hereby waive any rights or requirements under any statutes, regulations, rules, ordinances, or other laws in any jurisdiction which require an original signature or delivery or retention of non-electronic records, or to payments or the granting of credits by any means other than electronic means.</p>"},{"location":"terms/#25-california-users-and-residents","title":"25. California users and residents","text":"<p>If any complaint with us is not satisfactorily resolved, you can contact the Complaint Assistance Unit of the Division of Consumer Services of the California Department of Consumer Affairs in writing at 1625 North Market Blvd., Suite N 112, Sacramento, California 95834 or by telephone at (800) 952-5210 or (916) 445-1254.</p>"},{"location":"terms/#26-miscellaneous","title":"26. Miscellaneous","text":"<p>These Legal Terms and any policies or operating rules posted by us on the Services or in respect to the Services constitute the entire agreement and understanding between you and us. Our failure to exercise or enforce any right or provision of these Legal Terms shall not operate as a waiver of such right or provision. These Legal Terms operate to the fullest extent permissible by law. We may assign any or all of our rights and obligations to others at any time. We shall not be responsible or liable for any loss, damage, delay, or failure to act caused by any cause beyond our reasonable control. If any provision or part of a provision of these Legal Terms is determined to be unlawful, void, or unenforceable, that provision or part of the provision is deemed severable from these Legal Terms and does not affect the validity and enforceability of any remaining provisions. There is no joint venture, partnership, employment or agency relationship created between you and us as a result of these Legal Terms or use of the Services. You agree that these Legal Terms will not be construed against us by virtue of having drafted them. You hereby waive any and all defenses you may have based on the electronic form of these Legal Terms and the lack of signing by the parties hereto to execute these Legal Terms.</p>"},{"location":"terms/#27-contact-us","title":"27. Contact us","text":"<p>In order to resolve a complaint regarding the Services or to receive further information regarding use of the Services, please contact us at hello@dstack.ai.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/archive/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/ambassador-program/","title":"Get involved as a community ambassador","text":"<p>As we wrap up an exciting year at <code>dstack</code>, we\u2019re thrilled to introduce our Ambassador Program. This initiative invites AI infrastructure enthusiasts and those passionate about open-source AI to share their knowledge, contribute to the growth of the <code>dstack</code> community, and play a key role in advancing the open AI ecosystem.</p> <p></p>"},{"location":"blog/ambassador-program/#what-will-you-do-as-an-ambassador","title":"What will you do as an ambassador?","text":"<p>As an ambassador, you\u2019ll play a vital role in sharing best practices for using containers in AI workflows, advocating for open-source tools for AI model training and inference, and helping the community use <code>dstack</code> with  various cloud providers, data centers, and GPU vendors.</p> <p>Your contributions might include writing technical blog posts, delivering talks, organizing <code>dstack</code> meetups, and championing the open AI ecosystem within the broader community.</p>"},{"location":"blog/ambassador-program/#who-is-the-program-for","title":"Who is the program for?","text":"<p>Whether you\u2019re new to <code>dstack</code> or already experienced, the ambassador program is open to anyone passionate  about open-source AI, eager to share knowledge, and excited to engage with the AI community.</p>"},{"location":"blog/ambassador-program/#how-do-we-support-ambassadors","title":"How do we support ambassadors?","text":"<p>At <code>dstack</code>, we are committed to supporting ambassadors through recognition, amplifying their content, and providing cloud GPU credits to power their projects.</p>"},{"location":"blog/ambassador-program/#how-to-apply","title":"How to apply?","text":"<p>If you\u2019re interested in becoming an ambassador, fill out a quick form with details about yourself and your experience. We\u2019ll reach out with a starter kit and next steps.</p> <p>     Get involved </p> <p>Have questions? Reach out via Discord !</p> <p>\ud83d\udc9c In the meantime, we\u2019re thrilled to welcome Park Chansung , the first <code>dstack</code> ambassador.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/","title":"Benchmarking Llama 3.1 405B on 8x AMD MI300X GPUs","text":"<p>At <code>dstack</code>, we've been adding support for AMD GPUs with SSH fleets,  so we saw this as a great chance to test our integration by benchmarking AMD GPUs. Our friends at  Hot Aisle , who build top-tier  bare metal compute for AMD GPUs, kindly provided the hardware for the benchmark.</p> <p></p> <p>With access to a bare metal machine with 8x AMD MI300X GPUs from Hot Aisle, we decided to skip smaller models and went with Llama 3.1 405B. To make the benchmark interesting, we tested how inference performance varied across different backends (vLLM and TGI) and use cases (real-time vs batch inference, different context sizes, etc.).</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#benchmark-setup","title":"Benchmark setup","text":"<p>Here is the spec of the bare metal machine we got:</p> <ul> <li>Intel\u00ae Xeon\u00ae Platinum 8470 2G, 52C/104T, 16GT/s, 105M Cache, Turbo, HT (350W) [x2]</li> <li>AMD MI300X GPU OAM 192GB 750W GPUs [x8]</li> <li>64GB RDIMM, 4800MT/s Dual Rank [x32]</li> </ul> Set up an SSH fleet <p>Hot Aisle provided us with SSH access to the machine. To make it accessible via <code>dstack</code>, we created an SSH fleet using the following configuration:</p> <p> <pre><code>type: fleet\nname: hotaisle-fleet\n\nplacement: any\n\nssh_config:\n  user: hotaisle\n  identity_file: ~/.ssh/hotaisle_id_rsa\n\n  hosts:\n    - hostname: ssh.hotaisle.cloud\n      port: 22013\n</code></pre> <p>After running <code>dstack apply -f hotaisle.dstack.yml</code>, we were ready to run dev environments, tasks, and services on this fleet via <code>datack</code>.</p> vLLM <pre><code>PyTorch version: 2.4.1+rocm6.1\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.1.0 24103 7db7f5e49612030319346f900c08f474b1f9023a)\nCMake version: version 3.26.4\nLibc version: glibc-2.35\n\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.1.40093\nMIOpen runtime version: 3.1.0\nIs XNNPACK available: True\n\nVersions of relevant libraries:\n[pip3] mypy==1.4.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] pytorch-triton-rocm==3.0.0\n[pip3] pyzmq==24.0.1\n[pip3] torch==2.4.1+rocm6.1\n[pip3] torchaudio==2.4.1+rocm6.1\n[pip3] torchvision==0.16.1+fdea156\n[pip3] transformers==4.45.1\n[pip3] triton==3.0.0\n[conda] No relevant packages\nROCM Version: 6.1.40091-a8dbc0c19\nNeuron SDK Version: N/A\nvLLM Version: 0.6.3.dev116+g151ef4ef\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n</code></pre> TGI <p>The <code>ghcr.io/huggingface/text-generation-inference:sha-11d7af7-rocm</code> Docker image was used.</p> <p>For conducting the tests, we've been using the <code>benchmark_serving</code>  provided by vLLM. </p>"},{"location":"blog/amd-mi300x-inference-benchmark/#observations","title":"Observations","text":""},{"location":"blog/amd-mi300x-inference-benchmark/#tokensec-per-batch-size","title":"Token/sec per batch size","text":"<p>TGI consistently exceeds vLLM in token throughput across all batch sizes, with the performance difference growing larger as the batch size increases. For batch sizes exceeding 64, the performance disparity becomes quite notable.</p> <p></p> <p>The prompts maintain a constant sequence length of 80 tokens each.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#ttft-per-batch-size","title":"TTFT per batch size","text":"<p>TGI surpasses vLLM in Time to First Token for all batch sizes, except for batch sizes 2 and 32.</p> <p></p> <p>The performance difference is considerable for larger batch sizes.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#tokensec-per-context-size","title":"Token/sec per context size","text":"<p>To evaluate performance with larger prompt sizes, we conducted tests using prompts of 10,000 tokens.</p> <p></p>"},{"location":"blog/amd-mi300x-inference-benchmark/#ttft-per-context-size","title":"TTFT per context size","text":"<p>In this case, TGI demonstrated an advantage over vLLM in both token throughput and time to first token (TTFT).</p> <p></p>"},{"location":"blog/amd-mi300x-inference-benchmark/#tokensec-and-ttft-per-rps","title":"Token/sec and TTFT per RPS","text":"<p>To assess the performance scalability of TGI and vLLM, we conducted tests by gradually increasing the Requests Per Second (RPS) and the total Requests Sent (RS) while keeping the prompt size consistent at 1,000 tokens for all trials. </p> <p>In this experiment, we initiated requests beginning with 30 requests at 1 RPS, then increased to 60 requests at 2 RPS, and continued this pattern up to 150 requests at 5 RPS. </p> <p></p> <p>Ideally, we would expect all trials to complete within the same time frame. However, due to resource limitations and increasing resource utilization, higher RPS does not lead to a proportional increase in throughput (tokens per second) or maintain Time to First Token (TTFT). </p> <p></p> <p>At 1 RPS, vLLM performs slightly better than TGI. However, between 2 and 4 RPS, TGI outperforms vLLM in both throughput and TTFT.</p> <p>Notably, TGI begins to drop requests once it reaches 5 RPS.</p> <p>We repeated the test using a higher number of requests, ranging from 300 to 900.</p> <p></p> <p>At 900 requests with a rate of 3 requests per second (RPS), TGI dropped a majority of the requests. However, its performance improved notably when the number of requests was below 900.</p> <p></p>"},{"location":"blog/amd-mi300x-inference-benchmark/#vram-consumption","title":"vRAM consumption","text":"<p>When considering vRAM consumption right after loading model weights, TGI allocates approximately 28% less vRAM compared to vLLM.</p> <p></p> <p>This difference may be related to how vLLM pre-allocates GPU cache .</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#conclusion","title":"Conclusion","text":"<ol> <li>For small sequence lengths, starting with a batch size of 64, TGI significantly outperforms vLLM in terms of throughput and TTFT.</li> <li>For larger sequence lengths, TGI outperforms vLLM even more in both throughput and TTFT, with the difference increasing as the batch size grows.</li> <li>At higher request rates, TGI continues to outperform vLLM, likely due to its superior ability to batch requests efficiently.</li> </ol> <p>Limitation</p> <ul> <li>In certain circumstances (e.g., at higher request rates), for unknown reasons, TGI dropped requests, making it   impossible to accurately track throughput and TTFT.</li> <li>With vLLM, we used the default backend configuration. With better tuning, we might have achieved improved performance.</li> </ul> <p>In general, the 8x AMD MI300X is a good fit for larger models and allows us to make the most of its vRAM, especially for larger batches.</p> <p>If you\u2019d like to support us in doing more benchmarks, please let us know.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#whats-next","title":"What's next?","text":"<p>While we wait for AMD to announce new GPUs and for data centers to offer them, we\u2019re considering tests with NVIDIA GPUs like the H100 and H200, as well as possibly Google TPU. </p> <p>Also, the next step is to measure how the FP8 version of the model would perform on this hardware.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#source-code","title":"Source code","text":"<p>The source code used for this benchmark can be found in our  GitHub repo .</p> <p>If you have questions, feedback, or want to help improve the benchmark, please reach out to our team.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#thanks-to-our-friends","title":"Thanks to our friends","text":""},{"location":"blog/amd-mi300x-inference-benchmark/#hot-aisle","title":"Hot Aisle","text":"<p>Hot Aisle   is the primary sponsor of this benchmark, and we are sincerely grateful for their hardware and support.  </p> <p>If you'd like to use top-tier bare metal compute with AMD GPUs, we recommend going with Hot Aisle. Once you gain access to a cluster, it can be easily accessed via <code>dstack</code>'s SSH fleet easily.</p>"},{"location":"blog/amd-mi300x-inference-benchmark/#runpod","title":"RunPod","text":"<p>If you\u2019d like to use on-demand compute with AMD GPUs at affordable prices, you can configure <code>dstack</code> to use RunPod . In this case, <code>dstack</code> will be able to provision fleets automatically when you run dev environments, tasks, and services.</p>"},{"location":"blog/amd-on-runpod/","title":"Supporting AMD accelerators on RunPod","text":"<p>While <code>dstack</code> helps streamline the orchestration of containers for AI, its primary goal is to offer vendor independence and portability, ensuring compatibility across different hardware and cloud providers.</p> <p>Inspired by the recent <code>MI300X</code> benchmarks, we are pleased to announce that RunPod is the first cloud provider to offer AMD GPUs through <code>dstack</code>, with support for other cloud providers and on-prem servers to follow.</p>"},{"location":"blog/amd-on-runpod/#specification","title":"Specification","text":"<p>For the reference, below is a comparison of the <code>MI300X</code> and <code>H100 SXM</code> specs, incl. the prices offered by RunPod.</p> MI300X H100X SXM On-demand pricing $3.99/hr $3.99/hr VRAM 192 GB 80GB Memory bandwidth 5.3 TB/s 3.4TB/s FP16 2,610 TFLOPs 1,979 TFLOPs FP8 5,220 TFLOPs 3,958 TFLOPs <p>One of the main advantages of the <code>MI300X</code> is its VRAM. For example, with the <code>H100 SXM</code>, you wouldn't be able to fit the FP16 version of Llama 3.1 405B into a single node with 8 GPUs\u2014you'd have to use FP8 instead. However, with the <code>MI300X</code>, you can fit FP16 into a single node with 8 GPUs, and for FP8, you'd only need 4 GPUs.</p> <p>With the latest update , you can now specify an AMD GPU under <code>resources</code>. Below are a few examples.</p>"},{"location":"blog/amd-on-runpod/#configuration","title":"Configuration","text":"ServiceDev environment <p>Here's an example of a service that deploys Llama 3.1 70B in FP16 using TGI .</p> <p> <pre><code>type: service\nname: amd-service-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct\n  - TRUST_REMOTE_CODE=true\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-70B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n</code></pre> <p>Here's an example of a dev environment using TGI 's Docker image:</p> <pre><code>type: dev-environment\nname: amd-dev-tgi\n\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\nenv:\n  - HF_TOKEN\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\nide: vscode\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n</code></pre> <p>Docker image</p> <p>Please note that if you want to use AMD, specifying <code>image</code> is currently required. This must be an image that includes ROCm drivers.</p> <p>To request multiple GPUs, specify the quantity after the GPU name, separated by a colon, e.g., <code>MI300X:4</code>.</p> <p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p> Control plane <p>If you specify <code>model</code> when running a service, <code>dstack</code> will automatically register the model on an OpenAI-compatible endpoint and allow you to use it for chat via the control plane UI.</p> <p></p>"},{"location":"blog/amd-on-runpod/#whats-next","title":"What's next?","text":"<ol> <li>The examples above demonstrate the use of TGI .  AMD accelerators can also be used with other frameworks like vLLM, Ollama, etc., and we'll be adding more examples soon.</li> <li>RunPod is the first cloud provider where dstack supports AMD. More cloud providers will be supported soon as well.</li> <li>Want to give RunPod and <code>dstack</code> a try? Make sure you've signed up for RunPod ,     then set up the <code>dstack server</code>. </li> </ol> <p>Have questioned or feedback? Join our Discord   server.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/","title":"Beyond Kubernetes: 2024 recap and what's ahead for AI infra","text":"<p>At <code>dstack</code>, we aim to simplify AI model development, training, and deployment of AI models by offering an alternative to the complex Kubernetes ecosystem. Our goal is to enable seamless AI infrastructure management across any cloud or hardware vendor. </p> <p>As 2024 comes to a close, we reflect on the milestones we've achieved and look ahead to the next steps.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#ecosystem","title":"Ecosystem","text":"<p>While <code>dstack</code> integrates with leading cloud GPU providers, we aim to expand partnerships with more providers  sharing our vision of simplifying AI infrastructure orchestration with a lightweight, efficient alternative to Kubernetes.</p> <p>This year, we\u2019re excited to welcome our first partners: Lambda ,  RunPod ,  CUDO Compute ,  and Hot Aisle .</p> <p>We\u2019d also like to thank Oracle    for their collaboration, ensuring seamless integration between <code>dstack</code> and OCI.</p> <p>Special thanks to Lambda  and Hot Aisle  for providing NVIDIA and AMD hardware, enabling us conducting  benchmarks, which are essential to advancing open-source inference and training stacks for all accelerator chips.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#community","title":"Community","text":"<p>Thanks to your support, the project has reached 1.6K stars on GitHub , reflecting the growing interest and trust in its mission. Your issues, pull requests, as well as feedback on Discord , play a critical role in the project's development.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#fleets","title":"Fleets","text":"<p>A key milestone for <code>dstack</code> this year has been the introduction of fleets,  an abstraction that simplifies the management of clusters.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#cloud-providers","title":"Cloud providers","text":"<p>Unlike Kubernetes, where node groups are typically managed through auto-scaling policies, <code>dstack</code> offers a more streamlined approach. With <code>dstack</code>, you simply define a fleet YAML file and run <code>dstack apply</code>. This command automatically provisions clusters across any cloud provider.</p> <p>For quick deployments, you can skip defining a fleet altogether. When you run a dev environment, task, or service, <code>dstack</code> creates a fleet automatically.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#on-prem-server","title":"On-prem server","text":"<p>Managing on-prem resources with <code>dstack</code>'s fleets is equally straightforward. If you have SSH access to a group of hosts, simply list them in a YAML configuration file and run <code>dstack apply</code>.</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\n# Ensure instances are inter-connected\nplacement: cluster\n\n# The user, private SSH key, and hostnames of the on-prem servers\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre> <p>This turns your on-prem cluster into a <code>dstack</code> fleet, ready to run dev environments, tasks, and services.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#gpu-blocks","title":"GPU blocks","text":"<p>At <code>dstack</code>, when running a job on an instance, it uses all available GPUs on that instance. In Q1 2025, we will introduce GPU blocks , allowing the allocation of instance GPUs into discrete blocks that can be reused by concurrent jobs.</p> <p>This will enable more cost-efficient utilization of expensive instances.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#volumes","title":"Volumes","text":"<p>Another key milestone for <code>dstack</code> this year has been the introduction of volumes, addressing a critical need in AI infrastructure\u2014data storage.</p> <p>With <code>dstack</code>'s volumes, users can now leverage storage in both cloud and on-prem environments in a unified and efficient manner.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#accelerators","title":"Accelerators","text":""},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#nvidia","title":"NVIDIA","text":"<p>NVIDIA remains the top accelerator supported by <code>dstack</code>. Recently, we introduced a NIM example  for model deployment, and we continue to enhance support for the rest of NVIDIA's ecosystem.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#amd","title":"AMD","text":"<p>This year, we\u2019re particularly proud of our newly added integration with AMD.</p> <p><code>dstack</code> works seamlessly with any on-prem AMD clusters. For example, you can rent such servers through our partner  Hot Aisle .</p> <p>Among cloud providers, AMD  is supported only through RunPod. In Q1 2025, we plan to extend it to Nscale , Hot Aisle , and potentially other providers open to collaboration.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#intel","title":"Intel","text":"<p>In Q1 2025, our roadmap includes added integration with  Intel Gaudi  among other accelerator chips.</p>"},{"location":"blog/beyond-kubernetes-2024-recap-and-whats-ahead/#join-the-community","title":"Join the community","text":"<p>If you're interested in simplifying AI infrastructure, both in the cloud and on-prem, consider getting involved as a  <code>dstack</code> user, open-source contributor, or ambassador.</p> <p>Finally, if you're a cloud, hardware, or software vendor, consider contributing to <code>dstack</code> and helping us drive it as an open standard together.</p>"},{"location":"blog/docker-inside-containers/","title":"Using Docker and Docker Compose inside GPU-enabled containers","text":"<p>To run containers with <code>dstack</code>, you can use your own Docker image (or the default one) without a need to interact directly with Docker. However, some existing code may require direct use of Docker or Docker Compose. That's why, in our latest release, we've added this option.</p> <pre><code>type: task\nname: chat-ui-task\n\nimage: dstackai/dind\nprivileged: true\n\nworking_dir: examples/misc/docker-compose\ncommands:\n  - start-dockerd\n  - docker compose up\nports: [9000]\n\nresources:\n  gpu: 16GB..24GB\n</code></pre>"},{"location":"blog/docker-inside-containers/#how-it-works","title":"How it works","text":"<p>To use Docker or Docker Compose with your <code>dstack</code> configuration, set <code>image</code> to <code>dstackai/dind</code>, <code>privileged</code> to  <code>true</code>, and add the <code>start-dockerd</code> command. After this command, you can use Docker or Docker Compose directly.</p> <p>For dev environments, add <code>start-dockerd</code> as the first command in the <code>init</code> property.</p> Dev environment <pre><code>type: dev-environment\nname: vscode-dind\n\nimage: dstackai/dind\nprivileged: true\n\nide: vscode\ninit:\n  - start-dockerd\n\nresources:\ngpu: 16GB..24GB\n</code></pre> <p>The <code>start-dockerd</code> script is part of the <code>dstackai/dind</code> image, a pre-built image by <code>dstack</code> that enables Docker to run inside containers.</p> <p>With this setup, you don\u2019t have to worry about configuration\u2014both Docker and Docker Compose work out of the box and support GPU usage.</p> <p>Backends</p> <p>Note that the <code>privileged</code> option is only supported by VM-based backends. This does not include <code>runpod</code>, <code>vastai</code>,  and <code>kubernetes</code>. All other backends support it.</p>"},{"location":"blog/docker-inside-containers/#when-using-it","title":"When using it","text":""},{"location":"blog/docker-inside-containers/#docker-compose","title":"docker compose","text":"<p>One of the obvious use cases for this feature is when you need to use Docker Compose.  For example, the Hugging Face Chat UI requires a MongoDB database, so using Docker Compose to run it is  the easiest way:</p> <p></p>"},{"location":"blog/docker-inside-containers/#docker-build","title":"docker build","text":"<p>Another use case for this feature is when you need to build a custom Docker image using the <code>docker build</code> command.</p>"},{"location":"blog/docker-inside-containers/#docker-run","title":"docker run","text":"<p>Last but not least, you can, of course, use the <code>docker run</code> command, for example, if your existing code requires it.</p>"},{"location":"blog/docker-inside-containers/#examples","title":"Examples","text":"<p>A few examples of using this feature can be found in <code>examples/misc/docker-compose</code> .</p>"},{"location":"blog/docker-inside-containers/#feedback","title":"Feedback","text":"<p>If you find something not working as intended, please be sure to report it to our bug tracker .  Your feedback and feature requests are also very welcome on both  Discord  and the issue tracker .</p>"},{"location":"blog/dstack-sky-own-cloud-accounts/","title":"dstack Sky now supports your own cloud accounts","text":"<p>dstack Sky   enables you to access GPUs from the global marketplace at the most competitive rates. However, sometimes you may want to use your own cloud accounts.  With today's release, both options are now supported.</p> <p></p>"},{"location":"blog/dstack-sky-own-cloud-accounts/#configure-backends","title":"Configure backends","text":"<p>To use your own cloud account, open the project settings and edit the corresponding backend.</p> <p></p> <p>You can configure your cloud accounts for any of the supported providers, including AWS, GCP, Azure, TensorDock, Lambda, CUDO, RunPod, and Vast.ai.</p> <p>Additionally, you can disable certain backends if you do not plan to use them.</p> <p>Typically, if you prefer using your own cloud accounts, it's recommended that you use the  open-source version  of <code>dstack</code>. However, if you prefer not to host it yourself, now you can use <code>dstack Sky</code>  with your own cloud accounts as well.</p> <p>Seeking the cheapest on-demand and spot cloud GPUs? dstack Sky  has you covered!</p> <p>Need help, have a question, or just want to stay updated?</p> <p>Join Discord</p>"},{"location":"blog/dstack-sky/","title":"Introducing dstack Sky","text":"<p>Today we're previewing <code>dstack Sky</code>, a service built on top of  <code>dstack</code> that enables you to get GPUs at competitive rates from a wide pool of providers.</p> <p></p>"},{"location":"blog/dstack-sky/#tldr","title":"TL;DR","text":"<ul> <li>GPUs at competitive rates from multiple providers</li> <li>No need for your own cloud accounts</li> <li>Compatible with <code>dstack</code>'s CLI and API</li> <li>A pre-configured gateway for deploying services</li> </ul>"},{"location":"blog/dstack-sky/#introduction","title":"Introduction","text":"<p><code>dstack</code> is an open-source tool designed for managing AI infrastructure across various cloud platforms. It's lighter and more specifically geared towards AI tasks compared to Kubernetes.</p> <p>Due to its support for multiple cloud providers, <code>dstack</code> is frequently used to access on-demand and spot GPUs  across multiple clouds.  From our users, we've learned that managing various cloud accounts, quotas, and billing can be cumbersome.</p> <p>To streamline this process, we introduce <code>dstack Sky</code>, a managed service that enables users to access GPUs from multiple providers through <code>dstack</code> \u2013 without needing an account in each cloud provider. </p>"},{"location":"blog/dstack-sky/#what-is-dstack-sky","title":"What is dstack Sky?","text":"<p>Instead of running <code>dstack server</code> yourself, you point <code>dstack config</code> to a project set up with <code>dstack Sky</code>.</p> <pre><code>$ dstack config --url https://sky.dstack.ai \\\n    --project my-awesome-project \\\n    --token ca1ee60b-7b3f-8943-9a25-6974c50efa75\n</code></pre> <p>Now, you can use <code>dstack</code>'s CLI or API \u2013 just like you would with your own cloud accounts.</p> <pre><code>$ dstack run . -b tensordock -b vastai\n\n #  BACKEND     REGION  RESOURCES                    SPOT  PRICE \n 1  vastai      canada  16xCPU/64GB/1xRTX4090/1TB    no    $0.35\n 2  vastai      canada  16xCPU/64GB/1xRTX4090/400GB  no    $0.34\n 3  tensordock  us      8xCPU/48GB/1xRTX4090/480GB   no    $0.74\n    ...\n Shown 3 of 50 offers, $0.7424 max\n\nContinue? [y/n]:\n</code></pre> <p>Backends</p> <p><code>dstack Sky</code> supports the same backends as the open-source version, except that you don't need to set them up. By default, it uses all supported backends.</p> <p>You can use both on-demand and spot instances without needing to manage quotas, as they are automatically handled for you.</p> <p>With <code>dstack Sky</code> you can use all of <code>dstack</code>'s features, incl. dev environments,  tasks, services, and  fleets.</p> <p>To publish services, the open-source version requires setting up a gateway with your own domain.  <code>dstack Sky</code> comes with a pre-configured gateway.</p> <pre><code>$ dstack gateway list\n BACKEND  REGION     NAME    ADDRESS       DOMAIN                            DEFAULT\n aws      eu-west-1  dstack  3.252.79.143  my-awesome-project.sky.dstack.ai  \u2713\n</code></pre> <p>If you run it with <code>dstack Sky</code>, the service's endpoint will be available at <code>https://&lt;run name&gt;.&lt;project name&gt;.sky.dstack.ai</code>.</p> <p>Let's say we define a service:</p> <pre><code>type: service\n# Deploys Mixtral 8x7B with Ollama\n\n# Serve model using Ollama's Docker image\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull mixtral\n  - fg\nport: 11434\n\n# Configure hardware requirements\nresources:\n  gpu: 48GB..80GB\n\n# Enable OpenAI compatible endpoint\nmodel: mixtral\n</code></pre> <p>If it has a <code>model</code> mapping, the model will be accessible at <code>https://gateway.&lt;project name&gt;.sky.dstack.ai</code> via the OpenAI compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.&lt;project name&gt;.sky.dstack.ai\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mixtral\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Now, you can choose \u2014 either use <code>dstack</code> via the open-source version or via <code>dstack Sky</code>,  or even use them side by side.</p> <p>Credits</p> <p>Are you an active contributor to the AI community? Request free <code>dstack Sky</code> credits.</p> <p><code>dstack Sky</code>  is live on Product Hunt. Support it by giving it your vote!</p> <p>Join Discord</p>"},{"location":"blog/h100-mi300x-inference-benchmark/","title":"Exploring inference memory saturation effect: H100 vs MI300x","text":"<p>GPU memory plays a critical role in LLM inference, affecting both performance and cost. This benchmark evaluates memory saturation\u2019s impact on inference using NVIDIA's H100 and AMD's MI300x with Llama 3.1 405B FP8.</p> <p>We examine the effect of limited parallel computational resources on throughput and Time to First Token (TTFT). Additionally, we compare deployment strategies: running two Llama 3.1 405B FP8 replicas on 4xMI300x versus a single replica on 4xMI300x and 8xMI300x</p> <p>Finally, we extrapolate performance projections for upcoming GPUs like NVIDIA H200, B200, and AMD MI325x, MI350x.</p> <p></p> <p>This benchmark is made possible through the generous support of our friends at Hot Aisle  and  Lambda , who provided high-end hardware.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#benchmark-setup","title":"Benchmark setup","text":"<ol> <li>AMD 8xMI300x<ul> <li>2x Intel Xeon Platinum 8470, 52C/104T, 16GT/s, 105M Cache (350W)</li> <li>8x AMD MI300x GPU OAM, 192GB, 750W</li> <li>32x 64GB RDIMM, 4800MT/s</li> </ul> </li> <li>NVIDIA 8xH100 SXM5<ul> <li>2\u00d7 Intel Xeon Platinum 8480+, 56C/112T, 16GT/s, 105M Cache (350W)</li> <li>8\u00d7 NVIDIA H100 SXM5 GPU, 80GB, 700W</li> <li>32x 64GB DDR5</li> </ul> </li> </ol>"},{"location":"blog/h100-mi300x-inference-benchmark/#benchmark-modes","title":"Benchmark modes","text":"<ol> <li>Online inference: Benchmarked across QPS 16, 32, and 1000 using    the ShareGPT  dataset. Execution used    vLLM\u2019s benchmark_serving.</li> <li>Offline inference: Benchmarked with varying input/output lengths across different batch sizes, using vLLM\u2019s benchmark_throughput.py.</li> </ol> Input prompt lengths Batch size Short/Small 4 to 1024 Short/Large 128 256 Large/Large 32784 64 (MI300x) / 16 (H100)"},{"location":"blog/h100-mi300x-inference-benchmark/#observations","title":"Observations","text":""},{"location":"blog/h100-mi300x-inference-benchmark/#cost-per-token","title":"Cost per token","text":"<p>As prompt and batch sizes grow, the NVIDIA H100 reaches memory limits, causing a sharp drop in cost-effectiveness. In contrast, the 1 FP8 8xMI300x configuration is the most cost-efficient for large prompts.</p> <p>For large prompts, two parallel replicas running on 4xMI300x lose their cost advantage compared to a single replica on 8xMI300x. The latter offers 51% more memory for the KV cache, improving throughput and reducing cost per token.</p> <p></p> <p>While 4xMI300x is a cost-effective alternative to 8xH100 for smaller load profiles, it underperforms in online serving. 8xH100 SXM5 processes 74% more requests per second and reduces TTFT by at least 50% at all QPS levels.</p> <p></p>"},{"location":"blog/h100-mi300x-inference-benchmark/#throughput","title":"Throughput","text":"<p>With large prompts and batch sizes, two replicas on 4xMI300x GPUs hit memory saturation when total tokens (prompt length x batch size) exceed the available memory for the KV cache. This forces the inference engine to compute KV tensors on-the-fly or offload them to CPU memory, degrading throughput.</p> <p>In Lambda \u2019 benchmark, an 8xH200 setup processed 3.4 times more tokens per second than an 8xH100. Extrapolating to our setup, an 8xH200 would process around 2,186 tokens per second (3.4 \u00d7 643), though still lower than 8xMI300x.</p> AMD MI300x NVIDIA H200 GPU Memory 192 GB 141 GB Memory Type HBM3 HBM3e Peak Memory Bandwidth 5.3TB/s 4.8TB/s TFLOPS (FP8) 2610 1979"},{"location":"blog/h100-mi300x-inference-benchmark/#replicas-on-4xmi300x","title":"Replicas on 4xMi300x","text":"<p>Running two replicas on 4xMI300x delivers better throughput for small to medium prompts than a single replica on 8xMI300x. </p> <p></p> <p>This boost comes from distributing the Llama 3.1 405B model across four GPUs, enabling parallel execution. For small prompts, a single replica underutilizes the GPUs. Running two replicas doubles the batch size, improving GPU utilization and efficiency.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#time-to-first-token","title":"Time To First Token","text":"<p>The 4xMI300x setup provides 768 GB of memory (4 GPUs \u00d7 192 GB each), compared to 640 GB with 8xH100 (8 GPUs \u00d7 80 GB each). However, at 1000 QPS, TTFT for 4xMI300x is over twice as long as for 8xH100</p> <p>This difference occurs during the prefill stage, where KV tensors for input prompts are computed. Since tensors are processed in parallel, the 8xH100 configuration distributes the load more effectively, reducing computation time.</p> <p>Despite offering more memory, 4xMI300x lacks the parallelism of 8xH100, leading to longer TTFT.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#time-to-serve-1-request","title":"Time to Serve 1 Request","text":"<p>Processing a single large prompt request with 8xMI300x takes around 11.25 seconds. This latency is mainly due to computational demands during the prefill phase, where KV tensors are computed.</p> <p>Optimizations like automatic prefix caching  could help reduce this time, but are outside the scope of this benchmark.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#benchmark-notes","title":"Benchmark notes","text":""},{"location":"blog/h100-mi300x-inference-benchmark/#benchmark-setup_1","title":"Benchmark setup","text":"<p>The script used in this benchmark was designed for large prompts in offline inference. A different script tailored for online inference would provide more accurate insights.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#batch-size","title":"Batch size","text":"<p>We compared throughput at batch size 16 for 8xH100 and batch size 64 for 8xMI300x. The 8xH100 setup begins to struggle with batch size 16 due to memory saturation, resulting in slower generation times.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#model-checkpoints","title":"Model checkpoints","text":"<p>For AMD MI300x, we used <code>amd/Llama-3.1-405B-Instruct-FP8-KV</code>  to achieve optimal performance, relying on AMD for quantization.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#vllm-configuration","title":"vLLM configuration","text":"<p>To maximize inference results on AMD MI300x, we adjusted specific arguments:</p> <pre><code>$ VLLM_RPC_TIMEOUT=30000 VLLM_USE_TRITON_FLASH_ATTN=0 vllm serve \\\n  meta-llama/Llama-3.1-405B-FP8 -tp 8 \\\n  --max-seq-len-to-capture 16384 \\ \n  --served-model-name meta-llama/Llama-3.1-405B-FP8 \\ \n  --enable-chunked-prefill=False \\\n  --num-scheduler-step 15 \\\n  --max-num-seqs 1024\n</code></pre> <p>Our benchmark focused on testing inference with tensor parallelism. Integrating tensor and pipeline parallelism could provide additional insights.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#on-b200-mi325x-and-mi350x","title":"On B200, MI325x, and MI350x","text":"<p>The MI325x offers 64GB more HBM and 0.7TB/s higher bandwidth than MI300x. However, because it has the same FP8 TFLOPS, it doesn't provide significant compute gains, positioning it against NVIDIA's H200.</p> <p>The NVIDIA B200 outperforms MI300x and MI325x with more TFLOPS and higher peak memory bandwidth, resulting in lower TTFT by reducing compute time for KV tensors and memory transfer times during the decode stage. We expect the B200 to challenge MI325x, as long as memory saturation is avoided.</p> <p>Notably, future GPUs from AMD and NVIDIA are expected to support FP4 and FP6, improving throughput, latency, and cost-efficiency.</p> AMD MI300x AMD MI325x AMD MI350x NVIDIA B200 GPU Memory 192 GB 256 GB 288GB 192 GB Memory Type HBM3 HBM3e HBM3e Peak Memory Bandwidth 5.3TB/s 6TB/s 8TB/s TFLOPS (FP8) 2610 2610 4500 Low precision FP8 FP8 FP4, FP6, FP8 FP4, FP6, FP8"},{"location":"blog/h100-mi300x-inference-benchmark/#source-code","title":"Source code","text":"<p>All the source code and findings to help you replicate the results are available in  our GitHub repo .</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#thanks-to-our-friends","title":"Thanks to our friends","text":""},{"location":"blog/h100-mi300x-inference-benchmark/#hot-aisle","title":"Hot Aisle","text":"<p>Hot Aisle  sponsored this benchmark by providing access to 8x MI300x hardware. We\u2019re deeply grateful for their support.</p> <p>If you're looking for top-tier bare metal compute with AMD GPUs, we highly recommend Hot Aisle. With <code>dstack</code>, accessing your cluster via SSH is seamless and straightforward.</p>"},{"location":"blog/h100-mi300x-inference-benchmark/#lambda","title":"Lambda","text":"<p>Lambda  sponsored this benchmark with credits for on-demand 8x H100 instances.  We\u2019re truly thankful for their support.</p> <p>For top-tier cloud compute with NVIDIA GPUs, Lambda is an excellent choice. Once set up, you can easily provision compute, manage clusters, and orchestrate your AI workloads using <code>dstack</code>.</p>"},{"location":"blog/instance-volumes/","title":"Introducing instance volumes to persist data on instances","text":""},{"location":"blog/instance-volumes/#how-it-works","title":"How it works","text":"<p>Until now, <code>dstack</code> supported data persistence only with network volumes, managed by clouds. While convenient, sometimes you might want to use a simple cache on the instance or  mount an NFS share to your SSH fleet. To address this, we're now introducing instance volumes that work for both cases.</p> <pre><code>type: task \nname: llama32-task\n\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Llama-3.2-3B-Instruct\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL_ID --max-model-len 4096\nports: [8000]\n\nvolumes:\n  - /root/.dstack/cache:/root/.cache\n\nresources:\n  gpu: 16GB..\n</code></pre> <p>Instance volumes work with both SSH fleets and cloud fleets, and it is possible to mount any folders on the instance, whether they are regular folders or NFS share mounts.</p> <p>The configuration above mounts <code>/root/.dstack/cache</code> on the instance to <code>/root/.cache</code> inside container.</p>"},{"location":"blog/instance-volumes/#caching","title":"Caching data on fleet instances","text":"<p>If you use a folder on the instance that is not an NFS mount, instance volumes can only be used for caching purposes, as their state is bound to a particular instance while it's up.</p> <p>Caching can be especially useful if you want to re-run the same configuration on the same fleet and avoid downloading very large models, datasets, or dependencies with each run.</p>"},{"location":"blog/instance-volumes/#nfs","title":"Using NFS with SSH and cloud fleets","text":"<p>If you want to replicate the state across instances, you can mount an NFS share to the instance folder.</p> <p>With SSH fleets, it's easy to set up an NFS share, as you can do it when logging into your hosts via SSH. If you'd like to mount NFS with your cloud fleets, you will need to use a custom AMI for that.</p> <p>Here's an example of a dev environment that mounts the <code>data</code> folder from an NFS share, which is mounted to <code>/mnt/nfs-storage</code> on the instance, to the <code>/data</code> folder inside the container.</p> <pre><code>type: dev-environment\nname: vscode-nfs\n\nide: vscode\n\nvolumes:\n  - /mnt/nfs-storage/data:/data\n</code></pre>"},{"location":"blog/instance-volumes/#feedback","title":"Feedback","text":"<p>If you find something not working as intended, please be sure to report it to GitHub issues .  Your feedback and feature requests is also very welcome on our  Discord  server.</p>"},{"location":"blog/monitoring-gpu-usage/","title":"Monitoring GPU usage and other container metrics","text":""},{"location":"blog/monitoring-gpu-usage/#how-it-works","title":"How it works","text":"<p>While it's possible to use third-party monitoring tools with <code>dstack</code>, it is often more convenient to debug your run and track metrics out of the box. That's why, with the latest release, <code>dstack</code> introduced <code>dstack stats</code>, a new CLI (and API) for monitoring container metrics, including GPU usage for <code>NVIDIA</code>, <code>AMD</code>, and other accelerators.</p> <p></p> <p>The command is similar to <code>kubectl top</code> (in terms of semantics) and <code>docker stats</code> (in terms of the CLI interface). The key difference is that <code>dstack stats</code> includes GPU VRAM usage and GPU utilization percentage. </p> <p>The feature works right away with <code>NVIDIA</code> and <code>AMD</code>, whether you're running a development environment, task, or service. <code>TPU</code> support is coming soon.</p> <p>Similar to <code>kubectl top</code>, if a run consists of multiple jobs (such as distributed training or an auto-scalable service), <code>dstack stats</code> will display metrics per job.</p> <p>REST API</p> <p>In addition to the <code>dstack stats</code> CLI commands, metrics can be obtained via the <code>/api/project/{project_name}/metrics/job/{run_name}</code> REST endpoint.</p>"},{"location":"blog/monitoring-gpu-usage/#why-monitor-gpu-usage","title":"Why monitor GPU usage","text":"<p>Kubernetes and Docker don\u2019t offer built-in support for GPU usage tracking. Since <code>dstack</code> is tailored for AI containers, we consider native GPU monitoring essential. </p>"},{"location":"blog/monitoring-gpu-usage/#gpu-usage","title":"GPU  usage","text":"<p>Monitoring GPU memory usage in AI workloads helps prevent out-of-memory errors and provides a clearer picture of how much memory is actually used or needed by the workload.</p>"},{"location":"blog/monitoring-gpu-usage/#gpu-utilization","title":"GPU utilization","text":"<p>Monitoring GPU utilization is important for identifying under-utilization and ensuring that workloads are distributed evenly across GPUs.</p>"},{"location":"blog/monitoring-gpu-usage/#roadmap","title":"Roadmap","text":"<p>Monitoring is a critical part of observability, and we have many more features on our roadmap:</p> <ul> <li>Potentially adding more metrics, including disk usage, I/O, network, etc</li> <li>Support for the TPU accelerator</li> <li>Displaying historical metrics within the control plane UI</li> <li>Tracking deployment metrics, including LLM-related metrics</li> <li>A simple way to export metrics to Prometheus</li> </ul>"},{"location":"blog/monitoring-gpu-usage/#feedback","title":"Feedback","text":"<p>If you find something not working as intended, please be sure to report it to our bug tracker .  Your feedback and feature requests are also very welcome on both  Discord  and the issue tracker .</p>"},{"location":"blog/tpu-on-gcp/","title":"Using TPUs for fine-tuning and deploying LLMs","text":"<p>If you\u2019re using or planning to use TPUs with Google Cloud, you can now do so via <code>dstack</code>. Just specify the TPU version and the number of cores  (separated by a dash), in the <code>gpu</code> property under <code>resources</code>. </p> <p>Read below to find out how to use TPUs with <code>dstack</code> for fine-tuning and deploying LLMs, leveraging open-source tools like Hugging Face\u2019s  Optimum TPU   and vLLM .</p> <p>Below is an example of a dev environment:</p> <pre><code>type: dev-environment\nname: vscode-tpu    \n\npython: 3.11\nide: vscode\n\nresources:\n  gpu: v2-8\n</code></pre> <p>If you've configured the <code>gcp</code> backend, <code>dstack</code> will automatically provision the dev environment with a TPU.</p> <p>Currently, maximum 8 TPU cores can be specified, so the maximum supported values are <code>v2-8</code>, <code>v3-8</code>, <code>v4-8</code>, <code>v5litepod-8</code>,  and <code>v5e-8</code>. Multi-host TPU support, allowing for larger numbers of cores, is coming soon.</p>"},{"location":"blog/tpu-on-gcp/#deployment","title":"Deployment","text":"<p>You can use any serving framework, such as vLLM, TGI. Here's an example of a service that deploys Llama 3.1 8B using  Optimum TPU  and vLLM .</p> Optimum TPUvLLM <p> <p><pre><code>type: service\nname: llama31-service-optimum-tpu\n\nimage: dstackai/optimum-tpu:llama31\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_TOTAL_TOKENS=4096\n  - MAX_BATCH_PREFILL_TOKENS=4095\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n# Register the model\nmodel:\n  format: tgi\n  type: chat\n  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: v5litepod-4 \n</code></pre> </p> <p>Once the pull request  is merged,  the official Docker image can be used instead of <code>dstackai/optimum-tpu:llama31</code>.</p> <p> <p><pre><code>type: service\nname: llama31-service-vllm-tpu\n\nenv:\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - HF_TOKEN\n  - DATE=20240828\n  - TORCH_VERSION=2.5.0\n  - VLLM_TARGET_DEVICE=tpu\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n  - git clone https://github.com/vllm-project/vllm.git\n  - cd vllm\n  - pip install -r requirements-tpu.txt\n  - apt-get install -y libopenblas-base libopenmpi-dev libomp-dev\n  - python setup.py develop\n  - vllm serve $MODEL_ID \n      --tensor-parallel-size 4 \n      --max-model-len $MAX_MODEL_LEN\n      --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: v5litepod-4\n</code></pre> </p> Control plane <p>If you specify <code>model</code> when running a service, <code>dstack</code> will automatically register the model on an OpenAI-compatible endpoint and allow you to use it for chat via the control plane UI.</p> <p></p>"},{"location":"blog/tpu-on-gcp/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for serving LLMs with their corresponding TPUs. </p> Model size bfloat16 TPU int8 TPU 8B 16GB v5litepod-4 8GB v5litepod-4 70B 140GB v5litepod-16 70GB v5litepod-16 405B 810GB v5litepod-64 405GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for serving transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"blog/tpu-on-gcp/#supported-frameworks","title":"Supported frameworks","text":"Framework Quantization Note TGI bfloat16 To deploy with TGI, Optimum TPU must be used. vLLM int8, bfloat16 int8 quantization still requires the same memory because the weights are first moved to the TPU in bfloat16, and then converted to int8. See the pull request  for more details."},{"location":"blog/tpu-on-gcp/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p>"},{"location":"blog/tpu-on-gcp/#fine-tuning","title":"Fine-tuning","text":"<p>Below is an example of fine-tuning Llama 3.1 8B using Optimum TPU   and the Abirate/english_quotes  dataset.</p> <pre><code>type: task\nname: optimum-tpu-llama-train\n\npython: \"3.11\"\n\nenv:\n  - HF_TOKEN\ncommands:\n  - git clone -b add_llama_31_support https://github.com/dstackai/optimum-tpu.git\n  - mkdir -p optimum-tpu/examples/custom/\n  - cp examples/fine-tuning/optimum-tpu/llama31/train.py optimum-tpu/examples/custom/train.py\n  - cp examples/fine-tuning/optimum-tpu/llama31/config.yaml optimum-tpu/examples/custom/config.yaml\n  - cd optimum-tpu\n  - pip install -e . -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install datasets evaluate\n  - pip install accelerate -U\n  - pip install peft\n  - python examples/custom/train.py examples/custom/config.yaml\n\n\nresources:\n  gpu: v5litepod-8\n</code></pre>"},{"location":"blog/tpu-on-gcp/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning LLMs with their corresponding TPUs.</p> Model size LoRA TPU 8B 16GB v5litepod-8 70B 160GB v5litepod-16 405B 950GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for fine-tuning transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"blog/tpu-on-gcp/#supported-frameworks_1","title":"Supported frameworks","text":"Framework Quantization Note TRL bfloat16 To fine-tune using TRL, Optimum TPU is recommended. TRL doesn't support Llama 3.1 out of the box. Pytorch XLA bfloat16"},{"location":"blog/tpu-on-gcp/#whats-next","title":"What's next?","text":"<ol> <li>Browse Optimum TPU ,    Optimum TPU TGI  and    vLLM .</li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol> <p>Multi-host TPUs</p> <p>If you\u2019d like to use <code>dstack</code> with more than eight TPU cores, upvote the corresponding issue .</p>"},{"location":"blog/volumes-on-runpod/","title":"Using volumes to optimize cold starts on RunPod","text":"<p>Deploying custom models in the cloud often faces the challenge of cold start times, including the time to provision a new instance and download the model. This is especially relevant for services with autoscaling when new model replicas need to be provisioned quickly. </p> <p>Let's explore how <code>dstack</code> optimizes this process using volumes, with an example of deploying a model on RunPod.</p> <p>Suppose you want to deploy Llama 3.1 on RunPod as a service:</p> <pre><code>type: service\nname: llama31-service-tgi\n\nreplicas: 1..2\nscaling:\n  metric: rps\n  target: 30\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - text-generation-launcher\nport: 80\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: 24GB\n</code></pre> <p>When you run <code>dstack apply</code>, it creates a public endpoint with one service replica. <code>dstack</code> will then automatically scale the service by adjusting the number of replicas based on traffic.</p> <p>When starting each replica, <code>text-generation-launcher</code> downloads the model to the <code>/data</code> folder. For Llama 3.1 8B, this usually takes under a minute, but larger models may take longer. Repeated downloads can significantly affect auto-scaling efficiency.</p> <p>Great news: RunPod supports network volumes, which we can use for caching models across multiple replicas.</p> <p>With <code>dstack</code>, you can create a RunPod volume using the following configuration:</p> <pre><code>type: volume\nname: llama31-volume\n\nbackend: runpod\nregion: EU-SE-1\n\n# Required size\nsize: 100GB\n</code></pre> <p>Go ahead and create it via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/mist/volumes/runpod.dstack.yml\n</code></pre> <p>Once the volume is created, attach it to your service by updating the configuration file and mapping the  volume name to the <code>/data</code> path.</p> <pre><code>type: service\nname: llama31-service-tgi\n\nreplicas: 1..2\nscaling:\n  metric: rps\n  target: 30\n\nvolumes:\n - name: llama31-volume\n   path: /data\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - text-generation-launcher\nport: 80\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: 24GB\n</code></pre> <p>In this case, <code>dstack</code> attaches the specified volume to each new replica. This ensures the model is downloaded only once, reducing cold start time in proportion to the model size.</p> <p>A notable feature of RunPod is that volumes can be attached to multiple containers simultaneously. This capability is particularly useful for auto-scalable services or distributed tasks.</p> <p>Using volumes not only optimizes inference cold start times but also enhances the efficiency of data and model checkpoint loading during training and fine-tuning. Whether you're running tasks or dev environments, leveraging volumes can significantly streamline your workflow and improve overall performance.</p>"},{"location":"changelog/","title":"Blog","text":""},{"location":"docs/","title":"What is dstack?","text":"<p><code>dstack</code> is a streamlined alternative to Kubernetes and Slurm, specifically designed for AI. It simplifies container orchestration for AI workloads both in the cloud and on-prem, speeding up the development, training, and deployment of AI models.</p> <p><code>dstack</code> is easy to use with any cloud providers as well as on-prem servers. </p>"},{"location":"docs/#accelerators","title":"Accelerators","text":"<p><code>dstack</code> supports <code>NVIDIA</code>, <code>AMD</code>, <code>Google TPU</code>, and <code>Intel Gaudi</code> accelerators out of the box.</p>"},{"location":"docs/#how-does-it-work","title":"How does it work?","text":""},{"location":"docs/#1-set-up-the-server","title":"1. Set up the server","text":"<p>Before using <code>dstack</code>, ensure you've installed the server, or signed up for dstack Sky .</p>"},{"location":"docs/#2-define-configurations","title":"2. Define configurations","text":"<p><code>dstack</code> supports the following configurations:</p> <ul> <li>Dev environments \u2014 for interactive development using a desktop IDE</li> <li>Tasks \u2014 for scheduling jobs, incl. distributed ones (or running web apps)</li> <li>Services \u2014 for deploying models (or web apps)</li> <li>Fleets \u2014 for managing cloud and on-prem clusters</li> <li>Volumes \u2014 for managing network volumes (to persist data)</li> <li>Gateways \u2014 for publishing services with a custom domain and HTTPS</li> </ul> <p>Configuration can be defined as YAML files within your repo.</p>"},{"location":"docs/#3-apply-configurations","title":"3. Apply configurations","text":"<p>Apply the configuration either via the <code>dstack apply</code> CLI command (or through a programmatic API.)</p> <p><code>dstack</code> automatically manages infrastructure provisioning and job scheduling, while also handling auto-scaling, port-forwarding, ingress, and more.</p>"},{"location":"docs/#why-dstack","title":"Why dstack?","text":"<p><code>dstack</code>'s founder and CEO explains the challenges <code>dstack</code> addresses for AI and Ops teams.</p> <p><code>dstack</code> streamlines infrastructure management and container usage, enabling AI teams to work with any frameworks across cloud platforms or on-premise servers.</p>"},{"location":"docs/#how-does-it-compare-to-other-tools","title":"How does it compare to other tools?","text":"How does dstack compare to Kubernetes? <p><code>dstack</code> is more lightweight, and is designed specifically for AI, enabling AI engineers to handle development, training, and  deployment without needing extra tools or Ops support. </p> <p>With <code>dstack</code>, you don't need Kubeflow or other ML platforms on top\u2014everything is available out of the box.</p> <p>Additionally, <code>dstack</code> is much easier to use with on-prem servers\u2014just provide hostnames and SSH credentials,  and <code>dstack</code> will automatically create a fleet ready for use with development environments, tasks, and services.</p> Can dstack and Kubernetes be used together? <p>For AI development, it\u2019s more efficient to use <code>dstack</code> directly with your cloud accounts or on-prem servers\u2014without Kubernetes.</p> <p>However, if you prefer, you can set up the <code>dstack</code> server with a Kubernetes backend to provision through Kubernetes.</p> <p>Does your Ops team insist on using Kubernetes for production-grade deployment? You can use <code>dstack</code> and Kubernetes side by side; <code>dstack</code> for development and Kubernetes for production-grade deployment.</p> How does dstack compare to KubeFlow? <p><code>dstack</code> can be used entirely instead of Kubeflow. It covers everything that Kubeflow does, and much more on top,  including development environments, services, and additional features.</p> <p><code>dstack</code> is easier to set up with on-premises servers, doesn't require Kubernetes, and works with multiple cloud  providers out of the box.</p> How does dstack compare to Slurm? <p><code>dstack</code> can be used entirely instead of Slurm. It covers everything that Slurm does, and a lot more on top, including dev environments, services, out-of-the-box cloud support, easier setup with on-premises servers, and much more.</p> <p>Where do I start?</p> <ol> <li>Proceed to installation</li> <li>See quickstart</li> <li>Browse examples</li> <li>Join Discord </li> </ol>"},{"location":"docs/quickstart/","title":"Quickstart","text":"<p>Before using <code>dstack</code>, ensure you've installed the server, or signed up for dstack Sky .</p>"},{"location":"docs/quickstart/#initialize-a-repo","title":"Initialize a repo","text":"<p>Before using <code>dstack</code>'s CLI in a directory, initialize the directory as a repo with <code>dstack init</code>.</p> <pre><code>$ mkdir quickstart &amp;&amp; cd quickstart\n$ dstack init\n</code></pre>"},{"location":"docs/quickstart/#run-a-configuration","title":"Run a configuration","text":"Dev environmentTaskService <p>A dev environment lets you provision an instance and access it with your desktop IDE.</p> <p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: dev-environment\nname: vscode\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.11\"\n#image: dstackai/base:py3.13-0.6-cuda-12.1\n\nide: vscode\n\n# Uncomment to request resources\n#resources:\n#  gpu: 24GB\n</code></pre> <p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f .dstack.yml\n\n #  BACKEND  REGION           RESOURCES                 SPOT  PRICE\n 1  gcp      us-west4         2xCPU, 8GB, 100GB (disk)  yes   $0.010052\n 2  azure    westeurope       2xCPU, 8GB, 100GB (disk)  yes   $0.0132\n 3  gcp      europe-central2  2xCPU, 8GB, 100GB (disk)  yes   $0.013248\n\nSubmit the run vscode? [y/n]: y\n\nLaunching `vscode`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+vscode/workflow\n</code></pre> <p>Open the link to access the dev environment using your desktop IDE.</p> <p>Alternatively, you can access it via <code>ssh &lt;run name&gt;</code>.</p> <p>A task allows you to schedule a job or run a web app. Tasks can be distributed and can forward ports.</p> <p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: task\nname: streamlit\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.11\"\n#image: dstackai/base:py3.13-0.6-cuda-12.1\n\n# Commands of the task\ncommands:\n  - pip install streamlit\n  - streamlit hello\n# Ports to forward\nports:\n  - 8501\n\n# Uncomment to request resources\n#resources:\n#  gpu: 24GB\n</code></pre> <p>By default, tasks run on a single instance. To run a distributed task, specify  <code>nodes</code>, and <code>dstack</code> will run it on a cluster.</p> <p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f task.dstack.yml\n\n #  BACKEND  REGION           RESOURCES                 SPOT  PRICE\n 1  gcp      us-west4         2xCPU, 8GB, 100GB (disk)  yes   $0.010052\n 2  azure    westeurope       2xCPU, 8GB, 100GB (disk)  yes   $0.0132\n 3  gcp      europe-central2  2xCPU, 8GB, 100GB (disk)  yes   $0.013248\n\nSubmit the run streamlit? [y/n]: y\n\nProvisioning `streamlit`...\n---&gt; 100%\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: http://localhost:8501\n</code></pre> <p>If you specified <code>ports</code>, they will be automatically forwarded to <code>localhost</code> for convenient access.</p> <p>A service allows you to deploy a model or any web app as an endpoint.</p> <p>Create the following configuration file inside the repo:</p> <p> <pre><code>type: service\nname: llama31-service\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.11\"\n#image: dstackai/base:py3.13-0.6-cuda-12.1\n\n# Required environment variables\nenv:\n  - HF_TOKEN\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096\n# Expose the vllm server port\nport: 8000\n\n# Specify a name if it's an OpenAI-compatible model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Required resources\nresources:\n  gpu: 24GB\n</code></pre> <p>Run the configuration via <code>dstack apply</code>:</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f service.dstack.yml\n\n #  BACKEND  REGION     INSTANCE       RESOURCES                    SPOT  PRICE\n 1  aws      us-west-2  g5.4xlarge     16xCPU, 64GB, 1xA10G (24GB)  yes   $0.22\n 2  aws      us-east-2  g6.xlarge      4xCPU, 16GB, 1xL4 (24GB)     yes   $0.27\n 3  gcp      us-west1   g2-standard-4  4xCPU, 16GB, 1xL4 (24GB)     yes   $0.27\n\nSubmit the run llama31-service? [y/n]: y\n\nProvisioning `llama31-service`...\n---&gt; 100%\n\nService is published at: \n  http://localhost:3000/proxy/services/main/llama31-service/\nModel meta-llama/Meta-Llama-3.1-8B-Instruct is published at:\n  http://localhost:3000/proxy/models/main/\n</code></pre> <p>Gateway</p> <p>To enable auto-scaling, or use a custom domain with HTTPS,  set up a gateway before running the service. If you're using dstack Sky , a gateway is pre-configured for you.</p> <p><code>dstack apply</code> automatically provisions instances, uploads the contents of the repo (incl. your local uncommitted changes), and runs the configuration.</p>"},{"location":"docs/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Something not working? See the troubleshooting guide.</p> <p>What's next?</p> <ol> <li>Read about backends,  dev environments, tasks, and services</li> <li>Join Discord </li> <li>Browse examples</li> </ol>"},{"location":"docs/concepts/backends/","title":"Backends","text":"<p>To use <code>dstack</code> with cloud providers, configure backends via the <code>~/.dstack/server/config.yml</code> file. The server loads this file on startup. </p> <p>Alternatively, you can configure backends on the project settings page via UI.</p> <p>For using <code>dstack</code> with on-prem servers, no backend configuration is required. Use SSH fleets instead.</p> <p>Below examples of how to configure backends via <code>~/.dstack/server/config.yml</code>.</p>"},{"location":"docs/concepts/backends/#cloud-providers","title":"Cloud providers","text":""},{"location":"docs/concepts/backends/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p> Default credentialsAccess key <p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n</code></pre> <p>Create an access key by following the this guide . Once you've downloaded the <code>.csv</code> file with your IAM user's Access key ID and Secret access key, proceed to configure the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: access_key\n          access_key: KKAAUKLIZ5EHKICAOASV\n          secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> Required permissions <p>The following AWS policy permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:AuthorizeSecurityGroupEgress\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreatePlacementGroup\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeletePlacementGroup\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeCapacityReservations\"\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceAttribute\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DetachVolume\",\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"servicequotas:ListServiceQuotas\",\n                \"servicequotas:GetServiceQuota\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticloadbalancing:CreateLoadBalancer\",\n                \"elasticloadbalancing:CreateTargetGroup\",\n                \"elasticloadbalancing:CreateListener\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:DeleteLoadBalancer\",\n                \"elasticloadbalancing:DeleteTargetGroup\",\n                \"elasticloadbalancing:DeleteListener\",\n                \"elasticloadbalancing:DeregisterTargets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"acm:DescribeCertificate\",\n                \"acm:ListCertificates\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>The <code>elasticloadbalancing:*</code> and <code>acm:*</code> permissions are only needed for provisioning gateways with ACM (AWS Certificate Manager) certificates.</p> VPC <p>By default, <code>dstack</code> uses the default VPC. It's possible to customize it:</p> vpc_namevpc_ids <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        vpc_name: my-vpc\n</code></pre> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        default_vpcs: true\n        vpc_ids:\n          us-east-1: vpc-0a2b3c4d5e6f7g8h\n          us-east-2: vpc-9i8h7g6f5e4d3c2b\n          us-west-1: vpc-4d3c2b1a0f9e8d7\n</code></pre> <p>For the regions without configured <code>vpc_ids</code>, enable default VPCs by setting <code>default_vpcs</code> to <code>true</code>.</p> Private subnets <p>By default, <code>dstack</code> provisions instances with public IPs and permits inbound SSH traffic. If you want <code>dstack</code> to use private subnets and provision instances without public IPs, set <code>public_ips</code> to <code>false</code>.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        public_ips: false\n</code></pre> <p>Using private subnets assumes that both the <code>dstack</code> server and users can access the configured VPC's private subnets. Additionally, private subnets must have outbound internet connectivity provided by NAT Gateway, Transit Gateway, or other mechanism.</p> OS images <p>By default, <code>dstack</code> uses its own AMI  optimized for <code>dstack</code>. To use your own or other third-party images, set the <code>os_images</code> property:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: aws\n        creds:\n          type: default\n\n        os_images:\n          cpu:\n            name: my-ami-for-cpu-instances\n            owner: self\n            user: dstack\n          nvidia:\n            name: 'Some ThirdParty CUDA image'\n            owner: 123456789012\n            user: ubuntu\n</code></pre> <p>Here, both <code>cpu</code> and <code>nvidia</code> properties are optional, but if the property is not set, you won\u00b4t be able to use the corresponding instance types.</p> <p>The <code>name</code> is an AMI name. The <code>owner</code> is either an AWS account ID (a 12-digit number) or a special value <code>self</code> indicating the current account. The <code>user</code> specifies an OS user for instance provisioning.</p> <p>Image requirements</p> <ul> <li>SSH server listening on port 22</li> <li><code>user</code> with passwordless sudo access</li> <li>Docker is installed</li> <li>(For NVIDIA instances) NVIDIA/CUDA drivers and NVIDIA Container Toolkit are installed</li> </ul>"},{"location":"docs/concepts/backends/#azure","title":"Azure","text":"<p>There are two ways to configure Azure: using a client secret or using the default credentials.</p> Default credentialsClient secret <p>If you have default credentials set up, configure the backend like this:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: azure\n        subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n        tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n        creds:\n          type: default\n</code></pre> <p>If you don't know your <code>subscription_id</code> and <code>tenant_id</code>, use Azure CLI :</p> <pre><code>az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>A client secret can be created using the Azure CLI :</p> <pre><code>SUBSCRIPTION_ID=...\naz ad sp create-for-rbac\n    --name dstack-app \\\n    --role $DSTACK_ROLE \\\n    --scopes /subscriptions/$SUBSCRIPTION_ID \\\n    --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <p>If you don't know your <code>subscription_id</code>, use Azure CLI :</p> <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre> Required permissions <p>The following Azure permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"properties\": {\n        \"roleName\": \"dstack-role\",\n        \"description\": \"Minimal required permissions for using Azure with dstack\",\n        \"assignableScopes\": [\n            \"/subscriptions/${YOUR_SUBSCRIPTION_ID}\"\n        ],\n        \"permissions\": [\n            {\n            \"actions\": [\n                \"Microsoft.Authorization/*/read\",\n                \"Microsoft.Compute/availabilitySets/*\",\n                \"Microsoft.Compute/locations/*\",\n                \"Microsoft.Compute/virtualMachines/*\",\n                \"Microsoft.Compute/virtualMachineScaleSets/*\",\n                \"Microsoft.Compute/cloudServices/*\",\n                \"Microsoft.Compute/disks/write\",\n                \"Microsoft.Compute/disks/read\",\n                \"Microsoft.Compute/disks/delete\",\n                \"Microsoft.Network/networkSecurityGroups/*\",\n                \"Microsoft.Network/locations/*\",\n                \"Microsoft.Network/virtualNetworks/*\",\n                \"Microsoft.Network/networkInterfaces/*\",\n                \"Microsoft.Network/publicIPAddresses/*\",\n                \"Microsoft.Resources/subscriptions/resourceGroups/read\",\n                \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n                \"Microsoft.Resources/subscriptions/read\"\n            ],\n            \"notActions\": [],\n            \"dataActions\": [],\n            \"notDataActions\": []\n            }\n        ]\n    }\n}\n</code></pre> VPC <p>By default, <code>dstack</code> creates new Azure networks and subnets for every configured region. It's possible to use custom networks by specifying <code>vpc_ids</code>:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: azure\n        creds:\n          type: default\n    regions: [westeurope]\n    vpc_ids:\n      westeurope: myNetworkResourceGroup/myNetworkName\n</code></pre> Private subnets <p>By default, <code>dstack</code> provisions instances with public IPs and permits inbound SSH traffic. If you want <code>dstack</code> to use private subnets and provision instances without public IPs, specify custom networks using <code>vpc_ids</code> and set <code>public_ips</code> to <code>false</code>.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: azure\n        creds:\n          type: default\n        regions: [westeurope]\n        vpc_ids:\n          westeurope: myNetworkResourceGroup/myNetworkName\n        public_ips: false\n</code></pre> <p>Using private subnets assumes that both the <code>dstack</code> server and users can access the configured VPC's private subnets. Additionally, private subnets must have outbound internet connectivity provided by NAT Gateway or other mechanism.</p>"},{"location":"docs/concepts/backends/#gcp","title":"GCP","text":"<p>There are two ways to configure GCP: using a service account or using the default credentials.</p> Default credentialsService account <p>Enable GCP application default credentials:</p> <pre><code>gcloud auth application-default login \n</code></pre> <p>Then configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n</code></pre> <p>To create a service account, follow this guide . After setting up the service account create a key  for it and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path.</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: service_account\n        filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <p>If you don't know your GCP project ID, use Google Cloud CLI :</p> <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre> Required permissions <p>The following GCP permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>compute.disks.create\ncompute.disks.delete\ncompute.disks.get\ncompute.disks.list\ncompute.disks.setLabels\ncompute.disks.use\ncompute.firewalls.create\ncompute.images.useReadOnly\ncompute.instances.attachDisk\ncompute.instances.create\ncompute.instances.delete\ncompute.instances.detachDisk\ncompute.instances.get\ncompute.instances.setLabels\ncompute.instances.setMetadata\ncompute.instances.setServiceAccount\ncompute.instances.setTags\ncompute.networks.get\ncompute.networks.updatePolicy\ncompute.regions.get\ncompute.regions.list\ncompute.routers.list\ncompute.subnetworks.list\ncompute.subnetworks.use\ncompute.subnetworks.useExternalIp\ncompute.zoneOperations.get\n</code></pre> <p>If you plan to use TPUs, additional permissions are required:</p> <pre><code>tpu.nodes.create\ntpu.nodes.get\ntpu.nodes.update\ntpu.nodes.delete\ntpu.operations.get\ntpu.operations.list\n</code></pre> <p>Also, the use of TPUs requires the <code>serviceAccountUser</code> role. For TPU VMs, dstack will use the default service account.</p> Required APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>.</p> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com\n</code></pre> VPC VPCShared VPC <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n\n      vpc_name: my-custom-vpc\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n    - type: gcp\n      project_id: gcp-project-id\n      creds:\n        type: default\n\n      vpc_name: my-custom-vpc\n      vpc_project_id: another-project-id\n</code></pre> <p>When using a Shared VPC, ensure there is a firewall rule allowing <code>INGRESS</code> traffic on port <code>22</code>. You can limit this rule to <code>dstack</code> instances using the <code>dstack-runner-instance</code> target tag.</p> <p>When using GCP gateways with a Shared VPC, also ensure there is a firewall rule allowing <code>INGRESS</code> traffic on ports <code>22</code>, <code>80</code>, <code>443</code>. You can limit this rule to <code>dstack</code> gateway instances using the <code>dstack-gateway-instance</code> target tag.</p> <p>To use TPUs with a Shared VPC, you need to grant the TPU Service Account in your service project permissions to manage resources in the host project by granting the \"TPU Shared VPC Agent\" (roles/tpu.xpnAgent) role (more in the GCP docs).</p> Private subnets <p>By default, <code>dstack</code> provisions instances with public IPs and permits inbound SSH traffic. If you want <code>dstack</code> to use private subnets and provision instances without public IPs, set <code>public_ips</code> to <code>false</code>.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: gcp\n        creds:\n          type: default\n\n        public_ips: false\n</code></pre> <p>Using private subnets assumes that both the <code>dstack</code> server and users can access the configured VPC's private subnets. Additionally, Cloud NAT must be configured to provide access to external resources for provisioned instances.</p>"},{"location":"docs/concepts/backends/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud  account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: lambda\n      creds:\n        type: api_key\n        api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/concepts/backends/#runpod","title":"RunPod","text":"<p>Log into your RunPod  console, click Settings in the sidebar, expand the <code>API Keys</code> section, and click the button to create a Read &amp; Write key.</p> <p>Then proceed to configuring the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: runpod\n        creds:\n          type: api_key\n          api_key: US9XTPDIV8AR42MMINY8TCKRB8S4E7LNRQ6CAUQ9\n</code></pre>"},{"location":"docs/concepts/backends/#vultr","title":"Vultr","text":"<p>Log into your Vultr  account, click <code>Account</code> in the sidebar, select <code>API</code>, find the <code>Personal Access Token</code> panel and click the <code>Enable API</code> button. In the <code>Access Control</code> panel, allow API requests from all addresses or from the subnet where your <code>dstack</code> server is deployed.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: vultr\n        creds:\n          type: api_key\n          api_key: B57487240a466624b48de22865589\n</code></pre>"},{"location":"docs/concepts/backends/#vastai","title":"Vast.ai","text":"<p>Log into your Vast.ai  account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: vastai\n      creds:\n        type: api_key\n        api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/concepts/backends/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock  account, click Developers in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: tensordock\n        creds:\n          type: api_key\n          api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n          api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/concepts/backends/#cudo","title":"CUDO","text":"<p>Log into your CUDO Compute  account, click API keys in the sidebar, and click the <code>Create an API key</code> button.</p> <p>Ensure you've created a project with CUDO Compute, then proceed to configuring the backend.</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: cudo\n        project_id: my-cudo-project\n        creds:\n          type: api_key\n          api_key: 7487240a466624b48de22865589\n</code></pre>"},{"location":"docs/concepts/backends/#oci","title":"OCI","text":"<p>There are two ways to configure OCI: using client credentials or using the default credentials.</p> Default credentialsClient credentials <p>If you have default credentials set up in <code>~/.oci/config</code>, configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: default\n</code></pre> <p>Log into the OCI Console , go to <code>My profile</code>,  select <code>API keys</code>, and click <code>Add API key</code>.</p> <p>Once you add a key, you'll see the configuration file. Copy its values to configure the backend as follows:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: client\n      user: ocid1.user.oc1..g5vlaeqfu47akmaafq665xsgmyaqjktyfxtacfxc4ftjxuca7aohnd2ev66m\n      tenancy: ocid1.tenancy.oc1..ajqsftvk4qarcfaak3ha4ycdsaahxmaita5frdwg3tqo2bcokpd3n7oizwai\n      region: eu-frankfurt-1\n      fingerprint: 77:32:77:00:49:7c:cb:56:84:75:8e:77:96:7d:53:17\n      key_file: ~/.oci/private_key.pem\n</code></pre> <p>Make sure to include either the path to your private key via <code>key_file</code> or the contents of the key via <code>key_content</code>.</p> Required permissions <p>This is an example of a restrictive policy for a group of <code>dstack</code> users:</p> <pre><code>Allow group &lt;dstack-users&gt; to read compartments in tenancy where target.compartment.name = '&lt;dstack-compartment&gt;'\nAllow group &lt;dstack-users&gt; to read marketplace-community-listings in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage app-catalog-listing in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage instances in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage compute-capacity-reports in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage volumes in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage volume-attachments in compartment &lt;dstack-compartment&gt;\nAllow group &lt;dstack-users&gt; to manage virtual-network-family in compartment &lt;dstack-compartment&gt;\n</code></pre> <p>To use this policy, create a compartment for <code>dstack</code> and specify it in <code>~/.dstack/server/config.yml</code>.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: oci\n    creds:\n      type: default\n    compartment_id: ocid1.compartment.oc1..aaaaaaaa\n</code></pre>"},{"location":"docs/concepts/backends/#datacrunch","title":"DataCrunch","text":"<p>Log into your DataCrunch  account, click Keys in the sidebar, find <code>REST API Credentials</code> area and then click the <code>Generate Credentials</code> button.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: datacrunch\n        creds:\n          type: api_key\n          client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n          client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/concepts/backends/#on-prem-servers","title":"On-prem servers","text":""},{"location":"docs/concepts/backends/#ssh-fleets","title":"SSH fleets","text":"<p>For using <code>dstack</code> with on-prem servers, no backend configuration is required. See SSH fleets for more details. </p>"},{"location":"docs/concepts/backends/#kubernetes","title":"Kubernetes","text":"<p>To configure a Kubernetes backend, specify the path to the kubeconfig file, and the port that <code>dstack</code> can use for proxying SSH traffic. In case of a self-managed cluster, also specify the IP address of any node in the cluster.</p> Self-managedManaged <p>Here's how to configure the backend to use a self-managed cluster.</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: kubernetes\n      kubeconfig:\n        filename: ~/.kube/config\n      networking:\n        ssh_host: localhost # The external IP address of any node\n        ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> Kind <p>If you are using Kind, make sure to make  to set up <code>ssh_port</code> via <code>extraPortMappings</code> for proxying SSH traffic:</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraPortMappings:\n      - containerPort: 32000 # Must be same as `ssh_port`\n        hostPort: 32000 # Must be same as `ssh_port`\n</code></pre> <p>Go ahead and create the cluster like this: </p> <pre><code>kind create cluster --config examples/misc/kubernetes/kind-config.yml\n</code></pre> <p>Here's how to configure the backend to use a managed cluster (AWS, GCP, Azure).</p> <pre><code>projects:\n  - name: main\n    backends:\n      - type: kubernetes\n        kubeconfig:\n          filename: ~/.kube/config\n        networking:\n          ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> EKS <p>For example, if you are using EKS, make sure to add it via an ingress rule of the corresponding security group:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;cluster-security-group-id&gt; --protocol tcp --port 32000 --cidr 0.0.0.0/0\n</code></pre> NVIDIA GPU Operator <p>To use GPUs with Kubernetes, the cluster must be installed with the NVIDIA GPU Operator .</p>"},{"location":"docs/concepts/backends/#dstack-sky","title":"dstack Sky","text":"<p>If you're using dstack Sky , backends are pre-configured to use compute from <code>dstack</code>'s marketplace.</p> <p>You can reconfigure backends via the UI, to use your own cloud accounts instead.</p>"},{"location":"docs/concepts/dev-environments/","title":"Dev environments","text":"<p>A dev environment lets you provision an instance and access it with your desktop IDE.</p>"},{"location":"docs/concepts/dev-environments/#define-a-configuration","title":"Define a configuration","text":"<p>First, define a dev environment configuration as a YAML file in your project folder. The filename must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode\n\npython: \"3.11\"\n# Uncomment to use a custom Docker image\n#image: dstackai/base:py3.13-0.6-cuda-12.1\nide: vscode\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: 24GB\n</code></pre>"},{"location":"docs/concepts/dev-environments/#initialization","title":"Initialization","text":"<p>If you want to pre-configure the dev environment, specify the <code>init</code> property with a list of commands to run at startup:</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode\n\npython: \"3.11\"\nide: vscode\n\n# Commands to run on startup\ninit:\n  - pip install wandb\n</code></pre>"},{"location":"docs/concepts/dev-environments/#resources","title":"Resources","text":"<p>When you specify a resource value like <code>cpu</code> or <code>memory</code>, you can either use an exact value (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n  # Shared memory (required by multi-gpu)\n  shm_size: 16GB\n  # Disk size\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\nresources:\n  gpu: v2-8\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single TPU device workloads. Multi-TPU support is coming soon.</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/concepts/dev-environments/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the <code>nvcc</code> property to true.</p>"},{"location":"docs/concepts/dev-environments/#docker","title":"Docker","text":"<p>If you want, you can specify your own Docker image via <code>image</code>.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Any custom Docker image\nimage: ghcr.io/huggingface/text-generation-inference:latest\n\nide: vscode\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry. </p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Any private Docker image\nimage: ghcr.io/huggingface/text-generation-inference:latest\n# Credentials of the private Docker registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\nide: vscode\n</code></pre> Privileged mode <p>All backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code> support running containers in privileged mode. This mode enables features like using Docker and Docker Compose  inside <code>dstack</code> runs.</p>"},{"location":"docs/concepts/dev-environments/#environment-variables","title":"Environment variables","text":"<pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\n# Environment variables\nenv:\n  - HF_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\nide: vscode\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HF_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> System environment variables <p>The following environment variables are available in any run by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/concepts/dev-environments/#spot-policy","title":"Spot policy","text":"<p>By default, <code>dstack</code> uses on-demand instances. However, you can change that via the <code>spot_policy</code> property. It accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>.</p> <p>Reference</p> <p>Dev environments support many more configuration options, incl. <code>backends</code>,  <code>regions</code>,  <code>max_price</code>, and <code>max_duration</code>,  among others.</p>"},{"location":"docs/concepts/dev-environments/#run-a-configuration","title":"Run a configuration","text":"<p>To run a dev environment, pass the configuration to <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                SPOT  PRICE\n 1  runpod   CA-MTL-1  9xCPU, 48GB, A5000:24GB  yes   $0.11\n 2  runpod   EU-SE-1   9xCPU, 43GB, A5000:24GB  yes   $0.11\n 3  gcp      us-west4  4xCPU, 16GB, L4:24GB     yes   $0.214516\n\nSubmit the run vscode? [y/n]: y\n\nLaunching `vscode`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+vscode/workflow\n</code></pre> <p><code>dstack apply</code> automatically provisions an instance, uploads the contents of the repo (incl. your local uncommitted changes), and sets up an IDE on the instance.</p> <p>Windows</p> <p>On Windows, <code>dstack</code> works both natively and inside WSL. But, for dev environments,  it's recommended not to use <code>dstack apply</code> inside WSL due to a VS Code issue .</p> <p>To open the dev environment in your desktop IDE, use the link from the output  (such as <code>vscode://vscode-remote/ssh-remote+fast-moth-1/workflow</code>).</p> <p></p> SSH <p>Alternatively, while the CLI is attached to the run, you can connect to the dev environment via SSH:</p> <pre><code>$ ssh fast-moth-1\n</code></pre>"},{"location":"docs/concepts/dev-environments/#retry-policy","title":"Retry policy","text":"<p>By default, if <code>dstack</code> can't find capacity or the instance is interrupted, the run will fail.</p> <p>If you'd like <code>dstack</code> to automatically retry, configure the  retry property accordingly:</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: vscode    \n\nide: vscode\n\nretry:\n  # Retry on specific events\n  on_events: [no-capacity, error, interruption]\n  # Retry for up to 1 hour\n  duration: 1h\n</code></pre>"},{"location":"docs/concepts/dev-environments/#creation-policy","title":"Creation policy","text":"<p>By default, when you run <code>dstack apply</code> with a dev environment, task, or service, if no <code>idle</code> instances from the available fleets meet the requirements, <code>dstack</code> creates a new fleet  using configured backends.</p> <p>To ensure <code>dstack apply</code> doesn't create a new fleet but reuses an existing one, pass <code>-R</code> (or <code>--reuse</code>) to <code>dstack apply</code>.</p> <pre><code>$ dstack apply -R -f examples/.dstack.yml\n</code></pre> <p>Or, set <code>creation_policy</code> to <code>reuse</code> in the run configuration.</p>"},{"location":"docs/concepts/dev-environments/#idle-duration","title":"Idle duration","text":"<p>If a fleet is created automatically, it stays <code>idle</code> for 5 minutes by default and can be reused within that time. If the fleet is not reused within this period, it is automatically terminated. To change the default idle duration, set <code>idle_duration</code> in the run configuration (e.g., <code>0s</code>, <code>1m</code>, or <code>off</code> for unlimited).</p> <p>Fleets</p> <p>For greater control over fleet provisioning, it is recommended to create fleets explicitly.</p>"},{"location":"docs/concepts/dev-environments/#manage-runs","title":"Manage runs","text":""},{"location":"docs/concepts/dev-environments/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code>  command lists all running jobs and their statuses. Use <code>--watch</code> (or <code>-w</code>) to monitor the live status of runs.</p>"},{"location":"docs/concepts/dev-environments/#stop-a-run","title":"Stop a run","text":"<p>A dev environment runs until you stop it or its lifetime exceeds <code>max_duration</code>. To gracefully stop a dev environment, use <code>dstack stop</code>. Pass <code>--abort</code> or <code>-x</code> to stop without waiting for a graceful shutdown.</p>"},{"location":"docs/concepts/dev-environments/#attach-to-a-run","title":"Attach to a run","text":"<p>By default, <code>dstack apply</code> runs in attached mode \u2013 it establishes the SSH tunnel to the run, forwards ports, and shows real-time logs. If you detached from a run, you can reattach to it using <code>dstack attach</code>.</p>"},{"location":"docs/concepts/dev-environments/#see-run-logs","title":"See run logs","text":"<p>To see the logs of a run without attaching, use <code>dstack logs</code>. Pass <code>--diagnose</code>/<code>-d</code> to <code>dstack logs</code> to see the diagnostics logs. It may be useful if a run fails. For more information on debugging failed runs, see the troubleshooting guide.</p> <p>What's next?</p> <ol> <li>Read about tasks, services, and repos</li> <li>Learn how to manage fleets</li> </ol>"},{"location":"docs/concepts/fleets/","title":"Fleets","text":"<p>Fleets are groups of instances used to run dev environments, tasks, and services. Depending on the fleet configuration, instances can be interconnected clusters or standalone instances.</p> <p><code>dstack</code> supports two kinds of fleets: </p> <ul> <li>Cloud fleets \u2013 dynamically provisioned through configured backends</li> <li>SSH fleets \u2013 created using on-prem servers</li> </ul>"},{"location":"docs/concepts/fleets/#cloud","title":"Cloud fleets","text":"<p>When you call <code>dstack apply</code> to run a dev environment, task, or service, <code>dstack</code> reuses <code>idle</code> instances  from an existing fleet. If none match the requirements, <code>dstack</code> creates a new cloud fleet.</p> <p>For greater control over cloud fleet provisioning, create fleets explicitly using configuration files. </p>"},{"location":"docs/concepts/fleets/#define-a-configuration","title":"Define a configuration","text":"<p>Define a fleet configuration as a YAML file in your project directory. The file must have a <code>.dstack.yml</code> extension (e.g. <code>.dstack.yml</code> or <code>fleet.dstack.yml</code>).</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\n# Specify the number of instances\nnodes: 2\n# Uncomment to ensure instances are inter-connected\n#placement: cluster\n\nresources:\n  gpu: 24GB\n</code></pre>"},{"location":"docs/concepts/fleets/#cloud-placement","title":"Placement","text":"<p>To ensure instances are interconnected (e.g., for distributed tasks), set <code>placement</code> to <code>cluster</code>.  This ensures all instances are provisioned in the same backend and region with optimal inter-node connectivity</p> AWS <p><code>dstack</code> automatically enables the Elastic Fabric Adapter for all EFA-capable instance types . Currently, only one EFA interface is enabled per instance, regardless of its maximum capacity. This will change once this issue  is resolved.</p> <p>The <code>cluster</code> placement is supported only for <code>aws</code>, <code>azure</code>, <code>gcp</code>, <code>oci</code>, and <code>vultr</code> backends.</p>"},{"location":"docs/concepts/fleets/#resources","title":"Resources","text":"<p>When you specify a resource value like <code>cpu</code> or <code>memory</code>, you can either use an exact value (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\nnodes: 2\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n  # Disk size\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\nnodes: 2\n\nresources:\n  gpu: v2-8\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single TPU device workloads. Multi-TPU support is coming soon.</p>"},{"location":"docs/concepts/fleets/#idle-duration","title":"Idle duration","text":"<p>By default, fleet instances stay <code>idle</code> for 3 days and can be reused within that time. If the fleet is not reused within this period, it is automatically terminated.</p> <p>To change the default idle duration, set <code>idle_duration</code> in the run configuration (e.g., <code>0s</code>, <code>1m</code>, or <code>off</code> for unlimited).</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\nnodes: 2\n\n# Terminate instances idle for more than 1 hour\nidle_duration: 1h\n\nresources:\n  gpu: 24GB\n</code></pre>"},{"location":"docs/concepts/fleets/#spot-policy","title":"Spot policy","text":"<p>By default, <code>dstack</code> uses on-demand instances. However, you can change that via the <code>spot_policy</code> property. It accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>.</p>"},{"location":"docs/concepts/fleets/#retry-policy","title":"Retry policy","text":"<p>By default, if <code>dstack</code> fails to provision an instance or an instance is interrupted, no retry is attempted.</p> <p>If you'd like <code>dstack</code> to do it, configure the  retry property accordingly:</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\nnodes: 1\n\nresources:\n  gpu: 24GB\n\nretry:\n  # Retry on specific events\n  on_events: [no-capacity, interruption]\n  # Retry for up to 1 hour\n  duration: 1h\n</code></pre> <p>Cloud fleets are supported by all backends except <code>kubernetes</code>, <code>vastai</code>, and <code>runpod</code>.</p> <p>Reference</p> <p>Cloud fleets support many more configuration options, incl. <code>backends</code>,  <code>regions</code>,  <code>max_price</code>, and among others.</p>"},{"location":"docs/concepts/fleets/#create-or-update-a-fleet","title":"Create or update a fleet","text":"<p>To create or update the fleet, pass the fleet configuration to <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/misc/fleets/.dstack.yml\n</code></pre> <p>To ensure the fleet is created, you can use the <code>dstack fleet</code> command:</p> <pre><code>$ dstack fleet\n\n FLEET     INSTANCE  BACKEND              GPU             PRICE    STATUS  CREATED \n my-fleet  0         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago      \n           1         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago    \n</code></pre> <p>Once the status of instances changes to <code>idle</code>, they can be used by dev environments, tasks, and services.</p>"},{"location":"docs/concepts/fleets/#ssh","title":"SSH fleets","text":"<p>If you have a group of on-prem servers accessible via SSH, you can create an SSH fleet.</p>"},{"location":"docs/concepts/fleets/#define-a-configuration_1","title":"Define a configuration","text":"<p>Define a fleet configuration as a YAML file in your project directory. The file must have a <code>.dstack.yml</code> extension (e.g. <code>.dstack.yml</code> or <code>fleet.dstack.yml</code>).</p> <pre><code>type: fleet\n# The name is optional, if not specified, generated randomly\nname: my-fleet\n\n# Uncomment if instances are interconnected\n#placement: cluster\n\n# SSH credentials for the on-prem servers\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre> Requirements <p>1.\u00a0Hosts should be pre-installed with Docker.</p> NVIDIAAMDIntel Gaudi <p>2.\u00a0Hosts with NVIDIA GPUs should also be pre-installed with CUDA 12.1 and NVIDIA Container Toolkit .</p> <p>2.\u00a0Hosts with AMD GPUs should also be pre-installed with AMDGPU-DKMS kernel driver (e.g. via native package manager  or AMDGPU installer .)</p> <p>2.\u00a0Hosts with Intel Gaudi accelerators should be pre-installed with Gaudi software and drivers. This should include the drivers, <code>hl-smi</code>, and Habana Container Runtime.</p> <p>3.\u00a0The user specified should have passwordless <code>sudo</code> access.</p>"},{"location":"docs/concepts/fleets/#ssh-placement","title":"Placement","text":"<p>If the hosts are interconnected (i.e. share the same network), set <code>placement</code> to <code>cluster</code>.  This is required if you'd like to use the fleet for distributed tasks.</p>"},{"location":"docs/concepts/fleets/#network","title":"Network","text":"<p>By default, <code>dstack</code> automatically detects the network shared by the hosts.  However, it's possible to configure it explicitly via  the <code>network</code> property.</p>"},{"location":"docs/concepts/fleets/#environment-variables","title":"Environment variables","text":"<p>If needed, you can specify environment variables that will be used by <code>dstack-shim</code> and passed to containers.</p> <p>For example, these variables can be used to configure a proxy:</p> <pre><code>type: fleet\nname: my-fleet\n\nenv:\n  - HTTP_PROXY=http://proxy.example.com:80\n  - HTTPS_PROXY=http://proxy.example.com:80\n  - NO_PROXY=localhost,127.0.0.1\n\nssh_config:\n  user: ubuntu\n  identity_file: ~/.ssh/id_rsa\n  hosts:\n    - 3.255.177.51\n    - 3.255.177.52\n</code></pre> <p>Reference</p> <p>For all SSH fleet configuration options, refer to the reference.</p>"},{"location":"docs/concepts/fleets/#create-or-update-a-fleet_1","title":"Create or update a fleet","text":"<p>To create or update the fleet, pass the fleet configuration to <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f examples/misc/fleets/.dstack.yml\n</code></pre> <p>To ensure the fleet is created, you can use the <code>dstack fleet</code> command:</p> <pre><code>$ dstack fleet\n\n FLEET     INSTANCE  GPU             PRICE  STATUS  CREATED \n my-fleet  0         L4:24GB (spot)  $0     idle    3 mins ago      \n           1         L4:24GB (spot)  $0     idle    3 mins ago    \n</code></pre> <p>When you apply this configuration, <code>dstack</code> will connect to the specified hosts using the provided SSH credentials,  install the dependencies, and configure these servers as a fleet.</p> <p>Once the status of instances changes to <code>idle</code>, they can be used by dev environments, tasks, and services.</p>"},{"location":"docs/concepts/fleets/#troubleshooting","title":"Troubleshooting","text":"<p>Resources</p> <p>Once the fleet is created, double-check that the GPU, memory, and disk are detected correctly.</p> <p>If the status does not change to <code>idle</code> after a few minutes or the resources are not displayed correctly, ensure that all host requirements are satisfied.</p> <p>If the requirements are met but the fleet still fails to be created correctly, check the logs at <code>/root/.dstack/shim.log</code> on the hosts for error details.</p>"},{"location":"docs/concepts/fleets/#manage-fleets","title":"Manage fleets","text":""},{"location":"docs/concepts/fleets/#list-fleets","title":"List fleets","text":"<p>The <code>dstack fleet</code> command lists fleet instances and their status:</p> <pre><code>$ dstack fleet\n\n FLEET     INSTANCE  BACKEND              GPU             PRICE    STATUS  CREATED \n my-fleet  0         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago      \n           1         gcp (europe-west-1)  L4:24GB (spot)  $0.1624  idle    3 mins ago    \n</code></pre>"},{"location":"docs/concepts/fleets/#delete-fleets","title":"Delete fleets","text":"<p>When a fleet isn't used by a run, you can delete it by passing the fleet configuration to <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f cluster.dstack.yaml\nDelete the fleet my-gcp-fleet? [y/n]: y\nFleet my-gcp-fleet deleted\n</code></pre> <p>Alternatively, you can delete a fleet by passing the fleet name  to <code>dstack fleet delete</code>. To terminate and delete specific instances from a fleet, pass <code>-i INSTANCE_NUM</code>.</p> <p>What's next?</p> <ol> <li>Read about dev environments, tasks, and services</li> </ol>"},{"location":"docs/concepts/gateways/","title":"Gateways","text":"<p>Gateways manage the ingress traffic of running services, provide an HTTPS endpoint mapped to your domain, and handle auto-scaling.</p> <p>If you're using dstack Sky , the gateway is already set up for you.</p>"},{"location":"docs/concepts/gateways/#define-a-configuration","title":"Define a configuration","text":"<p>First, define a gateway configuration as a YAML file in your project folder. The filename must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>gateway.dstack.yml</code> are both acceptable).</p> <pre><code>type: gateway\n# A name of the gateway\nname: example-gateway\n\n# Gateways are bound to a specific backend and region\nbackend: aws\nregion: eu-west-1\n\n# This domain will be used to access the endpoint\ndomain: example.com\n</code></pre> <p>A domain name is required to create a gateway.</p> <p>Reference</p> <p>For all gateway configuration options, refer to the reference.</p>"},{"location":"docs/concepts/gateways/#create-or-update-a-gateway","title":"Create or update a gateway","text":"<p>To create or update the gateway, simply call the <code>dstack apply</code> command:</p> <pre><code>$ dstack apply -f gateway.dstack.yml\nThe example-gateway doesn't exist. Create it? [y/n]: y\n\n BACKEND  REGION     NAME             HOSTNAME  DOMAIN       DEFAULT  STATUS\n aws      eu-west-1  example-gateway            example.com  \u2713        submitted\n</code></pre>"},{"location":"docs/concepts/gateways/#update-dns-records","title":"Update DNS records","text":"<p>Once the gateway is assigned a hostname, go to your domain's DNS settings and add a DNS record for <code>*.&lt;gateway domain&gt;</code>, e.g. <code>*.example.com</code>. The record should point to the gateway's hostname shown in <code>dstack</code> and should be of type <code>A</code> if the hostname is an IP address (most cases), or of type <code>CNAME</code> if the hostname is another domain (some private gateways and Kubernetes).</p>"},{"location":"docs/concepts/gateways/#manage-gateways","title":"Manage gateways","text":""},{"location":"docs/concepts/gateways/#list-gateways","title":"List gateways","text":"<p>The <code>dstack gateway list</code> command lists existing gateways and their status.</p>"},{"location":"docs/concepts/gateways/#delete-a-gateway","title":"Delete a gateway","text":"<p>To delete a gateway, pass the gateway configuration to <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f examples/deployment/gateway.dstack.yml\n</code></pre> <p>Alternatively, you can delete a gateway by passing the gateway name  to <code>dstack gateway delete</code>.</p> <p>What's next?</p> <ol> <li>See services on how to run services</li> </ol>"},{"location":"docs/concepts/repos/","title":"Repos","text":"<p>Running a dev environment, task, or service with <code>dstack apply</code> in a directory mounts the contents of that directory to the container\u2019s <code>/workflow</code> directory and sets it as the container\u2019s current working directory. This allows accessing the directory files from within the run.</p>"},{"location":"docs/concepts/repos/#initialize-a-repo","title":"Initialize a repo","text":"<p>To use a directory with <code>dstack apply</code>, it must first be initialized as a repo by running <code>dstack init</code>. The directory can be either a regular local directory or a cloned Git repo.</p> <p><code>dstack init</code> is not required if you pass <code>-P</code> (or <code>--repo</code>) to <code>dstack apply</code> (see below).</p>"},{"location":"docs/concepts/repos/#git-credentials","title":"Git credentials","text":"<p>If the directory is a cloned Git repo, <code>dstack init</code> grants the <code>dstack</code> server access by uploading the current user's default Git credentials, ensuring that dstack can clone the Git repo when running the container.</p> <p>To use custom credentials, pass them directly with <code>--token</code> (GitHub token) or <code>--git-identity</code> (path to a private SSH key).</p> <p>The <code>dstack</code> server stores Git credentials individually for each <code>dstack</code> user and encrypts them if encryption is enabled.</p>"},{"location":"docs/concepts/repos/#gitignore-and-folder-size","title":".gitignore and folder size","text":"<p>If the directory is cloned Git repo, <code>dstack apply</code> uploads to the <code>dstack</code> server only local changes. If the directory is not a cloned Git repo, it uploads the entire directory.</p> <p>Uploads are limited to 2MB. Use <code>.gitignore</code> to exclude unnecessary files from being uploaded.</p>"},{"location":"docs/concepts/repos/#initialize-as-a-local-directory","title":"Initialize as a local directory","text":"<p>If the directory is a cloned Git repo but you want to initialize it as a regular local directory, use <code>--local</code> with <code>dstack init</code>.</p>"},{"location":"docs/concepts/repos/#specify-the-repo","title":"Specify the repo","text":"<p>By default, <code>dstack apply</code> uses the current directory as a repo and requires <code>dstack init</code>. You can change this by explicitly specifying the repo to use for <code>dstack apply</code>.</p>"},{"location":"docs/concepts/repos/#pass-the-repo-path","title":"Pass the repo path","text":"<p>To use a specific directory as the repo, specify its path using <code>-P</code> (or <code>--repo</code>):</p> <pre><code>$ dstack apply -f .dstack.yml -P ../parent_dir \n</code></pre>"},{"location":"docs/concepts/repos/#pass-a-remote-git-repo-url","title":"Pass a remote Git repo URL","text":"<p>To use a remote Git repo without cloning it locally, specify the repo URL with <code>-P</code> (or <code>--repo</code>):</p> <pre><code>$ dstack apply -f .dstack.yml -P https://github.com/dstackai/dstack.git\n</code></pre>"},{"location":"docs/concepts/repos/#automatic-initialization","title":"Automatic initialization","text":"<p>When specifying the repo with <code>-P</code> (or <code>--repo</code>), the repo is initialized automatically and <code>dstack init</code> is not required. If you use a private Git repo, you can pass Git credentials to <code>dstack apply</code> using <code>--token</code> or <code>--git-identity</code>.</p>"},{"location":"docs/concepts/repos/#do-not-use-a-repo","title":"Do not use a repo","text":"<p>To run a configuration without a repo (the <code>/workflow</code> directory inside the container will be empty), use <code>--no-repo</code>:</p> <pre><code>$ dstack apply -f .dstack.yml --no-repo\n</code></pre>"},{"location":"docs/concepts/repos/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks, services</li> </ol>"},{"location":"docs/concepts/services/","title":"Services","text":"<p>Services allow you to deploy models or web apps as secure and scalable endpoints.</p>"},{"location":"docs/concepts/services/#define-a-configuration","title":"Define a configuration","text":"<p>First, define a service configuration as a YAML file in your project folder. The filename must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\nname: llama31\n\n# If `image` is not specified, dstack uses its default image\npython: \"3.11\"\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL_ID\n    --max-model-len $MAX_MODEL_LEN\n    --tensor-parallel-size $DSTACK_GPUS_NUM\nport: 8000\n# (Optional) Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: 24GB\n</code></pre>"},{"location":"docs/concepts/services/#replicas-and-scaling","title":"Replicas and scaling","text":"<p>By default, <code>dstack</code> runs a single replica of the service. You can configure the number of replicas as well as the auto-scaling rules.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama31-service\n\npython: \"3.10\"\n\n# Required environment variables\nenv:\n  - HF_TOKEN\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096\n# Expose the port of the service\nport: 8000\n\nresources:\n  # Change to what is required\n  gpu: 24GB\n\n# Minimum and maximum number of replicas\nreplicas: 1..4\nscaling:\n  # Requests per seconds\n  metric: rps\n  # Target metric value\n  target: 10\n</code></pre> <p>The <code>replicas</code> property can be a number or a range.</p> <p>The <code>metric</code> property of <code>scaling</code> only supports the <code>rps</code> metric (requests per second). In this  case <code>dstack</code> adjusts the number of replicas (scales up or down) automatically based on the load. </p> <p>Setting the minimum number of replicas to <code>0</code> allows the service to scale down to zero when there are no requests.</p> <p>Gateways</p> <p>The <code>scaling</code> property currently requires creating a gateway. This requirement is expected to be removed soon.</p>"},{"location":"docs/concepts/services/#authorization","title":"Authorization","text":"<p>By default, the service enables authorization, meaning the service endpoint requires a <code>dstack</code> user token. This can be disabled by setting <code>auth</code> to <code>false</code>.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Disable authorization\nauth: false\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre>"},{"location":"docs/concepts/services/#path-prefix","title":"Path prefix","text":"<p>If your <code>dstack</code> project doesn't have a gateway, services are hosted with the <code>/proxy/services/&lt;project name&gt;/&lt;run name&gt;/</code> path prefix in the URL. When running web apps, you may need to set some app-specific settings so that browser-side scripts and CSS work correctly with the path prefix.</p> <pre><code>type: service\nname: dash\ngateway: false\n\n# Disable authorization\nauth: false\n# Do not strip the path prefix\nstrip_prefix: false\n\nenv:\n  # Configure Dash to work with a path prefix\n  # Replace `main` with your dstack project name\n  - DASH_ROUTES_PATHNAME_PREFIX=/proxy/services/main/dash/\n\ncommands:\n  - pip install dash\n  # Assuming the Dash app is in your repo at app.py\n  - python app.py\n\nport: 8050\n</code></pre> <p>By default, <code>dstack</code> strips the prefix before forwarding requests to your service, so to the service it appears as if the prefix isn't there. This allows some apps to work out of the box. If your app doesn't expect the prefix to be stripped, set <code>strip_prefix</code> to <code>false</code>.</p> <p>If your app cannot be configured to work with a path prefix, you can host it on a dedicated domain name by setting up a gateway.</p>"},{"location":"docs/concepts/services/#model","title":"Model","text":"<p>If the service is running a chat model with an OpenAI-compatible interface, set the <code>model</code> property to make the model accessible via <code>dstack</code>'s  global OpenAI-compatible endpoint, and also accessible via <code>dstack</code>'s UI.</p>"},{"location":"docs/concepts/services/#resources","title":"Resources","text":"<p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama31-service\n\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size $DSTACK_GPUS_NUM\n# Expose the port of the service\nport: 8000\n\nresources:\n  # 2 GPUs of 80GB\n  gpu: 80GB:2\n\n  # Minimum disk size\n  disk: 200GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: service\nname: llama31-service-optimum-tpu\n\nimage: dstackai/optimum-tpu:llama31\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_TOTAL_TOKENS=4096\n  - MAX_BATCH_PREFILL_TOKENS=4095\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\nresources:\n  gpu: v5litepod-4\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single TPU device workloads. Multi-TPU support is coming soon.</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/concepts/services/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the corresponding property to true.</p> <p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\n # Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre>"},{"location":"docs/concepts/services/#docker","title":"Docker","text":"<p>If you want, you can specify your own Docker image via <code>image</code>.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Any custom Docker image\nimage: dstackai/base:py3.13-0.6-cuda-12.1\n\n# Commands of the service\ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: http-server-service\n\n# Any private Docker iamge\nimage: dstackai/base:py3.13-0.6-cuda-12.1\n# Credentials of the private registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\n# Commands of the service  \ncommands:\n  - python3 -m http.server\n# The port of the service\nport: 8000\n</code></pre> Privileged mode <p>All backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code> support running containers in privileged mode. This mode enables features like using Docker and Docker Compose  inside <code>dstack</code> runs.</p>"},{"location":"docs/concepts/services/#environment-variables","title":"Environment variables","text":"<pre><code>type: service\n# The name is optional, if not specified, generated randomly\nname: llama-2-7b-service\n\npython: \"3.10\"\n\n# Environment variables\nenv:\n  - HF_TOKEN\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\n# Commands of the service\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\n# The port of the service\nport: 8000\n\nresources:\n  # Required GPU vRAM\n  gpu: 24GB\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HF_TOKEN</code> above), <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> System environment variables <p>The following environment variables are available in any run by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run"},{"location":"docs/concepts/services/#spot-policy","title":"Spot policy","text":"<p>By default, <code>dstack</code> uses on-demand instances. However, you can change that via the <code>spot_policy</code> property. It accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>.</p> <p>Reference</p> <p>Services support many more configuration options, incl. <code>backends</code>,  <code>regions</code>,  <code>max_price</code>, and among others.</p>"},{"location":"docs/concepts/services/#optional-set-up-a-gateway","title":"(Optional) Set up a gateway","text":"<p>Running services doesn't require gateways unless you need to enable auto-scaling or want the endpoint to use HTTPS and map it to your domain.</p> <p>Websockets and path prefix</p> <p>A gateway may also be required if the service needs Websockets or cannot be used with  a path prefix.</p> <p>If you're using dstack Sky , a gateway is already pre-configured for you.</p>"},{"location":"docs/concepts/services/#run-a-configuration","title":"Run a configuration","text":"<p>To run a service, pass the configuration to <code>dstack apply</code>:</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f service.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:3  yes   $0.33\n\nSubmit the run llama31? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nService is published at: \n  http://localhost:3000/proxy/services/main/llama31/\nModel meta-llama/Meta-Llama-3.1-8B-Instruct is published at:\n  http://localhost:3000/proxy/models/main/\n</code></pre> <p><code>dstack apply</code> automatically provisions instances, uploads the contents of the repo (incl. your local uncommitted changes), and runs the service.</p>"},{"location":"docs/concepts/services/#retry-policy","title":"Retry policy","text":"<p>By default, if <code>dstack</code> can't find capacity, the task exits with an error, or the instance is interrupted,  the run will fail.</p> <p>If you'd like <code>dstack</code> to automatically retry, configure the  retry property accordingly:</p>"},{"location":"docs/concepts/services/#access-the-endpoint","title":"Access the endpoint","text":"<p>If a gateway is not configured, the service\u2019s endpoint will be accessible at <code>&lt;dstack server URL&gt;/proxy/services/&lt;project name&gt;/&lt;run name&gt;/</code>.</p> <pre><code>$ curl http://localhost:3000/proxy/services/main/llama31/v1/chat/completions \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -d '{\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Compose a poem that explains the concept of recursion in programming.\"\n            }\n        ]\n    }'\n</code></pre> <p>If the service defines the <code>model</code> property, the model can be accessed with the global OpenAI-compatible endpoint at <code>&lt;dstack server URL&gt;/proxy/models/&lt;project name&gt;/</code>, or via <code>dstack</code> UI.</p> Gateway <p>If a gateway is configured, the service endpoint will be accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;/</code>.</p> <p>If the service defines the <code>model</code> property, the model will be available via the global OpenAI-compatible endpoint  at <code>https://gateway.&lt;gateway domain&gt;/</code>.</p> <p>If authorization is not disabled, the service endpoint requires the <code>Authorization</code> header with <code>Bearer &lt;dstack token&gt;</code>.</p> <p>What's next?</p> <ol> <li>Read about dev environments, tasks, and repos</li> <li>Learn how to manage fleets</li> <li>See how to set up gateways</li> <li>Check the TGI ,    vLLM , and     NIM  examples</li> </ol>"},{"location":"docs/concepts/tasks/","title":"Tasks","text":"<p>A task allows you to run arbitrary commands on one or more nodes. They are best suited for jobs like training or batch processing.</p>"},{"location":"docs/concepts/tasks/#define-a-configuration","title":"Define a configuration","text":"<p>First, define a task configuration as a YAML file in your project folder. The filename must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: axolotl-train\n\n# Using the official Axolotl's Docker image\nimage: winglian/axolotl-cloud:main-20240429-py3.11-cu121-2.2.1\n\n# Required environment variables\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\n# Commands of the task\ncommands:\n  - accelerate launch -m axolotl.cli.train examples/fine-tuning/axolotl/config.yaml\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # Two or more GPU\n    count: 2..\n</code></pre>"},{"location":"docs/concepts/tasks/#ports","title":"Ports","text":"<p>A task can configure ports. In this case, if the task is running an application on a port, <code>dstack apply</code>  will securely allow you to access this port from your local machine through port forwarding.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: streamlit-hello\n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\n# Expose the port to access the web app\nports: \n  - 8501\n</code></pre> <p>When running it, <code>dstack apply</code> forwards <code>8501</code> port to <code>localhost:8501</code>, enabling secure access to the running application.</p>"},{"location":"docs/concepts/tasks/#distributed-tasks","title":"Distributed tasks","text":"<p>By default, a task runs on a single node. However, you can run it on a cluster of nodes by specifying <code>nodes</code>.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train-distrib\n\n# The size of the cluster\nnodes: 2\n\npython: \"3.12\"\n\n# Commands to run on each node\ncommands:\n  - git clone https://github.com/pytorch/examples.git\n  - cd examples/distributed/ddp-tutorial-series\n  - pip install -r requirements.txt\n  - torchrun\n    --nproc-per-node=$DSTACK_GPUS_PER_NODE\n    --node-rank=$DSTACK_NODE_RANK\n    --nnodes=$DSTACK_NODES_NUM\n    --master-addr=$DSTACK_MASTER_NODE_IP\n    --master-port=12345\n    multinode.py 50 10\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre> <p>Nodes can communicate using their private IP addresses. Use <code>DSTACK_MASTER_NODE_IP</code>, <code>$DSTACK_NODE_RANK</code>, and other System environment variables to discover IP addresses and other details.</p> Network interface <p>Distributed frameworks usually detect the correct network interface automatically, but sometimes you need to specify it explicitly.</p> <p>For example, with PyTorch and the NCCL backend, you may need to add these commands to tell NCCL to use the private interface:</p> <pre><code>commands:\n  - apt-get install -y iproute2\n  - &gt;\n    if [[ $DSTACK_NODE_RANK == 0 ]]; then\n      export NCCL_SOCKET_IFNAME=$(ip -4 -o addr show | fgrep $DSTACK_MASTER_NODE_IP | awk '{print $2}')\n    else\n      export NCCL_SOCKET_IFNAME=$(ip route get $DSTACK_MASTER_NODE_IP | sed -E 's/.*?dev (\\S+) .*/\\1/;t;d')\n    fi\n  # ... The rest of the commands\n</code></pre> <p>Fleets</p> <p>Distributed tasks can only run on fleets with cluster placement. While <code>dstack</code> can provision such fleets automatically, it is recommended to create them via a fleet configuration to ensure the highest level of inter-node connectivity.</p> <p><code>dstack</code> is easy to use with <code>accelerate</code>, <code>torchrun</code>, Ray, Spark, and any other distributed frameworks.</p>"},{"location":"docs/concepts/tasks/#resources","title":"Resources","text":"<p>When you specify a resource value like <code>cpu</code> or <code>memory</code>, you can either use an exact value (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nresources:\n  # 200GB or more RAM\n  memory: 200GB..\n  # 4 GPUs from 40GB to 80GB\n  gpu: 40GB..80GB:4\n  # Shared memory (required by multi-gpu)\n  shm_size: 16GB\n  # Disk size\n  disk: 500GB\n</code></pre> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU vendor, names and their quantity. Examples: <code>nvidia</code> (one NVIDIA GPU), <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100), <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), <code>A100:40GB:2</code> (two A100 GPUs of 40GB).</p> Google Cloud TPU <p>To use TPUs, specify its architecture via the <code>gpu</code> property.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nresources:\n  gpu: v2-8\n</code></pre> <p>Currently, only 8 TPU cores can be specified, supporting single TPU device workloads. Multi-TPU support is coming soon.</p> Shared memory <p>If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure  <code>shm_size</code>, e.g. set it to <code>16GB</code>.</p>"},{"location":"docs/concepts/tasks/#python-version","title":"Python version","text":"<p>If you don't specify <code>image</code>, <code>dstack</code> uses its base Docker image pre-configured with  <code>python</code>, <code>pip</code>, <code>conda</code> (Miniforge), and essential CUDA drivers.  The <code>python</code> property determines which default Docker image is used.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> nvcc <p>By default, the base Docker image doesn\u2019t include <code>nvcc</code>, which is required for building custom CUDA kernels.  If you need <code>nvcc</code>, set the corresponding property to true.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# If `image` is not specified, dstack uses its base image\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre>"},{"location":"docs/concepts/tasks/#docker","title":"Docker","text":"<p>If you want, you can specify your own Docker image via <code>image</code>.</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\n# Any custom Docker image\nimage: dstackai/base:py3.13-0.6-cuda-12.1\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> Private registry <p>Use the <code>registry_auth</code> property to provide credentials for a private Docker registry.</p> <pre><code>type: dev-environment\n# The name is optional, if not specified, generated randomly\nname: train\n\n# Any private Docker image\nimage: dstackai/base:py3.13-0.6-cuda-12.1\n# Credentials of the private Docker registry\nregistry_auth:\n  username: peterschmidt85\n  password: ghp_e49HcZ9oYwBzUbcSk2080gXZOU2hiT9AeSR5\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> Privileged mode <p>All backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code> support running containers in privileged mode. This mode enables features like using Docker and Docker Compose  inside <code>dstack</code> runs.</p>"},{"location":"docs/concepts/tasks/#environment-variables","title":"Environment variables","text":"<pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train\n\npython: \"3.10\"\n\n# Environment variables\nenv:\n  - HF_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n</code></pre> <p>If you don't assign a value to an environment variable (see <code>HF_TOKEN</code> above),  <code>dstack</code> will require the value to be passed via the CLI or set in the current process.</p> <p></p> System environment variables <p>The following environment variables are available in any run by default:</p> Name Description <code>DSTACK_RUN_NAME</code> The name of the run <code>DSTACK_REPO_ID</code> The ID of the repo <code>DSTACK_GPUS_NUM</code> The total number of GPUs in the run <code>DSTACK_NODES_NUM</code> The number of nodes in the run <code>DSTACK_GPUS_PER_NODE</code> The number of GPUs per node <code>DSTACK_NODE_RANK</code> The rank of the node <code>DSTACK_MASTER_NODE_IP</code> The internal IP address of the master node <code>DSTACK_NODES_IPS</code> The list of internal IP addresses of all nodes delimited by \"\\n\""},{"location":"docs/concepts/tasks/#spot-policy","title":"Spot policy","text":"<p>By default, <code>dstack</code> uses on-demand instances. However, you can change that via the <code>spot_policy</code> property. It accepts <code>spot</code>, <code>on-demand</code>, and <code>auto</code>.</p> <p>Reference</p> <p>Tasks support many more configuration options, incl. <code>backends</code>,  <code>regions</code>,  <code>max_price</code>, and <code>max_duration</code>,  among others.</p>"},{"location":"docs/concepts/tasks/#run-a-configuration","title":"Run a configuration","text":"<p>To run a task, pass the configuration to <code>dstack apply</code>:</p> <pre><code>$ HF_TOKEN=...\n$ WANDB_API_KEY=...\n$ dstack apply -f examples/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB:2  yes   $0.22\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:3  yes   $0.33\n\nSubmit the run axolotl-train? [y/n]: y\n\nLaunching `axolotl-train`...\n---&gt; 100%\n\n{'loss': 1.4967, 'grad_norm': 1.2734375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n  0% 1/24680 [00:13&lt;95:34:17, 13.94s/it]\n  6% 73/1300 [00:48&lt;13:57,  1.47it/s]\n</code></pre> <p><code>dstack apply</code> automatically provisions instances, uploads the contents of the repo (incl. your local uncommitted changes), and runs the commands.</p>"},{"location":"docs/concepts/tasks/#retry-policy","title":"Retry policy","text":"<p>By default, if <code>dstack</code> can't find capacity, the task exits with an error, or the instance is interrupted,  the run will fail.</p> <p>If you'd like <code>dstack</code> to automatically retry, configure the  retry property accordingly:</p> <pre><code>type: task\n# The name is optional, if not specified, generated randomly\nname: train    \n\npython: \"3.10\"\n\n# Commands of the task\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\nretry:\n  # Retry on specific events\n  on_events: [no-capacity, error, interruption]\n  # Retry for up to 1 hour\n  duration: 1h\n</code></pre>"},{"location":"docs/concepts/tasks/#creation-policy","title":"Creation policy","text":"<p>By default, when you run <code>dstack apply</code> with a dev environment, task, or service, if no <code>idle</code> instances from the available fleets meet the requirements, <code>dstack</code> creates a new fleet  using configured backends.</p> <p>To ensure <code>dstack apply</code> doesn't create a new fleet but reuses an existing one, pass <code>-R</code> (or <code>--reuse</code>) to <code>dstack apply</code>.</p> <pre><code>$ dstack apply -R -f examples/.dstack.yml\n</code></pre> <p>Or, set <code>creation_policy</code> to <code>reuse</code> in the run configuration.</p>"},{"location":"docs/concepts/tasks/#idle-duration","title":"Idle duration","text":"<p>If a fleet is created automatically, it stays <code>idle</code> for 5 minutes by default and can be reused within that time. If the fleet is not reused within this period, it is automatically terminated. To change the default idle duration, set <code>idle_duration</code> in the run configuration (e.g., <code>0s</code>, <code>1m</code>, or <code>off</code> for unlimited).</p> <p>Fleets</p> <p>For greater control over fleet provisioning, it is recommended to create fleets explicitly.</p>"},{"location":"docs/concepts/tasks/#manage-runs","title":"Manage runs","text":""},{"location":"docs/concepts/tasks/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code>  command lists all running jobs and their statuses. Use <code>--watch</code> (or <code>-w</code>) to monitor the live status of runs.</p>"},{"location":"docs/concepts/tasks/#stop-a-run","title":"Stop a run","text":"<p>A dev environment runs until you stop it or its lifetime exceeds <code>max_duration</code>. To gracefully stop a dev environment, use <code>dstack stop</code>. Pass <code>--abort</code> or <code>-x</code> to stop without waiting for a graceful shutdown.</p>"},{"location":"docs/concepts/tasks/#attach-to-a-run","title":"Attach to a run","text":"<p>By default, <code>dstack apply</code> runs in attached mode \u2013 it establishes the SSH tunnel to the run, forwards ports, and shows real-time logs. If you detached from a run, you can reattach to it using <code>dstack attach</code>.</p>"},{"location":"docs/concepts/tasks/#see-run-logs","title":"See run logs","text":"<p>To see the logs of a run without attaching, use <code>dstack logs</code>. Pass <code>--diagnose</code>/<code>-d</code> to <code>dstack logs</code> to see the diagnostics logs. It may be useful if a run fails. For more information on debugging failed runs, see the troubleshooting guide.</p> <p>What's next?</p> <ol> <li>Read about dev environments, services, and repos</li> <li>Learn how to manage fleets</li> <li>Check the Axolotl example</li> </ol>"},{"location":"docs/concepts/volumes/","title":"Volumes","text":"<p>Volumes enable data persistence between runs of dev environments, tasks, and services. </p> <p><code>dstack</code> supports two kinds of volumes: </p> <ul> <li>Network volumes \u2014 provisioned via backends and mounted to specific container directories.   Ideal for persistent storage.</li> <li>Instance volumes \u2014 bind directories on the host instance to container directories. Useful as a cache for cloud fleets or for persistent storage with SSH fleets.</li> </ul>"},{"location":"docs/concepts/volumes/#network-volumes","title":"Network volumes","text":"<p>Network volumes are currently supported for the <code>aws</code>, <code>gcp</code>, and <code>runpod</code> backends.</p>"},{"location":"docs/concepts/volumes/#define-a-configuration","title":"Define a configuration","text":"<p>First, define a volume configuration as a YAML file in your project folder. The filename must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>volume.dstack.yml</code> are both acceptable).</p> <pre><code>type: volume\n# A name of the volume\nname: my-volume\n\n# Volumes are bound to a specific backend and region\nbackend: aws\nregion: eu-central-1\n\n# Required size\nsize: 100GB\n</code></pre> <p>If you use this configuration, <code>dstack</code> will create a new volume based on the specified options.</p> Register existing volumes <p>If you prefer not to create a new volume but to reuse an existing one (e.g., created manually), you can  specify its ID via <code>volume_id</code>. In this case, <code>dstack</code> will register the specified volume so that you can use it with dev environments, tasks, and services.</p> <p> <pre><code>type: volume\n# The name of the volume\nname: my-volume\n\n# Volumes are bound to a specific backend and region\nbackend: aws\nregion: eu-central-1\n\n# The ID of the volume in AWS\nvolume_id: vol1235\n</code></pre> <p>Filesystem</p> <p>If you register an existing volume, you must ensure the volume already has a filesystem.</p> <p>Reference</p> <p>For all volume configuration options, refer to the reference.</p>"},{"location":"docs/concepts/volumes/#create-register-or-update-a-volume","title":"Create, register, or update a volume","text":"<p>To create or register the volume, pass the volume configuration to <code>dstack apply</code>:</p> <pre><code>$ dstack apply -f volume.dstack.yml\nVolume my-volume does not exist yet. Create the volume? [y/n]: y\n\n NAME       BACKEND  REGION        STATUS     CREATED \n my-volume  aws      eu-central-1  submitted  now     \n</code></pre> <p>Once created, the volume can be attached to dev environments, tasks, and services.</p> <p>Filesystem</p> <p>When creating a new network volume, <code>dstack</code> automatically creates an <code>ext4</code> filesystem on it.</p>"},{"location":"docs/concepts/volumes/#attach-network-volume","title":"Attach a volume","text":"<p>Dev environments, tasks, and services let you attach any number of network volumes. To attach a network volume, simply specify its name using the <code>volumes</code> property and specify where to mount its contents:</p> <pre><code>type: dev-environment\n# A name of the dev environment\nname: vscode-vol\n\nide: vscode\n\n# Map the name of the volume to any path \nvolumes:\n  - name: my-volume\n    path: /volume_data\n\n# You can also use the short syntax in the `name:path` form\n# volumes:\n#   - my-volume:/volume_data\n</code></pre> <p>Once you run this configuration, the contents of the volume will be attached to <code>/volume_data</code> inside the dev environment,  and its contents will persist across runs.</p> <p>Attach volumes across regions and backends</p> <p>If you're unsure in advance which region or backend you'd like to use (or which is available), you can specify multiple volumes for the same path.</p> <pre><code>volumes:\n  - name: [my-aws-eu-west-1-volume, my-aws-us-east-1-volume]\n    path: /volume_data\n</code></pre> <p><code>dstack</code> will attach one of the volumes based on the region and backend of the run.  </p> Container path <p>When you're running a dev environment, task, or service with <code>dstack</code>, it automatically mounts the project folder contents to <code>/workflow</code> (and sets that as the current working directory). Right now, <code>dstack</code> doesn't allow you to  attach volumes to <code>/workflow</code> or any of its subdirectories.</p>"},{"location":"docs/concepts/volumes/#detach-network-volume","title":"Detach a volume","text":"<p><code>dstack</code> automatically detaches volumes from instances when a run stops.</p> <p>Force detach</p> <p>In some clouds such as AWS a volume may stuck in the detaching state. To fix this, you can abort the run, and <code>dstack</code> will force detach the volume. <code>dstack</code> will also force detach the stuck volume automatically after <code>stop_duration</code>. Note that force detaching a volume is a last resort measure and may corrupt the file system. Contact your cloud support if you're experience volumes stuck in the detaching state.</p>"},{"location":"docs/concepts/volumes/#manage-network-volumes","title":"Manage volumes","text":""},{"location":"docs/concepts/volumes/#list-volumes","title":"List volumes","text":"<p>The <code>dstack volume list</code> command lists created and registered volumes:</p> <pre><code>$ dstack volume list\nNAME        BACKEND  REGION        STATUS  CREATED\n my-volume  aws      eu-central-1  active  3 weeks ago\n</code></pre>"},{"location":"docs/concepts/volumes/#delete-volumes","title":"Delete volumes","text":"<p>When the volume isn't attached to any active dev environment, task, or service, you can delete it by passing the volume configuration to <code>dstack delete</code>:</p> <pre><code>$ dstack delete -f vol.dstack.yaml\n</code></pre> <p>Alternatively, you can delete a volume by passing the volume name  to <code>dstack volume delete</code>.</p> <p>If the volume was created using <code>dstack</code>, it will be physically destroyed along with the data. If you've registered an existing volume, it will be de-registered with <code>dstack</code> but will keep the data.</p>"},{"location":"docs/concepts/volumes/#faqs","title":"FAQs","text":"Can I use network volumes across backends? <p>Since volumes are backed up by cloud network disks, you can only use them within the same cloud. If you need to access data across different backends, you should either use object storage or replicate the data across multiple volumes.</p> Can I use network volumes across regions? <p>Typically, network volumes are associated with specific regions, so you can't use them in other regions. Often, volumes are also linked to availability zones, but some providers support volumes that can be used across different availability zones within the same region.</p> <p>If you don't want to limit a run to one particular region, you can create different volumes for different regions and specify them for the same mount point as documented above.</p> Can I attach network volumes to multiple runs or instances? <p>You can mount a volume in multiple runs. This feature is currently supported only by the <code>runpod</code> backend.</p>"},{"location":"docs/concepts/volumes/#instance-volumes","title":"Instance volumes","text":"<p>Instance volumes allow mapping any directory on the instance where the run is executed to any path inside the container. This means that the data in instance volumes is persisted only if the run is executed on the same instance.</p>"},{"location":"docs/concepts/volumes/#attach-a-volume","title":"Attach a volume","text":"<p>A run can configure any number of instance volumes. To attach an instance volume, specify the <code>instance_path</code> and <code>path</code> in the <code>volumes</code> property:</p> <pre><code>type: dev-environment\n# A name of the dev environment\nname: vscode-vol\n\nide: vscode\n\n# Map the instance path to any container path\nvolumes:\n  - instance_path: /mnt/volume\n    path: /volume_data\n\n# You can also use the short syntax in the `instance_path:path` form\n# volumes:\n#   - /mnt/volume:/volume_data\n</code></pre> <p>Since persistence isn't guaranteed (instances may be interrupted or runs may occur on different instances), use instance volumes only for caching or with directories manually mounted to network storage.</p> <p>Backends</p> <p>Instance volumes are currently supported for all backends except <code>runpod</code>, <code>vastai</code> and <code>kubernetes</code>, and can also be used with SSH fleets.</p> Optional volumes <p>If the volume is not critical for your workload, you can mark it as <code>optional</code>.</p> <pre><code>type: task\n\nvolumes:\n  - instance_path: /dstack-cache\n    path: /root/.cache/\n    optional: true\n</code></pre> <p>Configurations with optional volumes can run in any backend, but the volume is only mounted if the selected backend supports it.</p>"},{"location":"docs/concepts/volumes/#use-instance-volumes-for-caching","title":"Use instance volumes for caching","text":"<p>For example, if a run regularly installs packages with <code>pip install</code>, you can mount the <code>/root/.cache/pip</code> folder inside the container to a folder on the instance for  reuse.</p> <pre><code>type: task\n\nvolumes:\n  - /dstack-cache/pip:/root/.cache/pip\n</code></pre>"},{"location":"docs/concepts/volumes/#use-instance-volumes-with-ssh-fleets","title":"Use instance volumes with SSH fleets","text":"<p>If you control the instances (e.g. they are on-prem servers configured via SSH fleets),  you can mount network storage (e.g., NFS or SMB) and use the mount points as instance volumes.</p> <p>For example, if you mount a network storage to <code>/mnt/nfs-storage</code> on all hosts of your SSH fleet, you can map this directory via instance volumes and be sure the data is persisted.</p> <pre><code>type: task\n\nvolumes:\n  - /mnt/nfs-storage:/storage\n</code></pre>"},{"location":"docs/guides/administration/","title":"Administration","text":"<p>Projects enable the isolation of different teams and their resources. Each project can configure its own backends and control which users have access to it.</p> <p>While project backends can be configured via <code>~/.dstack/server/config.yml</code>,  use the UI to fully manage projects, users, and user permissions.</p>"},{"location":"docs/guides/administration/#backends","title":"Project backends","text":"<p>In addition to <code>~/.dstack/server/config.yml</code>,  a global admin or a project admin can configure backends on the project settings page.</p> <p></p>"},{"location":"docs/guides/administration/#global-admins","title":"Global admins","text":"<p>A user can be assigned or unassigned a global admin role on the user account settings page. This can only be done by  another global admin.</p> <p></p> <p>The global admin role allows a user to manage all projects and users.</p>"},{"location":"docs/guides/administration/#project-members","title":"Project members","text":"<p>A user can be added to a project and assigned or unassigned as a project role on the project settings page.</p> <p></p>"},{"location":"docs/guides/administration/#project-roles","title":"Project roles","text":"<ul> <li>Admin \u2013 The project admin role allows a user to manage the project's settings,   including backends, gateways, and members.</li> <li>Manager \u2013 The project manager role allows a user to manage project members.   Unlike admins, managers cannot configure backends and gateways.</li> <li>User \u2013 A user can manage project resources including runs, fleets, and volumes.</li> </ul>"},{"location":"docs/guides/administration/#authorization","title":"Authorization","text":""},{"location":"docs/guides/administration/#user-token","title":"User token","text":"<p>Once created, a user is issued a token. This token can be found on the user account settings page. </p> <p></p> <p>The token must be used for authentication when logging into the control plane UI and when using the CLI or API.</p>"},{"location":"docs/guides/administration/#setting-up-the-cli","title":"Setting up the CLI","text":"<p>To use the CLI with a specific project, run the <code>dstack config</code> command with the server address, user token, and project name.</p> <p>You can find the command on the project\u2019s settings page:</p> <p></p> API <p>In addition to the UI, managing projects, users, and user permissions can also be done via the REST API).</p>"},{"location":"docs/guides/dstack-sky/","title":"dstack Sky","text":"<p>If you don't want to host the <code>dstack</code> server or would like to access GPU from the <code>dstack</code> marketplace,  sign up with dstack Sky.</p>"},{"location":"docs/guides/dstack-sky/#set-up-the-cli","title":"Set up the CLI","text":"<p>If you've signed up, open your project settings, and copy the <code>dstack config</code> command to point the CLI to the project.</p> <p></p> <p>Then, install the CLI on your machine and use the copied command.</p> <pre><code>$ pip install dstack\n$ dstack config --url https://sky.dstack.ai \\\n    --project peterschmidt85 \\\n    --token bbae0f28-d3dd-4820-bf61-8f4bb40815da\n\nConfiguration is updated at ~/.dstack/config.yml\n</code></pre>"},{"location":"docs/guides/dstack-sky/#configure-clouds","title":"Configure clouds","text":"<p>By default, dstack Sky   uses the GPU from its marketplace, which requires a credit card to be attached in your account settings.</p> <p>To use your own cloud accounts, click the settings icon of the corresponding backend and specify credentials:</p> <p></p> <p>For more details on how to configure your own cloud accounts, check the server/config.yml reference.</p>"},{"location":"docs/guides/dstack-sky/#whats-next","title":"What's next?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/guides/protips/","title":"Protips","text":"<p>Below are tips and tricks to use <code>dstack</code> more efficiently.</p>"},{"location":"docs/guides/protips/#fleets","title":"Fleets","text":""},{"location":"docs/guides/protips/#creation-policy","title":"Creation policy","text":"<p>By default, when you run <code>dstack apply</code> with a dev environment, task, or service, <code>dstack</code> reuses <code>idle</code> instances from an existing fleet. If no <code>idle</code> instances match the requirements, <code>dstack</code> automatically creates a new fleet  using configured backends.</p> <p>To ensure <code>dstack apply</code> doesn't create a new fleet but reuses an existing one, pass <code>-R</code> (or <code>--reuse</code>) to <code>dstack apply</code>.</p> <pre><code>$ dstack apply -R -f examples/.dstack.yml\n</code></pre>"},{"location":"docs/guides/protips/#idle-duration","title":"Idle duration","text":"<p>If a fleet is created automatically, it stays <code>idle</code> for 5 minutes by default and can be reused within that time. If the fleet is not reused within this period, it is automatically terminated. To change the default idle duration, set <code>idle_duration</code> in the run configuration (e.g., <code>0s</code>, <code>1m</code>, or <code>off</code> for unlimited).</p> <p>Fleets</p> <p>For greater control over fleet provisioning, configuration, and lifecycle management, it is recommended to use fleets directly.</p>"},{"location":"docs/guides/protips/#volumes","title":"Volumes","text":"<p>To persist data across runs, it is recommended to use volumes. <code>dstack</code> supports two types of volumes: network  (for persisting data even if the instance is interrupted) and instance (useful for persisting cached data across runs while the instance remains active).</p> <p>If you use SSH fleets, you can mount network storage (e.g., NFS or SMB) to the hosts and access it in runs via instance volumes.</p>"},{"location":"docs/guides/protips/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p> <p>Once the commands work, go ahead and run them as a task or a service.</p> Notebooks <p>VS Code</p> <p>When you access a dev environment using your desktop VS Code, it allows you to work with Jupyter notebooks via its pre-configured and easy-to-use extension.</p> <p>JupyterLab</p> <p>If you prefer to use JupyterLab, you can run it as a task:</p> <pre><code>type: task\n\ncommands:\n    - pip install jupyterlab\n    - jupyter lab --allow-root\n\nports:\n    - 8888\n</code></pre>"},{"location":"docs/guides/protips/#tasks","title":"Tasks","text":"<p>Tasks can be used not only for batch jobs but also for web applications.</p> <pre><code>type: task\nname: streamlit-task\n\npython: \"3.10\"\n\ncommands:\n  - pip3 install streamlit\n  - streamlit hello\nports: \n  - 8501\n</code></pre> <p>While you run a task, <code>dstack apply</code> forwards the remote ports to <code>localhost</code>.</p> <pre><code>$ dstack apply -f app.dstack.yml\n\n  Welcome to Streamlit. Check out our demo in your browser.\n\n  Local URL: http://localhost:8501\n</code></pre> <p>This allows you to access the remote <code>8501</code> port on <code>localhost:8501</code> while the CLI is attached.</p> Port mapping <p>If you want to override the local port, use the <code>--port</code> option:</p> <pre><code>$ dstack apply -f app.dstack.yml --port 3000:8501\n</code></pre> <p>This will forward the remote <code>8501</code> port to <code>localhost:3000</code>.</p> <p>Tasks vs. services</p> <p>Services provide external access, <code>https</code>, replicas with autoscaling, OpenAI-compatible endpoint and other service features. If you don't need them, you can use tasks for running apps.</p>"},{"location":"docs/guides/protips/#docker-and-docker-compose","title":"Docker and Docker Compose","text":"<p>All backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code> allow using Docker and Docker Compose  inside <code>dstack</code> runs. To do that, additional configuration steps are required:</p> <ol> <li>Set the <code>privileged</code> property to <code>true</code>.</li> <li>Set the <code>image</code> property to <code>dstackai/dind</code> (or another DinD image).</li> <li>For tasks and services, add <code>start-dockerd</code> as the first command. For dev environments, add <code>start-dockerd</code> as the first command    in the <code>init</code> property.</li> </ol> <p>Note, <code>start-dockerd</code> is a part of <code>dstackai/dind</code> image, if you use a different DinD image, replace it with a corresponding command to start Docker daemon.</p> TaskDev environment <pre><code>type: task\nname: task-dind\n\nprivileged: true\nimage: dstackai/dind\n\ncommands:\n  - start-dockerd\n  - docker compose up\n</code></pre> <pre><code>type: dev-environment\nname: vscode-dind\n\nprivileged: true\nimage: dstackai/dind\n\nide: vscode\n\ninit:\n  - start-dockerd\n</code></pre> Volumes <p>To persist Docker data between runs (e.g. images, containers, volumes, etc), create a <code>dstack</code> volume and add attach it in your run configuration:</p> <pre><code>    type: dev-environment\n    name: vscode-dind\n\n    privileged: true\n    image: dstackai/dind\n    ide: vscode\n\n    init:\n      - start-dockerd\n\n    volumes:\n      - name: docker-volume\n        path: /var/lib/docker\n</code></pre> <p>See more Docker examples here.</p>"},{"location":"docs/guides/protips/#environment-variables","title":"Environment variables","text":"<p>If a configuration requires an environment variable that you don't want to hardcode in the YAML, you can define it without assigning a value:</p> <pre><code>type: dev-environment\nname: vscode\n\npython: \"3.10\"\n\nenv:\n  - HF_TOKEN\nide: vscode\n</code></pre> <p>Then, you can pass the environment variable either via the shell:</p> <pre><code>$ HF_TOKEN=... \n$ dstack apply -f .dstack.yml\n</code></pre> <p>Or via the <code>-e</code> option of the <code>dstack apply</code> command:</p> <pre><code>$ dstack apply -e HF_TOKEN=... -f .dstack.yml\n</code></pre> .envrc <p>A better way to configure environment variables not hardcoded in YAML is by specifying them in a <code>.envrc</code> file:</p> <p> <pre><code>export HF_TOKEN=...\n</code></pre> <p>If you install <code>direnv</code> , it will automatically pass the environment variables from the <code>.env</code> file to the <code>dstack apply</code> command.</p> <p>Remember to add <code>.env</code> to <code>.gitignore</code> to avoid pushing it to the repo.    </p>"},{"location":"docs/guides/protips/#attached-mode","title":"Attached mode","text":"<p>By default, <code>dstack apply</code> runs in attached mode. This means it streams the logs as they come in and, in the case of a task, forwards its ports to <code>localhost</code>.</p> <p>To run in detached mode, use <code>-d</code> with <code>dstack apply</code>.</p> <p>If you detached the CLI, you can always re-attach to a run via <code>dstack attach</code>.</p>"},{"location":"docs/guides/protips/#gpu","title":"GPU","text":"<p><code>dstack</code> natively supports NVIDIA GPU, AMD GPU, and Google Cloud TPU accelerator chips.</p> <p>The <code>gpu</code> property within <code>resources</code> (or the <code>--gpu</code> option with <code>dstack apply</code>) allows specifying not only memory size but also GPU vendor, names, their memory, and quantity.</p> <p>Examples:</p> <ul> <li><code>1</code> (any GPU)</li> <li><code>amd:2</code> (two AMD GPUs)</li> <li><code>A100</code> (A100)</li> <li><code>24GB..</code> (any GPU starting from 24GB)</li> <li><code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB)</li> <li><code>A10G,A100</code> (either A10G or A100)</li> <li><code>A100:80GB</code> (one A100 of 80GB)</li> <li><code>A100:2</code> (two A100)</li> <li><code>MI300X:4</code> (four MI300X)</li> <li><code>A100:40GB:2</code> (two A100 40GB)</li> <li><code>tpu:v2-8</code> (<code>v2</code> Google Cloud TPU with 8 cores)</li> </ul> <p>The GPU vendor is indicated by one of the following case-insensitive values:</p> <ul> <li><code>nvidia</code> (NVIDIA GPUs)</li> <li><code>amd</code> (AMD GPUs)</li> <li><code>tpu</code> (Google Cloud TPUs)</li> </ul> AMD <p>Currently, when an AMD GPU is specified, either by name or by vendor, the <code>image</code> property must be specified as well.</p> TPU <p>Currently, you can't specify other than 8 TPU cores. This means only single host workloads are supported. Support for multiple hosts is coming soon.</p>"},{"location":"docs/guides/protips/#monitoring-metrics","title":"Monitoring metrics","text":"<p>While <code>dstack</code> allows the use of any third-party monitoring tools (e.g., Weights and Biases), you can also monitor container metrics such as CPU, memory, and GPU usage using the built-in <code>dstack stats</code> CLI command or the corresponding API.</p>"},{"location":"docs/guides/protips/#service-quotas","title":"Service quotas","text":"<p>If you're using your own AWS, GCP, Azure, or OCI accounts, before you can use GPUs or spot instances, you have to request the corresponding service quotas for each type of instance in each region.</p> AWS <p>Check this guide   on EC2 service quotas. The relevant service quotas include:</p> <ul> <li><code>Running On-Demand P instances</code> (on-demand V100, A100 80GB x8)</li> <li><code>All P4, P3 and P2 Spot Instance Requests</code> (spot V100, A100 80GB x8)</li> <li><code>Running On-Demand G and VT instances</code> (on-demand T4, A10G, L4)</li> <li><code>All G and VT Spot Instance Requests</code> (spot T4, A10G, L4)</li> <li><code>Running Dedicated p5 Hosts</code> (on-demand H100)</li> <li><code>All P5 Spot Instance Requests</code> (spot H100)</li> </ul> GCP <p>Check this guide   on Compute Engine service quotas. The relevant service quotas include:</p> <ul> <li><code>NVIDIA V100 GPUs</code> (on-demand V100)</li> <li><code>Preemtible V100 GPUs</code> (spot V100)</li> <li><code>NVIDIA T4 GPUs</code> (on-demand T4)</li> <li><code>Preemtible T4 GPUs</code> (spot T4)</li> <li><code>NVIDIA L4 GPUs</code> (on-demand L4)</li> <li><code>Preemtible L4 GPUs</code> (spot L4)</li> <li><code>NVIDIA A100 GPUs</code> (on-demand A100)</li> <li><code>Preemtible A100 GPUs</code> (spot A100)</li> <li><code>NVIDIA A100 80GB GPUs</code> (on-demand A100 80GB)</li> <li><code>Preemtible A100 80GB GPUs</code> (spot A100 80GB)</li> <li><code>NVIDIA H100 GPUs</code> (on-demand H100)</li> <li><code>Preemtible H100 GPUs</code> (spot H100)</li> </ul> Azure <p>Check this guide   on Azure service quotas. The relevant service quotas include:</p> <ul> <li><code>Total Regional Spot vCPUs</code> (any spot instances)</li> <li><code>Standard NCASv3_T4 Family vCPUs</code> (on-demand T4)</li> <li><code>Standard NVADSA10v5 Family vCPUs</code> (on-demand A10)</li> <li><code>Standard NCADS_A100_v4 Family vCPUs</code> (on-demand A100 80GB)</li> <li><code>Standard NDASv4_A100 Family vCPUs</code> (on-demand A100 40GB x8)</li> <li><code>Standard NDAMSv4_A100Family vCPUs</code> (on-demand A100 80GB x8)</li> <li><code>Standard NCadsH100v5 Family vCPUs</code> (on-demand H100)</li> <li><code>Standard NDSH100v5 Family vCPUs</code> (on-demand H100 x8)</li> </ul> OCI <p>Check this guide   on requesting OCI service limits increase. The relevant service category is compute. The relevant resources include:</p> <ul> <li><code>GPUs for GPU.A10 based VM and BM instances</code> (on-demand A10)</li> <li><code>GPUs for GPU2 based VM and BM instances</code> (on-demand P100)</li> <li><code>GPUs for GPU3 based VM and BM instances</code> (on-demand V100)</li> </ul> <p>Note, for AWS, GCP, and Azure, service quota values are measured with the number of CPUs rather than GPUs.</p>"},{"location":"docs/guides/server-deployment/","title":"Server deployment","text":"<p>The <code>dstack</code> server can run anywhere: on your laptop, a dedicated server, or in the cloud.</p> <p>You can run the server either through <code>pip</code> or using Docker.</p> pipDocker <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <p>The server can be set up via <code>pip</code> on Linux, macOS, and Windows (via WSL 2). It requires Git and OpenSSH. The minimum hardware requirements are 1 CPU and 1GB of RAM.</p> <pre><code>$ docker run -p 3000:3000 \\\n    -v $HOME/.dstack/server/:/root/.dstack/server \\\n    dstackai/dstack\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> AWS CloudFormation <p>If you'd like to deploy the server to a private AWS VPC, you can use  our CloudFormation template .</p> <p>First, ensure, you've set up a private VPC with public and private subnets.</p> <p></p> <p>Create a stack using the template, and specify the VPC and private subnets. Once, the stack is created, go to <code>Outputs</code> for the server URL and admin token.</p> <p>To access the server URL, ensure you're connected to the VPC, e.g. via VPN client.</p> <p>If you'd like to adjust anything, the source code of the template can be found at <code>examples/server-deployment/cloudformation/template.yaml</code> .</p>"},{"location":"docs/guides/server-deployment/#backend-configuration","title":"Backend configuration","text":"<p>To use <code>dstack</code> with cloud providers, configure backends  via the <code>~/.dstack/server/config.yml</code> file. The server loads this file on startup. </p> <p>Alternatively, you can configure backends on the project settings page via UI.</p> <p>For using <code>dstack</code> with on-prem servers, no backend configuration is required. Use SSH fleets instead.</p>"},{"location":"docs/guides/server-deployment/#state-persistence","title":"State persistence","text":"<p>The <code>dstack</code> server can store its internal state in SQLite or Postgres. By default, it stores the state locally in <code>~/.dstack/server</code> using SQLite. With SQLite, you can run at most one server replica. Postgres has no such limitation and is recommended for production deployment.</p> Replicate SQLite to cloud storage <p>You can configure automatic replication of your SQLite state to a cloud object storage using Litestream. This allows persisting the server state across re-deployments when using SQLite.</p> <p>To enable Litestream replication, set the following environment variables:</p> <ul> <li><code>LITESTREAM_REPLICA_URL</code> - The url of the cloud object storage.   Examples: <code>s3://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>gcs://&lt;bucket-name&gt;/&lt;path&gt;</code>, <code>abs://&lt;storage-account&gt;@&lt;container-name&gt;/&lt;path&gt;</code>, etc.</li> </ul> <p>You also need to configure cloud storage credentials.</p> <p>AWS S3</p> <p>To persist state into an AWS S3 bucket, provide the following environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> - The AWS access key ID</li> <li><code>AWS_SECRET_ACCESS_KEY</code> -  The AWS secret access key</li> </ul> <p>GCP Storage</p> <p>To persist state into a GCP Storage bucket, provide one of the following environment variables:</p> <ul> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> - The path to the GCP service account key JSON file</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> - The GCP service account key JSON</li> </ul> <p>Azure Blob Storage</p> <p>To persist state into an Azure blog storage, provide the following environment variable.</p> <ul> <li><code>LITESTREAM_AZURE_ACCOUNT_KEY</code> - The Azure storage account key</li> </ul> <p>More details on options for configuring replication.</p>"},{"location":"docs/guides/server-deployment/#postgresql","title":"PostgreSQL","text":"<p>To store the server state in Postgres, set the <code>DSTACK_DATABASE_URL</code> environment variable.</p> Migrate from SQLite to PostgreSQL <p>You can migrate the existing state from SQLite to PostgreSQL using <code>pgloader</code>:</p> <ol> <li>Create a new PostgreSQL database</li> <li>Clone the <code>dstack</code> repo and install <code>dstack</code> from source.    Ensure you've checked out the tag that corresponds to your server version (e.g. <code>git checkout 0.18.10</code>).</li> <li>Apply database migrations to the new database:   <pre><code>cd src/dstack/_internal/server/\nexport DSTACK_DATABASE_URL=\"postgresql+asyncpg://...\"\nalembic upgrade head\n</code></pre></li> <li>Install pgloader </li> <li>Pass the path to the <code>~/.dstack/server/data/sqlite.db</code> file to <code>SOURCE_PATH</code> and     set <code>TARGET_PATH</code> with the URL of the PostgreSQL database. Example:    <pre><code>cd scripts/\nexport SOURCE_PATH=sqlite:///Users/me/.dstack/server/data/sqlite.db\nexport TARGET_PATH=postgresql://postgres:postgres@localhost:5432/postgres\npgloader sqlite_to_psql.load\n</code></pre>    The <code>pgloader</code> script will migrate the SQLite data to PostgreSQL. It may emit warnings that are safe to ignore. </li> </ol> <p>If you encounter errors, please submit an issue.</p>"},{"location":"docs/guides/server-deployment/#logs-storage","title":"Logs storage","text":"<p>By default, <code>dstack</code> stores workload logs locally in <code>~/.dstack/server/projects/&lt;project_name&gt;/logs</code>. For multi-replica server deployments, it's required to store logs externally, e.g. in AWS CloudWatch.</p>"},{"location":"docs/guides/server-deployment/#aws-cloudwatch","title":"AWS CloudWatch","text":"<p>To store logs in AWS CloudWatch, set the <code>DSTACK_SERVER_CLOUDWATCH_LOG_GROUP</code> and the <code>DSTACK_SERVER_CLOUDWATCH_LOG_REGION</code> environment variables. </p> <p>The log group must be created beforehand, <code>dstack</code> won't try to create it.</p> Required permissions <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"DstackLogStorageAllow\",\n          \"Effect\": \"Allow\",\n          \"Action\": [\n              \"logs:DescribeLogStreams\",\n              \"logs:CreateLogStream\",\n              \"logs:GetLogEvents\",\n              \"logs:PutLogEvents\"\n          ],\n          \"Resource\": [\n              \"arn:aws:logs:::log-group:&lt;group name&gt;\",\n              \"arn:aws:logs:::log-group:&lt;group name&gt;:*\"\n          ]\n      }\n  ]\n}\n</code></pre>"},{"location":"docs/guides/server-deployment/#encryption","title":"Encryption","text":"<p>By default, <code>dstack</code> stores data in plaintext. To enforce encryption, you  specify one or more encryption keys.</p> <p><code>dstack</code> currently supports AES and identity (plaintext) encryption keys. Support for external providers like HashiCorp Vault and AWS KMS is planned.</p> AESIdentity <p>The <code>aes</code> encryption key encrypts data using AES-256 in GCM mode. To configure the <code>aes</code> encryption, generate a random 32-byte key:</p> <pre><code>$ head -c 32 /dev/urandom | base64\n\nopmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>And specify it as <code>secret</code>:</p> <pre><code># ...\n\nencryption:\n  keys:\n    - type: aes\n      name: key1\n      secret: opmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>The <code>identity</code> encryption performs no encryption and stores data in plaintext. You can specify an <code>identity</code> encryption key explicitly if you want to decrypt the data:</p> <pre><code># ...\n\nencryption:\n  keys:\n  - type: identity\n  - type: aes\n    name: key1\n    secret: opmx+r5xGJNVZeErnR0+n+ElF9ajzde37uggELxL\n</code></pre> <p>With this configuration, the <code>aes</code> key will still be used to decrypt the old data, but new writes will store the data in plaintext.</p> Key rotation <p>If multiple keys are specified, the first is used for encryption, and all are tried for decryption. This enables key rotation by specifying a new encryption key.</p> <pre><code># ...\n\nencryption:\n  keys:\n  - type: aes\n    name: key2\n    secret: cR2r1JmkPyL6edBQeHKz6ZBjCfS2oWk87Gc2G3wHVoA=\n\n  - type: aes\n    name: key1\n    secret: E5yzN6V3XvBq/f085ISWFCdgnOGED0kuFaAkASlmmO4=\n</code></pre> <p>Old keys may be deleted once all existing records have been updated to re-encrypt sensitive data.  Encrypted values are prefixed with key names, allowing DB admins to identify the keys used for encryption.</p>"},{"location":"docs/guides/server-deployment/#default-permissions","title":"Default permissions","text":"<p>By default, all users can create and manage their own projects. You can specify <code>default_permissions</code> to <code>false</code> so that only global admins can create and manage projects:</p> <pre><code># ...\n\ndefault_permissions:\n  allow_non_admins_create_projects: false\n</code></pre>"},{"location":"docs/guides/server-deployment/#backward-compatibility","title":"Backward compatibility","text":"<p><code>dstack</code> follows the <code>{major}.{minor}.{patch}</code> versioning scheme. Backward compatibility is maintained based on these principles:</p> <ul> <li>The server backward compatibility is maintained across all minor and patch releases. The specific features can be removed but the removal is preceded with deprecation warnings for several minor releases. This means you can use older client versions with newer server versions.</li> <li>The client backward compatibility is maintained across patch releases. A new minor release indicates that the release breaks client backward compatibility. This means you don't need to update the server when you update the client to a new patch release. Still, upgrading a client to a new minor version requires upgrading the server too.</li> </ul>"},{"location":"docs/guides/server-deployment/#server-limits","title":"Server limits","text":"<p>A single <code>dstack</code> server replica can support:</p> <ul> <li>Up to 150 active runs.</li> <li>Up to 150 active jobs.</li> <li>Up to 150 active instances.</li> </ul> <p>Having more active resources can affect server performance. If you hit these limits, consider using Postgres with multiple server replicas.</p>"},{"location":"docs/guides/server-deployment/#faqs","title":"FAQs","text":"Can I run multiple replicas of dstack server? <p>Yes, you can if you configure <code>dstack</code> to use PostgreSQL and AWS CloudWatch.</p> Does dstack server support blue-green or rolling deployments? <p>Yes, it does if you configure <code>dstack</code> to use PostgreSQL and AWS CloudWatch.</p>"},{"location":"docs/guides/troubleshooting/","title":"Troubleshooting","text":""},{"location":"docs/guides/troubleshooting/#reporting-issues","title":"Reporting issues","text":"<p>When you encounter a problem, please report it as a GitHub issue .</p> <p>If you have a question or need help, feel free to ask it in our Discord server.</p> <p>When bringing up issues, always include the steps to reproduce.</p>"},{"location":"docs/guides/troubleshooting/#steps-to-reproduce","title":"Steps to reproduce","text":"<p>Make sure to provide clear, detailed steps to reproduce the issue.  Include server logs, CLI outputs, and configuration samples. Avoid using screenshots for logs or errors\u2014use text instead. </p> <p>To get more detailed logs, make sure to set the <code>DSTACK_CLI_LOG_LEVEL</code> and <code>DSTACK_SERVER_LOG_LEVEL</code>  environment variables to <code>debug</code> when running the CLI and the server, respectively.</p> <p>See these examples for well-reported issues: this  and this .</p>"},{"location":"docs/guides/troubleshooting/#typical-issues","title":"Typical issues","text":""},{"location":"docs/guides/troubleshooting/#no-offers","title":"No instance offers","text":"<p>If you run <code>dstack apply</code> and don't see any instance offers, it means that <code>dstack</code> could not find instances that match the requirements in your configuration. Below are some of the reasons why this might happen.</p>"},{"location":"docs/guides/troubleshooting/#cause-1-no-capacity-providers","title":"Cause 1: No capacity providers","text":"<p>Before you can run any workloads, you need to configure a backend, create an SSH fleet, or sign up for dstack Sky . If you have configured a backend and still can't use it, check the output of <code>dstack server</code> for backend configuration errors.</p> <p>Tip: You can find a list of successfully configured backends on the project settings page in the UI.</p>"},{"location":"docs/guides/troubleshooting/#cause-2-requirements-mismatch","title":"Cause 2: Requirements mismatch","text":"<p>When you apply a configuration, <code>dstack</code> tries to find instances that match the <code>resources</code>, <code>backends</code>, <code>regions</code>, <code>instance_types</code>, <code>spot_policy</code>, and <code>max_price</code> properties from the configuration.</p> <p><code>dstack</code> will only select instances that meet all the requirements. Make sure your configuration doesn't set any conflicting requirements, such as <code>regions</code> that don't exist in the specified <code>backends</code>, or <code>instance_types</code> that don't match the specified <code>resources</code>.</p>"},{"location":"docs/guides/troubleshooting/#cause-3-too-specific-resources","title":"Cause 3: Too specific resources","text":"<p>If you set a resource requirement to an exact value, <code>dstack</code> will only select instances that have exactly that amount of resources. For example, <code>cpu: 5</code> and <code>memory: 10GB</code> will only match instances that have exactly 5 CPUs and exactly 10GB of memory.</p> <p>Typically, you will want to set resource ranges to match more instances. For example, <code>cpu: 4..8</code> and <code>memory: 10GB..</code> will match instances with 4 to 8 CPUs and at least 10GB of memory.</p>"},{"location":"docs/guides/troubleshooting/#cause-4-default-resources","title":"Cause 4: Default resources","text":"<p>By default, <code>dstack</code> uses these resource requirements: <code>cpu: 2..</code>, <code>memory: 8GB..</code>, <code>disk: 100GB..</code>. If you want to use smaller instances, override the <code>cpu</code>, <code>memory</code>, or <code>disk</code> properties in your configuration.</p>"},{"location":"docs/guides/troubleshooting/#cause-5-gpu-requirements","title":"Cause 5: GPU requirements","text":"<p>By default, <code>dstack</code> only selects instances with no GPUs or a single NVIDIA GPU. If you want to use non-NVIDIA GPUs or multi-GPU instances, set the <code>gpu</code> property in your configuration.</p> <p>Examples: <code>gpu: amd</code> (one AMD GPU), <code>gpu: A10:4..8</code> (4 to 8 A10 GPUs), <code>gpu: 8:Gaudi2</code> (8 Gaudi2 accelerators).</p> <p>If you don't specify the number of GPUs, <code>dstack</code> will only select single-GPU instances.</p>"},{"location":"docs/guides/troubleshooting/#cause-6-network-volumes","title":"Cause 6: Network volumes","text":"<p>If your run configuration uses network volumes, <code>dstack</code> will only select instances from the same backend and region as the volumes. For AWS, the availability zone of the volume and the instance should also match.</p>"},{"location":"docs/guides/troubleshooting/#cause-7-feature-support","title":"Cause 7: Feature support","text":"<p>Some <code>dstack</code> features are not supported by all backends. If your configuration uses one of these features, <code>dstack</code> will only select offers from the backends that support it.</p> <ul> <li>Cloud fleet configurations,   Instance volumes,   and Privileged containers   are supported by all backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code>.</li> <li>Clusters   and distributed tasks   are only supported by the <code>aws</code>, <code>azure</code>, <code>gcp</code>, <code>oci</code>, and <code>vultr</code> backends,   as well as SSH fleets.</li> <li>Reservations   are only supported by the <code>aws</code> backend.</li> </ul>"},{"location":"docs/guides/troubleshooting/#cause-8-dstack-sky-balance","title":"Cause 8: dstack Sky balance","text":"<p>If you are using dstack Sky , you will not see marketplace offers until you top up your balance. Alternatively, you can configure your own cloud accounts on the project settings page or use SSH fleets.</p>"},{"location":"docs/guides/troubleshooting/#provisioning-fails","title":"Provisioning fails","text":"<p>In certain cases, running <code>dstack apply</code> may show instance offers, but then produce the following output:</p> <pre><code>wet-mangust-1 provisioning completed (failed)\nAll provisioning attempts failed. This is likely due to cloud providers not having enough capacity. Check CLI and server logs for more details.\n</code></pre>"},{"location":"docs/guides/troubleshooting/#cause-1-insufficient-service-quotas","title":"Cause 1: Insufficient service quotas","text":"<p>If some runs fail to provision, it may be due to an insufficient service quota. For cloud providers like AWS, GCP, Azure, and OCI, you often need to request an increased service quota before you can use specific instances.</p>"},{"location":"docs/guides/troubleshooting/#run-starts-but-fails","title":"Run starts but fails","text":"<p>There could be several reasons for a run failing after successful provisioning. </p> <p>Termination reason</p> <p>To find out why a run terminated, use <code>--verbose</code> (or <code>-v</code>) with <code>dstack ps</code>. This will show the run's status and any failure reasons.</p> <p>Diagnostic logs</p> <p>You can get more information on why a run fails with diagnostic logs. Pass <code>--diagnose</code> (or <code>-d</code>) to <code>dstack logs</code> and you'll see logs of the run executor.</p>"},{"location":"docs/guides/troubleshooting/#cause-1-spot-interruption","title":"Cause 1: Spot interruption","text":"<p>If a run fails after provisioning with the termination reason <code>INTERRUPTED_BY_NO_CAPACITY</code>, it is likely that the run was using spot instances and was interrupted. To address this, you can either set the <code>spot_policy</code> to <code>on-demand</code> or specify the  <code>retry</code> property.</p>"},{"location":"docs/guides/troubleshooting/#services-fail-to-start","title":"Services fail to start","text":""},{"location":"docs/guides/troubleshooting/#cause-1-gateway-misconfiguration","title":"Cause 1: Gateway misconfiguration","text":"<p>If all services fail to start with a specific gateway, make sure a correct DNS record pointing to the gateway's hostname is configured.</p>"},{"location":"docs/guides/troubleshooting/#service-endpoint-doesnt-work","title":"Service endpoint doesn't work","text":""},{"location":"docs/guides/troubleshooting/#cause-1-bad-authorization","title":"Cause 1: Bad Authorization","text":"<p>If the service endpoint returns a 403 error, it is likely because the <code>Authorization</code>  header with the correct <code>dstack</code> token was not provided.</p>"},{"location":"docs/guides/troubleshooting/#cannot-access-dev-environment-or-task-ports","title":"Cannot access dev environment or task ports","text":""},{"location":"docs/guides/troubleshooting/#cause-1-detached-from-run","title":"Cause 1: Detached from run","text":"<p>When running a dev environment or task with configured ports, <code>dstack apply</code>  automatically forwards remote ports to <code>localhost</code> via SSH for easy and secure access. If you interrupt the command, the port forwarding will be disconnected. To reattach, use <code>dstack attach &lt;run name</code>.</p>"},{"location":"docs/guides/troubleshooting/#cause-2-windows","title":"Cause 2: Windows","text":"<p>If you're using the CLI on Windows, make sure to run it through WSL by following these instructions.  Native support will be available soon.</p>"},{"location":"docs/guides/troubleshooting/#ssh-fleet-fails-to-provision","title":"SSH fleet fails to provision","text":"<p>If you set up an SSH fleet and it fails to provision after a long wait, first check the server logs.  Also, review the  <code>/root/.dstack/shim.log</code> file on each host used to create the fleet.</p>"},{"location":"docs/guides/troubleshooting/#community","title":"Community","text":"<p>If you have a question, please feel free to ask it in our Discord server.</p>"},{"location":"docs/installation/","title":"Installation","text":"<p>To use the open-source version of <code>dstack</code> with your own cloud accounts or on-prem clusters, follow this guide.</p> <p>If you don't want to host the <code>dstack</code> server (or want to access GPU marketplace), skip installation and proceed to dstack Sky .</p>"},{"location":"docs/installation/#optional-configure-backends","title":"(Optional) Configure backends","text":"<p>To use <code>dstack</code> with cloud providers, configure backends.</p> <p>For using <code>dstack</code> with on-prem servers, create SSH fleets instead.</p>"},{"location":"docs/installation/#start-the-server","title":"Start the server","text":"<p>The server can run on your laptop, a dedicated server, a private cloud VPC, or any location with access to your cloud or on-prem clusters.</p> pipDocker <p>The server can be set up via <code>pip</code> on Linux, macOS, and Windows (via WSL 2). It requires Git and OpenSSH.</p> <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <pre><code>$ docker run -p 3000:3000 \\\n    -v $HOME/.dstack/server/:/root/.dstack/server \\\n    dstackai/dstack\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <p>Server deployment</p> <p>For more details on server deployment options, see the server deployment guide.</p>"},{"location":"docs/installation/#set-up-the-cli","title":"Set up the CLI","text":"<p>Once it's up, you can use either the CLI or the API.</p> <p>The CLI can be set up on Linux, macOS, and Windows. It requires Git and OpenSSH.</p> Windows <p>To use the CLI on Windows, ensure you've installed Git and OpenSSH via  Git for Windows. </p> <p>When installing it, ensure you've checked  <code>Git from the command line and also from 3-rd party software</code>  (or <code>Use Git and optional Unix tools from the Command Prompt</code>), and  <code>Use bundled OpenSSH</code>.</p> <p>To point the CLI to the <code>dstack</code> server, configure it with the server address, user token, and project name:</p> <pre><code>$ pip install dstack\n$ dstack config --url http://127.0.0.1:3000 \\\n    --project main \\\n    --token bbae0f28-d3dd-4820-bf61-8f4bb40815da\n\nConfiguration is updated at ~/.dstack/config.yml\n</code></pre> <p>This configuration is stored in <code>~/.dstack/config.yml</code>.</p> <p>What's next?</p> <ol> <li>Check the server/config.yml reference on how to configure backends</li> <li>Check SSH fleets to learn about running on your on-prem servers</li> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord </li> </ol>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<ul> <li><code>dev-environment</code></li> <li><code>task</code></li> <li><code>service</code></li> </ul>"},{"location":"docs/reference/environment-variables/","title":"Environment variables","text":""},{"location":"docs/reference/environment-variables/#dstackyml","title":".dstack.yml","text":"<p>The following read-only environment variables are automatically propagated to configurations for dev environments, tasks, and services:</p> <ul> <li> <p><code>DSTACK_RUN_NAME</code> \u2013 The name of the run.</p> <p>The example below simply prints <code>vscode</code> to the output.</p> <pre><code>type: task\nname: vscode\n\ncommands:\n  - echo $DSTACK_RUN_NAME\n</code></pre> <p>If <code>name</code> is not set in the configuration, it is assigned a random name (e.g. <code>wet-mangust-1</code>).</p> </li> <li> <p><code>DSTACK_REPO_ID</code> \u2013 The ID of the repo.</p> </li> <li> <p><code>DSTACK_GPUS_NUM</code> \u2013 The total number of GPUs in the run.</p> <p>Example:</p> <pre><code>type: service\nname: llama31\n\nenv:\n  - HF_TOKEN\ncommands:\n  - pip install vllm\n  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct\n    --max-model-len 4096\n    --tensor-parallel-size $DSTACK_GPUS_NUM\nport: 8000\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\nresources:\n  gpu: 24GB\n</code></pre> </li> <li> <p><code>DSTACK_NODES_NUM</code> \u2013 The number of nodes in the run</p> </li> <li><code>DSTACK_GPUS_PER_NODE</code> \u2013 The number of GPUs per node</li> <li><code>DSTACK_NODE_RANK</code> \u2013 The rank of the node</li> <li> <p><code>DSTACK_MASTER_NODE_IP</code> \u2013 The internal IP address of the master node.</p> <p>Below is an example of using <code>DSTACK_NODES_NUM</code>, <code>DSTACK_GPUS_PER_NODE</code>, <code>DSTACK_NODE_RANK</code>, and <code>DSTACK_MASTER_NODE_IP</code>  for distributed training:</p> <pre><code> type: task\n name: train-distrib\n\n nodes: 2\n python: \"3.12\"\n\n commands:\n   - git clone https://github.com/pytorch/examples.git\n   - cd examples/distributed/ddp-tutorial-series\n   - pip install -r requirements.txt\n   - torchrun\n     --nproc-per-node=$DSTACK_GPUS_PER_NODE\n     --node-rank=$DSTACK_NODE_RANK\n     --nnodes=$DSTACK_NODES_NUM\n     --master-addr=$DSTACK_MASTER_NODE_IP\n     --master-port=12345\n     multinode.py 50 10\n\n resources:\n   gpu: 24GB\n   shm_size: 24GB\n</code></pre> </li> <li> <p><code>DSTACK_NODES_IPS</code> \u2013 The list of internal IP addresses of all nodes delimited by <code>\"\\n\"</code>.</p> </li> </ul>"},{"location":"docs/reference/environment-variables/#server","title":"Server","text":"<p>The following environment variables are supported by the <code>dstack</code> server and can be specified whether the server is run via <code>dstack server</code> or deployed using Docker.</p> <p>For more details on the options below, refer to the server deployment guide. </p> <ul> <li> <p><code>DSTACK_SERVER_LOG_LEVEL</code> \u2013 Has the same effect as <code>--log-level</code>. Defaults to <code>INFO</code>.</p> <p>Example:</p> <pre><code>$ DSTACK_SERVER_LOG_LEVEL=debug dstack server\n</code></pre> </li> <li> <p><code>DSTACK_SERVER_LOG_FORMAT</code> \u2013 Sets format of log output. Can be <code>rich</code>, <code>standard</code>, <code>json</code>. Defaults to <code>rich</code>.</p> </li> <li><code>DSTACK_SERVER_HOST</code> \u2013 Has the same effect as <code>--host</code>. Defaults to <code>127.0.0.1</code>.</li> <li><code>DSTACK_SERVER_PORT</code> \u2013 Has the same effect as <code>--port</code>. Defaults to <code>3000</code>.</li> <li><code>DSTACK_SERVER_ADMIN_TOKEN</code> \u2013 Has the same effect as <code>--token</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_SERVER_DIR</code> \u2013 Sets path to store data and server configs. Defaults to <code>~/.dstack/server</code>.</li> <li><code>DSTACK_DATABASE_URL</code> \u2013 The database URL to use instead of default SQLite. Currently <code>dstack</code> supports Postgres. Example: <code>postgresql+asyncpg://myuser:mypassword@localhost:5432/mydatabase</code>. Defaults to <code>None</code>.</li> <li><code>DSTACK_SERVER_CLOUDWATCH_LOG_GROUP</code> \u2013 The CloudWatch Logs group for workloads logs. If not set, the default file-based log storage is used.</li> <li><code>DSTACK_SERVER_CLOUDWATCH_LOG_REGION</code> \u2013 The CloudWatch Logs region. Defaults to <code>None</code>.</li> <li><code>DSTACK_DEFAULT_SERVICE_CLIENT_MAX_BODY_SIZE</code> \u2013 Request body size limit for services running with a gateway, in bytes. Defaults to 64 MiB.</li> <li><code>DSTACK_FORBID_SERVICES_WITHOUT_GATEWAY</code> \u2013 Forbids registering new services without a gateway if set to any value.</li> </ul> Internal environment variables <p>The following environment variables are intended for development purposes: </p> <ul> <li><code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> \u2013 Sets root logger log level. Defaults to <code>ERROR</code>.</li> <li><code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> \u2013 Sets uvicorn logger log level. Defaults to <code>ERROR</code>.</li> <li><code>DSTACK_RUNNER_VERSION</code> \u2013 Sets exact runner version for debug. Defaults to <code>latest</code>. Ignored if <code>DSTACK_RUNNER_DOWNLOAD_URL</code> is set.</li> <li><code>DSTACK_RUNNER_DOWNLOAD_URL</code> \u2013 Overrides <code>dstack-runner</code> binary download URL.</li> <li><code>DSTACK_SHIM_DOWNLOAD_URL</code> \u2013 Overrides <code>dstack-shim</code> binary download URL.</li> <li><code>DSTACK_DEFAULT_CREDS_DISABLED</code> \u2013 Disables default credentials detection if set. Defaults to <code>None</code>.</li> <li><code>DSTACK_LOCAL_BACKEND_ENABLED</code> \u2013 Enables local backend for debug if set. Defaults to <code>None</code>.</li> </ul>"},{"location":"docs/reference/environment-variables/#cli","title":"CLI","text":"<p>The following environment variables are supported by the CLI.</p> <ul> <li><code>DSTACK_CLI_LOG_LEVEL</code> \u2013 Configures CLI logging level. Defaults to <code>INFO</code>.</li> </ul> <p>Example:</p> <pre><code>$ DSTACK_CLI_LOG_LEVEL=debug dstack apply -f .dstack.yml\n</code></pre> <ul> <li><code>DSTACK_PROJECT</code> \u2013 Has the same effect as <code>--project</code>. Defaults to <code>None</code>.</li> </ul>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Sometimes, you may want to reuse the same parameters across different <code>.dstack.yml</code> configurations.</p> <p>This can be achieved by defining those parameters in a profile.</p> <p>Profiles can be defined on the repository level (via the <code>.dstack/profiles.yml</code> file in the root directory of the repository) or on the global level (via the <code>~/.dstack/profiles.yml</code> file).</p> <p>Any profile can be marked as default so that it will be applied automatically for any run. Otherwise, you can refer to a specific profile via <code>--profile NAME</code> in <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: my-profile\n\n    # The spot pololicy can be \"spot\", \"on-demand\", or \"auto\"\n    spot_policy: auto\n\n    # Limit the maximum price of the instance per hour\n    max_price: 1.5\n\n    # Stop any run if it runs longer that this duration\n    max_duration: 1d\n\n    # Use only these backends\n    backends: [azure, lambda]\n\n    # If set to true, this profile will be applied automatically\n    default: true\n</code></pre> <p>The profile configuration supports many properties. See below.</p>"},{"location":"docs/reference/profiles.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/profiles.yml/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/profiles.yml/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/profiles.yml/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/profiles.yml/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/profiles.yml/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/profiles.yml/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/profiles.yml/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/profiles.yml/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/profiles.yml/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/profiles.yml/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/profiles.yml/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/profiles.yml/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/profiles.yml/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/profiles.yml/#name","title":"<code>name</code> -  The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code>.","text":""},{"location":"docs/reference/profiles.yml/#default","title":"<code>default</code> - (Optional) If set to true, <code>dstack run</code> will use this profile by default..","text":""},{"location":"docs/reference/profiles.yml/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/profiles.yml/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/profiles.yml/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/api/python/","title":"Python API","text":"<p>The Python API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and displaying its logs.</p> <pre><code>import sys\n\nfrom dstack.api import Task, GPU, Client, Resources\n\nclient = Client.from_config()\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",  # If not specified, a random name is assigned \n    configuration=task,\n    repo=None, # Specify to mount additional files\n)\n\nrun.attach()\n\ntry:\n    for log in run.logs():\n        sys.stdout.buffer.write(log)\n        sys.stdout.buffer.flush()\nexcept KeyboardInterrupt:\n    run.stop(abort=True)\nfinally:\n    run.detach()\n</code></pre> <p>NOTE:</p> <ol> <li>The <code>configuration</code> argument in the <code>submit</code> method can be either <code>dstack.api.Task</code> or <code>dstack.api.Service</code>. </li> <li>If you create <code>dstack.api.Task</code> or <code>dstack.api.Service</code>, you may specify the <code>image</code> argument. If <code>image</code> isn't    specified, the default image will be used. For a private Docker registry, ensure you also pass the <code>registry_auth</code> argument.</li> <li>The <code>repo</code> argument in the <code>submit</code> method allows the mounting of a local folder, a remote repo, or a    programmatically created repo. In this case, the <code>commands</code> argument can refer to the files within this repo.</li> <li>The <code>attach</code> method waits for the run to start and, for <code>dstack.api.Task</code> sets up an SSH tunnel and forwards configured <code>ports</code> to <code>localhost</code>.</li> </ol>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>runs</code> <code>RunCollection</code> <p>Operations with runs.</p> <code>repos</code> <code>RepoCollection</code> <p>Operations with repositories.</p> <code>backends</code> <code>BackendCollection</code> <p>Operations with backends.</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using the default configuration from <code>~/.dstack/config.yml</code> if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>The dstack server URL (e.g. <code>http://localhost:3000/</code> or <code>https://sky.dstack.ai</code>)</p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>The dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for SSH tunneling</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>A client instance</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.RunCollection</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, regions=None, instance_types=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>Union[Task, Service]</code> <p>A run configuration.</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>The path to the configuration file, relative to the root directory of the repo.</p> <code>None</code> <code>repo</code> <code>Union[LocalRepo, RemoteRepo, VirtualRepo]</code> <p>A repo to mount to the run.</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>A list of allowed backend for provisioning.</p> <code>None</code> <code>regions</code> <code>Optional[List[str]]</code> <p>A list of cloud regions for provisioning.</p> <code>None</code> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration. Overrides the configuration's resources.</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>A spot policy for provisioning.</p> <code>None</code> <code>retry_policy</code> <code>RetryPolicy</code> <p>A retry policy.</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>The max instance running duration in seconds.</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>The max instance price in dollars per hour for provisioning.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>A working directory relative to the repo root directory</p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>A desired name of the run. Must be unique in the project. If not specified, a random name is assigned.</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>Whether local ports should be reserved in advance.</p> <code>True</code> <p>Returns:</p> Type Description <code>Run</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.RepoCollection</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Initializes the repo and configures its credentials in the project. Must be invoked before mounting the repo to a run.</p> <p>Example:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\",\n)\nclient.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Once the repo is initialized, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The repo to initialize.</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for accessing the remote repo.</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>The GitHub OAuth token to access the remote repo.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":""},{"location":"docs/reference/api/python/#nodes","title":"<code>nodes</code> - (Optional) Number of nodes. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The run name. If not specified, a random name is generated.","text":""},{"location":"docs/reference/api/python/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/api/python/#user","title":"<code>user</code> - (Optional) The user inside the container, <code>user_name_or_id[:group_name_or_id]</code> (e.g., <code>ubuntu</code>, <code>1000:1000</code>). Defaults to the default <code>image</code> user.","text":""},{"location":"docs/reference/api/python/#privileged","title":"<code>privileged</code> - (Optional) Run the container in privileged mode.","text":""},{"location":"docs/reference/api/python/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/api/python/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/api/python/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/api/python/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#single_branch","title":"<code>single_branch</code> - (Optional) Whether to clone and track only the current branch or all remote branches. Relevant only when using remote Git repos. Defaults to <code>false</code> for dev environments and to <code>true</code> for tasks and services.","text":""},{"location":"docs/reference/api/python/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/api/python/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/api/python/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/api/python/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/api/python/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/api/python/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/api/python/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/api/python/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/api/python/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/api/python/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/api/python/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/api/python/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/api/python/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/api/python/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/api/python/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/api/python/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/api/python/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/api/python/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":""},{"location":"docs/reference/api/python/#port","title":"<code>port</code> -  The port, that application listens on or the mapping.","text":""},{"location":"docs/reference/api/python/#gateway","title":"<code>gateway</code> - (Optional) The name of the gateway. Specify boolean <code>false</code> to run without a gateway. Omit to run with the default gateway.","text":""},{"location":"docs/reference/api/python/#strip_prefix","title":"<code>strip_prefix</code> - (Optional) Strip the <code>/proxy/services/&lt;project name&gt;/&lt;run name&gt;/</code> path prefix when forwarding requests to the service. Only takes effect when running the service without a gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/api/python/#model","title":"<code>model</code> - (Optional) Mapping of the model for the OpenAI-compatible endpoint provided by <code>dstack</code>. Can be a full model format definition or just a model name. If it's a name, the service is expected to expose an OpenAI-compatible API at the <code>/v1</code> path.","text":""},{"location":"docs/reference/api/python/#https","title":"<code>https</code> - (Optional) Enable HTTPS if running with a gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/api/python/#auth","title":"<code>auth</code> - (Optional) Enable the authorization. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/api/python/#replicas","title":"<code>replicas</code> - (Optional) The number of replicas. Can be a number (e.g. <code>2</code>) or a range (<code>0..4</code> or <code>1..8</code>). If it's a range, the <code>scaling</code> property is required. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#_scaling","title":"<code>scaling</code> - (Optional) The auto-scaling rules. Required if <code>replicas</code> is set to a range.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The run name. If not specified, a random name is generated.","text":""},{"location":"docs/reference/api/python/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/api/python/#user","title":"<code>user</code> - (Optional) The user inside the container, <code>user_name_or_id[:group_name_or_id]</code> (e.g., <code>ubuntu</code>, <code>1000:1000</code>). Defaults to the default <code>image</code> user.","text":""},{"location":"docs/reference/api/python/#privileged","title":"<code>privileged</code> - (Optional) Run the container in privileged mode.","text":""},{"location":"docs/reference/api/python/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/api/python/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/api/python/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/api/python/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/api/python/#single_branch","title":"<code>single_branch</code> - (Optional) Whether to clone and track only the current branch or all remote branches. Relevant only when using remote Git repos. Defaults to <code>false</code> for dev environments and to <code>true</code> for tasks and services.","text":""},{"location":"docs/reference/api/python/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/api/python/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/api/python/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/api/python/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/api/python/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/api/python/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/api/python/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/api/python/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/api/python/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/api/python/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/api/python/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/api/python/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/api/python/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/api/python/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/api/python/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/api/python/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/api/python/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>RunStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None, bind_address=None, ports_overrides=None, replica_num=0, job_num=0)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False, replica_num=0, job_num=0)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/#dstack.api.Resources","title":"<code>dstack.api.Resources</code>","text":""},{"location":"docs/reference/api/python/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/api/python/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/api/python/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/api/python/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements.","text":""},{"location":"docs/reference/api/python/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/api/python/#dstack.api.GPU","title":"<code>dstack.api.GPU</code>","text":""},{"location":"docs/reference/api/python/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator, one of: <code>nvidia</code>, <code>amd</code>, <code>google</code> (alias: <code>tpu</code>), <code>intel</code>.","text":""},{"location":"docs/reference/api/python/#name","title":"<code>name</code> - (Optional) The name of the GPU (e.g., <code>A100</code> or <code>H100</code>).","text":""},{"location":"docs/reference/api/python/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/api/python/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/api/python/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/api/python/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/api/python/#dstack.api.Disk","title":"<code>dstack.api.Disk</code>","text":""},{"location":"docs/reference/api/python/#size","title":"<code>size</code> -  Disk size.","text":""},{"location":"docs/reference/api/python/#dstack.api.LocalRepo","title":"<code>dstack.api.LocalRepo</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Example:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=LocalRepo.from_dir(\".\"), # Mount the current folder to the run\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.LocalRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>LocalRepo</code> <p>A local repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo","title":"<code>dstack.api.RemoteRepo</code>","text":"<p>Creates an instance of a remote Git repo for mounting to a submitted run.</p> <p>Using a locally checked-out remote Git repo:</p> <pre><code>repo=RemoteRepo.from_dir(repo_dir=\".\")\n</code></pre> <p>Using a remote Git repo by a URL:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\"\n)\n</code></pre> <p>Initialize the repo before mounting it.</p> <pre><code>client.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Finally, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_url","title":"<code>from_url(repo_url, repo_branch=None, repo_hash=None)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>The URL of a remote Git repo</p> required <code>repo_branch</code> <code>Optional[str]</code> <p>The name of the remote branch. Must be specified if <code>hash</code> is not specified.</p> <code>None</code> <code>repo_hash</code> <code>Optional[str]</code> <p>The hash of the revision. Must be specified if <code>branch</code> is not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo","title":"<code>dstack.api.VirtualRepo</code>","text":"<p>Allows mounting a repo created programmatically.</p> <p>Example:</p> <pre><code>virtual_repo = VirtualRepo(repo_id=\"some-unique-repo-id\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"requirements.txt\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"train.py\")\n\nrun = client.runs.submit(\n    configuration=...,\n    repo=virtual_repo,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>repo_id</code> <p>A unique identifier of the repo</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file","title":"<code>add_file(path, content)</code>","text":"<p>Adds a given file to the repo.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path inside the repo to add the file.</p> <code>content</code> <code>bytes</code> <p>The contents of the file.</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file_from_package","title":"<code>add_file_from_package(package, path)</code>","text":"<p>Includes a file from a given package to the repo.</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>Union[ModuleType, str]</code> <p>A package to include the file from.</p> <code>path</code> <code>str</code> <p>The path to the file to include to the repo. Must be relative to the package directory.</p>"},{"location":"docs/reference/api/python/#dstack.api.RegistryAuth","title":"<code>dstack.api.RegistryAuth</code>","text":""},{"location":"docs/reference/api/python/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/api/python/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/api/python/#dstack.api.Scaling","title":"<code>dstack.api.Scaling</code>","text":""},{"location":"docs/reference/api/python/#metric","title":"<code>metric</code> -  The target metric to track. Currently, the only supported value is <code>rps</code> (meaning requests per second).","text":""},{"location":"docs/reference/api/python/#target","title":"<code>target</code> -  The target value of the metric. The number of replicas is calculated based on this number and automatically adjusts (scales up or down) as this metric changes.","text":""},{"location":"docs/reference/api/python/#scale_up_delay","title":"<code>scale_up_delay</code> - (Optional) The delay in seconds before scaling up. Defaults to <code>300</code>.","text":""},{"location":"docs/reference/api/python/#scale_down_delay","title":"<code>scale_down_delay</code> - (Optional) The delay in seconds before scaling down. Defaults to <code>600</code>.","text":""},{"location":"docs/reference/api/python/#dstack.api.BackendType","title":"<code>dstack.api.BackendType</code>","text":"<p>Attributes:</p> Name Type Description <code>AWS</code> <code>BackendType</code> <p>Amazon Web Services</p> <code>AZURE</code> <code>BackendType</code> <p>Microsoft Azure</p> <code>CUDO</code> <code>BackendType</code> <p>Cudo</p> <code>DSTACK</code> <code>BackendType</code> <p>dstack Sky</p> <code>GCP</code> <code>BackendType</code> <p>Google Cloud Platform</p> <code>DATACRUNCH</code> <code>BackendType</code> <p>DataCrunch</p> <code>KUBERNETES</code> <code>BackendType</code> <p>Kubernetes</p> <code>LAMBDA</code> <code>BackendType</code> <p>Lambda Cloud</p> <code>RUNPOD</code> <code>BackendType</code> <p>Runpod Cloud</p> <code>TENSORDOCK</code> <code>BackendType</code> <p>TensorDock Marketplace</p> <code>VASTAI</code> <code>BackendType</code> <p>Vast.ai Marketplace</p> <code>VULTR</code> <code>BackendType</code> <p>Vultr</p>"},{"location":"docs/reference/api/rest/","title":"REST API","text":"<p>The REST API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/rest/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and waiting for its completion.</p> <pre><code>import os\nfrom pathlib import Path\nimport time\nimport requests\n\nurl = os.environ[\"DSTACK_URL\"]\ntoken = os.environ[\"DSTACK_TOKEN\"]\nproject = os.environ[\"DSTACK_PROJECT\"]\nssh_public_key = Path(os.environ[\"SSH_PUBLIC_KEY_PATH\"]).read_text()\n\nprint(\"Submitting task\")\nresp = requests.post(\n    url=f\"{url}/api/project/{project}/runs/apply\",\n    headers={\"Authorization\": f\"Bearer {token}\"},\n    json={\n        \"plan\":{\n            \"run_spec\": {\n                \"configuration\": {\n                    \"type\": \"task\",\n                    \"commands\": [\n                        \"echo Start\",\n                        \"sleep 10\", # do some work here\n                        \"echo Finish\"\n                    ],\n                },\n                \"ssh_key_pub\": ssh_public_key,\n            }\n        },\n        \"force\": False,\n    },\n)\nrun_name = resp.json()[\"run_spec\"][\"run_name\"]\n\nprint(\"Waiting for task completion\")\nwhile True:\n    resp = requests.post(\n        url=f\"{url}/api/project/{project}/runs/get\",\n        headers={\"Authorization\": f\"Bearer {token}\"},\n        json={\"run_name\": run_name}\n    )\n    if resp.json()[\"status\"] in [\"terminated\", \"aborted\", \"failed\", \"done\"]:\n        print(f\"Run finished with status {resp.json()['status']}\")\n        break\n    time.sleep(2)\n</code></pre> <p></p> <p></p>"},{"location":"docs/reference/cli/dstack/apply/","title":"dstack apply","text":"<p>This command applies a given configuration. If a resource does not exist, <code>dstack apply</code> creates the resource. If a resource exists, <code>dstack apply</code> updates the resource in-place or re-creates the resource if the update is not possible.</p> <p>When applying run configurations, <code>dstack apply</code> requires that you run <code>dstack init</code> first, or specify a repo to work with via <code>-P</code> (or <code>--repo</code>), or specify <code>--no-repo</code> if you don't need any repo for the run.</p>"},{"location":"docs/reference/cli/dstack/apply/#usage","title":"Usage","text":"<pre><code>$ dstack apply --help\nUsage: dstack apply [--project NAME] [-h [TYPE]] [-f FILE] [-y] [--force] [-d]\n                    [-P REPO] [--repo-branch REPO_BRANCH]\n                    [--repo-hash REPO_HASH] [--no-repo] [-t OAUTH_TOKEN]\n                    [--git-identity SSH_PRIVATE_KEY]\n                    [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit.\n  -f, --file FILE       The path to the configuration file. Defaults to\n                        $PWD/.dstack.yml\n  -y, --yes             Do not ask for confirmation\n  --force               Force apply when no changes detected\n  -d, --detach          Exit immediately after sumbitting configuration\n\nRepo Options:\n  -P, --repo REPO       The repo to use for the run. Can be a local path or a\n                        Git repo URL.\n  --repo-branch REPO_BRANCH\n                        The repo branch to use for the run\n  --repo-hash REPO_HASH\n                        The hash of the repo commit to use for the run\n  --no-repo             Do not use any repo for the run\n  -t, --token OAUTH_TOKEN\n                        An authentication token to access a private Git repo\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access a private Git repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use Git\n</code></pre>"},{"location":"docs/reference/cli/dstack/attach/","title":"dstack attach","text":"<p>This command attaches to a given run. It establishes the SSH tunnel, forwards ports, and shows real-time run logs.</p>"},{"location":"docs/reference/cli/dstack/attach/#usage","title":"Usage","text":"<pre><code>$ dstack attach --help\nUsage: dstack attach [-h] [--project NAME] [--ssh-identity SSH_PRIVATE_KEY]\n                     [--logs] [--host HOST] [-p MAPPING] [--replica REPLICA]\n                     [--job JOB]\n                     run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --logs                Print run logs as they follow\n  --host HOST           Local address to bind. Defaults to localhost.\n  -p, --port MAPPING    Port mapping overrides\n  --replica REPLICA     The replica number. Defaults to 0.\n  --job JOB             The job number inside the replica. Defaults to 0.\n</code></pre>"},{"location":"docs/reference/cli/dstack/config/","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p>"},{"location":"docs/reference/cli/dstack/config/#usage","title":"Usage","text":"<pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN] [-y]\n                     [--remove] [-n]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  -y, --yes          Don't ask for confirmation (e.g. update the config)\n  --remove           Delete project configuration\n  -n, --no           Don't ask for confirmation (e.g. do not update the\n                     config)\n</code></pre>"},{"location":"docs/reference/cli/dstack/delete/","title":"dstack delete","text":"<p>This command deletes the resources defined by a given configuration.</p>"},{"location":"docs/reference/cli/dstack/delete/#usage","title":"Usage","text":"<pre><code>$ dstack delete --help\nUsage: dstack delete [-h] [--project NAME] [-f FILE] [-y]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -f, --file FILE       The path to the configuration file. Defaults to\n                        $PWD/.dstack.yml\n  -y, --yes             Do not ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/dstack/fleet/","title":"dstack fleet","text":"<p>Fleets enable efficient provisioning and management of clusters and instances.</p>"},{"location":"docs/reference/cli/dstack/fleet/#dstack-fleet-list","title":"dstack fleet list","text":"<p>The <code>dstack fleet list</code> command displays fleets and instances.</p> <pre><code>$ dstack fleet list --help\nUsage: dstack fleet list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/dstack/fleet/#dstack-fleet-delete","title":"dstack fleet delete","text":"<p>The <code>dstack fleet delete</code> deletes fleets and instances. Cloud instances are terminated upon deletion.</p> <pre><code>$ dstack fleet delete --help\nUsage: dstack fleet delete [-h] [-i INSTANCE_NUM] [-y] name\n\nPositional Arguments:\n  name                  The name of the fleet\n\nOptions:\n  -h, --help            show this help message and exit\n  -i, --instance INSTANCE_NUM\n                        The instances to delete\n  -y, --yes             Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/dstack/gateway/","title":"dstack gateway","text":"<p>A gateway allows publishing services at a custom domain with HTTPS.</p>"},{"location":"docs/reference/cli/dstack/gateway/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p>"},{"location":"docs/reference/cli/dstack/gateway/#usage","title":"Usage","text":"<pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/dstack/gateway/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p>"},{"location":"docs/reference/cli/dstack/gateway/#usage_1","title":"Usage","text":"<pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/dstack/init/","title":"dstack init","text":"<p>This command initializes the current directory as a <code>dstack</code> repo.</p> <p>Git credentials</p> <p>If the directory is a cloned Git repository, <code>dstack init</code> ensures that <code>dstack</code> can access it. By default, the command uses the user's default Git credentials. These can be overridden with  <code>--git-identity</code> (private SSH key) or <code>--token</code> (OAuth token).</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token to access a private Git repo\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access a private Git repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use Git\n</code></pre> <p>User SSH key</p> <p>By default, <code>dstack</code> uses its own SSH key to access instances (<code>~/.dstack/ssh/id_rsa</code>).  It is possible to override this key via the <code>--ssh-identity</code> argument.</p>"},{"location":"docs/reference/cli/dstack/logs/","title":"dstack logs","text":"<p>This command shows the output of a given run.</p>"},{"location":"docs/reference/cli/dstack/logs/#usage","title":"Usage","text":"<pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--replica REPLICA]\n                   [--job JOB]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose        Show run diagnostic logs\n  -a, --attach          Set up an SSH tunnel and print logs as they follow\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --replica REPLICA     The relica number. Defaults to 0.\n  --job JOB             The job number inside the replica. Defaults to 0.\n</code></pre>"},{"location":"docs/reference/cli/dstack/ps/","title":"dstack ps","text":"<p>This command shows the status of runs.</p>"},{"location":"docs/reference/cli/dstack/ps/#usage","title":"Usage","text":"<pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/dstack/server/","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p>"},{"location":"docs/reference/cli/dstack/server/#usage","title":"Usage","text":"<pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL] [-y] [-n]\n                     [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to INFO.\n  -y, --yes             Don't ask for confirmation (e.g. update the config)\n  -n, --no              Don't ask for confirmation (e.g. do not update the\n                        config)\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/dstack/stats/","title":"dstack stats","text":"<p>This command shows run hardware metrics such as CPU, memory, and GPU utilization.</p>"},{"location":"docs/reference/cli/dstack/stats/#usage","title":"Usage","text":"<pre><code>$ dstack stats --help\nUsage: dstack stats [-h] [--project NAME] [-w] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -w, --watch     Watch run stats in realtime\n</code></pre>"},{"location":"docs/reference/cli/dstack/stop/","title":"dstack stop","text":"<p>This command stops run(s).</p>"},{"location":"docs/reference/cli/dstack/stop/#usage","title":"Usage","text":"<pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/dstack/volume/","title":"dstack volume","text":"<p>The volumes commands.</p>"},{"location":"docs/reference/cli/dstack/volume/#dstack-volume-list","title":"dstack volume list","text":"<p>The <code>dstack volume list</code> command lists volumes.</p>"},{"location":"docs/reference/cli/dstack/volume/#usage","title":"Usage","text":"<pre><code>$ dstack volume list --help\nUsage: dstack volume list [-h] [-w] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -w, --watch    Update listing in realtime\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/dstack/volume/#dstack-volume-delete","title":"dstack volume delete","text":"<p>The <code>dstack volume delete</code> command deletes volumes.</p>"},{"location":"docs/reference/cli/dstack/volume/#usage_1","title":"Usage","text":"<pre><code>$ dstack volume delete --help\nUsage: dstack volume delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the volume\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/dstack.yml/dev-environment/","title":"<code>dev-environment</code>","text":"<p>The <code>dev-environment</code> configuration type allows running dev environments.</p>"},{"location":"docs/reference/dstack.yml/dev-environment/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ide","title":"<code>ide</code> -  The IDE to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#version","title":"<code>version</code> - (Optional) The version of the IDE.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#init","title":"<code>init</code> - (Optional) The bash commands to run on startup.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> - (Optional) The run name. If not specified, a random name is generated.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#user","title":"<code>user</code> - (Optional) The user inside the container, <code>user_name_or_id[:group_name_or_id]</code> (e.g., <code>ubuntu</code>, <code>1000:1000</code>). Defaults to the default <code>image</code> user.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#privileged","title":"<code>privileged</code> - (Optional) Run the container in privileged mode.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#single_branch","title":"<code>single_branch</code> - (Optional) Whether to clone and track only the current branch or all remote branches. Relevant only when using remote Git repos. Defaults to <code>false</code> for dev environments and to <code>true</code> for tasks and services.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-gpu","title":"<code>resources.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator, one of: <code>nvidia</code>, <code>amd</code>, <code>google</code> (alias: <code>tpu</code>), <code>intel</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#resources-disk","title":"<code>resources.disk</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#size","title":"<code>size</code> -  The disk size. Can be set to a range (e.g., <code>100GB..</code> or <code>100GB..200GB</code>).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#_volumes","title":"<code>volumes[n]</code>","text":"Network volumesInstance volumes Short syntax <p>The short syntax for volumes is a colon-separated string in the form of <code>source:destination</code></p> <ul> <li><code>volume-name:/container/path</code> for network volumes</li> <li><code>/instance/path:/container/path</code> for instance volumes</li> </ul>"},{"location":"docs/reference/dstack.yml/dev-environment/#name","title":"<code>name</code> -  The network volume name or the list of network volume names to mount. If a list is specified, one of the volumes in the list will be mounted. Specify volumes from different backends/regions to increase availability..","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#path","title":"<code>path</code> -  The absolute container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#instance_path","title":"<code>instance_path</code> -  The absolute path on the instance (host).","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#path","title":"<code>path</code> -  The absolute path in the container.","text":""},{"location":"docs/reference/dstack.yml/dev-environment/#optional","title":"<code>optional</code> - (Optional) Allow running without this volume in backends that do not support instance volumes.","text":""},{"location":"docs/reference/dstack.yml/fleet/","title":"<code>fleet</code>","text":"<p>The <code>fleet</code> configuration type allows creating and updating fleets.</p>"},{"location":"docs/reference/dstack.yml/fleet/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/fleet/#name","title":"<code>name</code> - (Optional) The fleet name.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_ssh_config","title":"<code>ssh_config</code> - (Optional) The parameters for adding instances via SSH.","text":""},{"location":"docs/reference/dstack.yml/fleet/#nodes","title":"<code>nodes</code> - (Optional) The number of instances.","text":""},{"location":"docs/reference/dstack.yml/fleet/#placement","title":"<code>placement</code> - (Optional) The placement of instances: <code>any</code> or <code>cluster</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_resources","title":"<code>resources</code> - (Optional) The resources requirements.","text":""},{"location":"docs/reference/dstack.yml/fleet/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_retry","title":"<code>retry</code> - (Optional) The policy for provisioning retry. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/fleet/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/dstack.yml/fleet/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#ssh_config","title":"<code>ssh_config</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#user","title":"<code>user</code> - (Optional) The user to log in with on all hosts.","text":""},{"location":"docs/reference/dstack.yml/fleet/#port","title":"<code>port</code> - (Optional) The SSH port to connect to.","text":""},{"location":"docs/reference/dstack.yml/fleet/#identity_file","title":"<code>identity_file</code> - (Optional) The private key to use for all hosts.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_hosts","title":"<code>hosts</code> -  The per host connection parameters: a hostname or an object that overrides default ssh parameters.","text":""},{"location":"docs/reference/dstack.yml/fleet/#network","title":"<code>network</code> - (Optional) The network address for cluster setup in the format <code>&lt;ip&gt;/&lt;netmask&gt;</code>. <code>dstack</code> will use IP addresses from this network for communication between hosts. If not specified, <code>dstack</code> will use IPs from the first found internal network..","text":""},{"location":"docs/reference/dstack.yml/fleet/#ssh_config-hosts","title":"<code>ssh_config.hosts[n]</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#hostname","title":"<code>hostname</code> -  The IP address or domain to connect to.","text":""},{"location":"docs/reference/dstack.yml/fleet/#port","title":"<code>port</code> - (Optional) The SSH port to connect to for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#user","title":"<code>user</code> - (Optional) The user to log in with for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#identity_file","title":"<code>identity_file</code> - (Optional) The private key to use for this host.","text":""},{"location":"docs/reference/dstack.yml/fleet/#internal_ip","title":"<code>internal_ip</code> - (Optional) The internal IP of the host used for communication inside the cluster. If not specified, <code>dstack</code> will use the IP address from <code>network</code> or from the first found internal network..","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/fleet/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator, one of: <code>nvidia</code>, <code>amd</code>, <code>google</code> (alias: <code>tpu</code>), <code>intel</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/fleet/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#size","title":"<code>size</code> -  The disk size. Can be set to a range (e.g., <code>100GB..</code> or <code>100GB..200GB</code>).","text":""},{"location":"docs/reference/dstack.yml/fleet/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/fleet/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/fleet/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/","title":"<code>gateway</code>","text":"<p>The <code>gateway</code> configuration type allows creating and updating gateways.</p>"},{"location":"docs/reference/dstack.yml/gateway/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/gateway/#name","title":"<code>name</code> - (Optional) The gateway name.","text":""},{"location":"docs/reference/dstack.yml/gateway/#default","title":"<code>default</code> - (Optional) Make the gateway default.","text":""},{"location":"docs/reference/dstack.yml/gateway/#backend","title":"<code>backend</code> -  The gateway backend.","text":""},{"location":"docs/reference/dstack.yml/gateway/#region","title":"<code>region</code> -  The gateway region.","text":""},{"location":"docs/reference/dstack.yml/gateway/#domain","title":"<code>domain</code> - (Optional) The gateway domain, e.g. <code>example.com</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#public_ip","title":"<code>public_ip</code> - (Optional) Allocate public IP for the gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#_certificate","title":"<code>certificate</code> - (Optional) The SSL certificate configuration. Defaults to <code>type: lets-encrypt</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#certificate","title":"<code>certificate</code>","text":"Let's encryptACM"},{"location":"docs/reference/dstack.yml/gateway/#type","title":"<code>type</code> -  Automatic certificates by Let's Encrypt. Must be <code>lets-encrypt</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#type","title":"<code>type</code> -  Certificates by AWS Certificate Manager (ACM). Must be <code>acm</code>.","text":""},{"location":"docs/reference/dstack.yml/gateway/#arn","title":"<code>arn</code> -  The ARN of the wildcard ACM certificate for the domain.","text":""},{"location":"docs/reference/dstack.yml/service/","title":"<code>service</code>","text":"<p>The <code>service</code> configuration type allows running services.</p>"},{"location":"docs/reference/dstack.yml/service/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/service/#port","title":"<code>port</code> -  The port, that application listens on or the mapping.","text":""},{"location":"docs/reference/dstack.yml/service/#gateway","title":"<code>gateway</code> - (Optional) The name of the gateway. Specify boolean <code>false</code> to run without a gateway. Omit to run with the default gateway.","text":""},{"location":"docs/reference/dstack.yml/service/#strip_prefix","title":"<code>strip_prefix</code> - (Optional) Strip the <code>/proxy/services/&lt;project name&gt;/&lt;run name&gt;/</code> path prefix when forwarding requests to the service. Only takes effect when running the service without a gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#model","title":"<code>model</code> - (Optional) Mapping of the model for the OpenAI-compatible endpoint provided by <code>dstack</code>. Can be a full model format definition or just a model name. If it's a name, the service is expected to expose an OpenAI-compatible API at the <code>/v1</code> path.","text":""},{"location":"docs/reference/dstack.yml/service/#https","title":"<code>https</code> - (Optional) Enable HTTPS if running with a gateway. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#auth","title":"<code>auth</code> - (Optional) Enable the authorization. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#replicas","title":"<code>replicas</code> - (Optional) The number of replicas. Can be a number (e.g. <code>2</code>) or a range (<code>0..4</code> or <code>1..8</code>). If it's a range, the <code>scaling</code> property is required. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_scaling","title":"<code>scaling</code> - (Optional) The auto-scaling rules. Required if <code>replicas</code> is set to a range.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> - (Optional) The run name. If not specified, a random name is generated.","text":""},{"location":"docs/reference/dstack.yml/service/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/service/#user","title":"<code>user</code> - (Optional) The user inside the container, <code>user_name_or_id[:group_name_or_id]</code> (e.g., <code>ubuntu</code>, <code>1000:1000</code>). Defaults to the default <code>image</code> user.","text":""},{"location":"docs/reference/dstack.yml/service/#privileged","title":"<code>privileged</code> - (Optional) Run the container in privileged mode.","text":""},{"location":"docs/reference/dstack.yml/service/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/service/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/service/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/service/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#single_branch","title":"<code>single_branch</code> - (Optional) Whether to clone and track only the current branch or all remote branches. Relevant only when using remote Git repos. Defaults to <code>false</code> for dev environments and to <code>true</code> for tasks and services.","text":""},{"location":"docs/reference/dstack.yml/service/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/service/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/service/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/service/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/service/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/dstack.yml/service/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/service/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#model_1","title":"<code>model</code>","text":"OpenAITGI <p>TGI provides an OpenAI-compatible API starting with version 1.4.0, so models served by TGI can be defined with <code>format: openai</code> too.</p>"},{"location":"docs/reference/dstack.yml/service/#type","title":"<code>type</code> -  The type of the model. Must be <code>chat</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The name of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#format","title":"<code>format</code> -  The serving format. Must be set to <code>openai</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#prefix","title":"<code>prefix</code> - (Optional) The <code>base_url</code> prefix (after hostname). Defaults to <code>/v1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#type","title":"<code>type</code> -  The type of the model. Must be <code>chat</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The name of the model.","text":""},{"location":"docs/reference/dstack.yml/service/#format","title":"<code>format</code> -  The serving format. Must be set to <code>tgi</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#chat_template","title":"<code>chat_template</code> - (Optional) The custom prompt template for the model. If not specified, the default prompt template from the HuggingFace Hub configuration will be used.","text":""},{"location":"docs/reference/dstack.yml/service/#eos_token","title":"<code>eos_token</code> - (Optional) The custom end of sentence token. If not specified, the default end of sentence token from the HuggingFace Hub configuration will be used.","text":"Chat template <p>By default, <code>dstack</code> loads the chat template from the model's repository. If it is not present there, manual configuration is required.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ\ncommands:\n  - text-generation-launcher --port 8000 --trust-remote-code --quantize gptq\nport: 8000\n\nresources:\n  gpu: 80GB\n\n# Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Llama-2-13B-chat-GPTQ\n  format: tgi\n  chat_template: \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\\n' + system_message + '\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '&lt;s&gt;[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' &lt;/s&gt;' }}{% endif %}{% endfor %}\"\n  eos_token: \"&lt;/s&gt;\"\n</code></pre> <p>Please note that model mapping is an experimental feature with the following limitations:</p> <ol> <li>Doesn't work if your <code>chat_template</code> uses <code>bos_token</code>. As a workaround, replace <code>bos_token</code> inside <code>chat_template</code> with the token content itself.</li> <li>Doesn't work if <code>eos_token</code> is defined in the model repository as a dictionary. As a workaround, set <code>eos_token</code> manually, as shown in the example above (see Chat template).</li> </ol> <p>If you encounter any other issues, please make sure to file a  GitHub issue .</p>"},{"location":"docs/reference/dstack.yml/service/#scaling","title":"<code>scaling</code>","text":""},{"location":"docs/reference/dstack.yml/service/#metric","title":"<code>metric</code> -  The target metric to track. Currently, the only supported value is <code>rps</code> (meaning requests per second).","text":""},{"location":"docs/reference/dstack.yml/service/#target","title":"<code>target</code> -  The target value of the metric. The number of replicas is calculated based on this number and automatically adjusts (scales up or down) as this metric changes.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_up_delay","title":"<code>scale_up_delay</code> - (Optional) The delay in seconds before scaling up. Defaults to <code>300</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#scale_down_delay","title":"<code>scale_down_delay</code> - (Optional) The delay in seconds before scaling down. Defaults to <code>600</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/service/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/service/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/service/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/service/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/service/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/service/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator, one of: <code>nvidia</code>, <code>amd</code>, <code>google</code> (alias: <code>tpu</code>), <code>intel</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/service/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/service/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/service/#size","title":"<code>size</code> -  The disk size. Can be set to a range (e.g., <code>100GB..</code> or <code>100GB..200GB</code>).","text":""},{"location":"docs/reference/dstack.yml/service/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/service/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/service/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/service/#_volumes","title":"<code>volumes[n]</code>","text":"Network volumesInstance volumes Short syntax <p>The short syntax for volumes is a colon-separated string in the form of <code>source:destination</code></p> <ul> <li><code>volume-name:/container/path</code> for network volumes</li> <li><code>/instance/path:/container/path</code> for instance volumes</li> </ul>"},{"location":"docs/reference/dstack.yml/service/#name","title":"<code>name</code> -  The network volume name or the list of network volume names to mount. If a list is specified, one of the volumes in the list will be mounted. Specify volumes from different backends/regions to increase availability..","text":""},{"location":"docs/reference/dstack.yml/service/#path","title":"<code>path</code> -  The absolute container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/service/#instance_path","title":"<code>instance_path</code> -  The absolute path on the instance (host).","text":""},{"location":"docs/reference/dstack.yml/service/#path","title":"<code>path</code> -  The absolute path in the container.","text":""},{"location":"docs/reference/dstack.yml/service/#optional","title":"<code>optional</code> - (Optional) Allow running without this volume in backends that do not support instance volumes.","text":""},{"location":"docs/reference/dstack.yml/task/","title":"<code>task</code>","text":"<p>The <code>task</code> configuration type allows running tasks.</p>"},{"location":"docs/reference/dstack.yml/task/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/task/#nodes","title":"<code>nodes</code> - (Optional) Number of nodes. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> - (Optional) The run name. If not specified, a random name is generated.","text":""},{"location":"docs/reference/dstack.yml/task/#image","title":"<code>image</code> - (Optional) The name of the Docker image to run.","text":""},{"location":"docs/reference/dstack.yml/task/#user","title":"<code>user</code> - (Optional) The user inside the container, <code>user_name_or_id[:group_name_or_id]</code> (e.g., <code>ubuntu</code>, <code>1000:1000</code>). Defaults to the default <code>image</code> user.","text":""},{"location":"docs/reference/dstack.yml/task/#privileged","title":"<code>privileged</code> - (Optional) Run the container in privileged mode.","text":""},{"location":"docs/reference/dstack.yml/task/#entrypoint","title":"<code>entrypoint</code> - (Optional) The Docker entrypoint.","text":""},{"location":"docs/reference/dstack.yml/task/#working_dir","title":"<code>working_dir</code> - (Optional) The path to the working directory inside the container. It's specified relative to the repository directory (<code>/workflow</code>) and should be inside it. Defaults to <code>\".\"</code> .","text":""},{"location":"docs/reference/dstack.yml/task/#_registry_auth","title":"<code>registry_auth</code> - (Optional) Credentials for pulling a private Docker image.","text":""},{"location":"docs/reference/dstack.yml/task/#python","title":"<code>python</code> - (Optional) The major version of Python. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#nvcc","title":"<code>nvcc</code> - (Optional) Use image with NVIDIA CUDA Compiler (NVCC) included. Mutually exclusive with <code>image</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#single_branch","title":"<code>single_branch</code> - (Optional) Whether to clone and track only the current branch or all remote branches. Relevant only when using remote Git repos. Defaults to <code>false</code> for dev environments and to <code>true</code> for tasks and services.","text":""},{"location":"docs/reference/dstack.yml/task/#_env","title":"<code>env</code> - (Optional) The mapping or the list of environment variables.","text":""},{"location":"docs/reference/dstack.yml/task/#_resources","title":"<code>resources</code> - (Optional) The resources requirements to run the configuration.","text":""},{"location":"docs/reference/dstack.yml/task/#_volumes","title":"<code>volumes</code> - (Optional) The volumes mount points.","text":""},{"location":"docs/reference/dstack.yml/task/#ports","title":"<code>ports</code> - (Optional) Port numbers/mapping to expose.","text":""},{"location":"docs/reference/dstack.yml/task/#commands","title":"<code>commands</code> - (Optional) The bash commands to run.","text":""},{"location":"docs/reference/dstack.yml/task/#backends","title":"<code>backends</code> - (Optional) The backends to consider for provisioning (e.g., <code>[aws, gcp]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#regions","title":"<code>regions</code> - (Optional) The regions to consider for provisioning (e.g., <code>[eu-west-1, us-west4, westeurope]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#instance_types","title":"<code>instance_types</code> - (Optional) The cloud-specific instance types to consider for provisioning (e.g., <code>[p3.8xlarge, n1-standard-4]</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#reservation","title":"<code>reservation</code> - (Optional) The existing reservation to use for instance provisioning. Supports AWS Capacity Reservations and Capacity Blocks.","text":""},{"location":"docs/reference/dstack.yml/task/#spot_policy","title":"<code>spot_policy</code> - (Optional) The policy for provisioning spot or on-demand instances: <code>spot</code>, <code>on-demand</code>, or <code>auto</code>. Defaults to <code>on-demand</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#_retry","title":"<code>retry</code> - (Optional) The policy for resubmitting the run. Defaults to <code>false</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#max_duration","title":"<code>max_duration</code> - (Optional) The maximum duration of a run (e.g., <code>2h</code>, <code>1d</code>, etc). After it elapses, the run is automatically stopped. Use <code>off</code> for unlimited duration. Defaults to <code>off</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#stop_duration","title":"<code>stop_duration</code> - (Optional) The maximum duration of a run gracefull stopping. After it elapses, the run is automatically forced stopped. This includes force detaching volumes used by the run. Use <code>off</code> for unlimited duration. Defaults to <code>5m</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#max_price","title":"<code>max_price</code> - (Optional) The maximum instance price per hour, in dollars.","text":""},{"location":"docs/reference/dstack.yml/task/#creation_policy","title":"<code>creation_policy</code> - (Optional) The policy for using instances from the pool. Defaults to <code>reuse-or-create</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#idle_duration","title":"<code>idle_duration</code> - (Optional) Time to wait before terminating idle instances. Defaults to <code>5m</code> for runs and <code>3d</code> for fleets. Use <code>off</code> for unlimited duration.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_policy","title":"<code>termination_policy</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#termination_idle_time","title":"<code>termination_idle_time</code> - (Optional) Deprecated in favor of <code>idle_duration</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#retry","title":"<code>retry</code>","text":""},{"location":"docs/reference/dstack.yml/task/#on_events","title":"<code>on_events</code> -  The list of events that should be handled with retry. Supported events are <code>no-capacity</code>, <code>interruption</code>, and <code>error</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#duration","title":"<code>duration</code> - (Optional) The maximum period of retrying the run, e.g., <code>4h</code> or <code>1d</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#resources","title":"<code>resources</code>","text":""},{"location":"docs/reference/dstack.yml/task/#cpu","title":"<code>cpu</code> - (Optional) The number of CPU cores. Defaults to <code>2..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>8GB</code>). Defaults to <code>8GB..</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#shm_size","title":"<code>shm_size</code> - (Optional) The size of shared memory (e.g., <code>8GB</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.","text":""},{"location":"docs/reference/dstack.yml/task/#_gpu","title":"<code>gpu</code> - (Optional) The GPU requirements. Can be set to a number, a string (e.g. <code>A100</code>, <code>80GB:2</code>, etc.), or an object.","text":""},{"location":"docs/reference/dstack.yml/task/#_disk","title":"<code>disk</code> - (Optional) The disk resources.","text":""},{"location":"docs/reference/dstack.yml/task/#resources-gpu","title":"<code>resouces.gpu</code>","text":""},{"location":"docs/reference/dstack.yml/task/#vendor","title":"<code>vendor</code> - (Optional) The vendor of the GPU/accelerator, one of: <code>nvidia</code>, <code>amd</code>, <code>google</code> (alias: <code>tpu</code>), <code>intel</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> - (Optional) The GPU name or list of names.","text":""},{"location":"docs/reference/dstack.yml/task/#count","title":"<code>count</code> - (Optional) The number of GPUs. Defaults to <code>1</code>.","text":""},{"location":"docs/reference/dstack.yml/task/#memory","title":"<code>memory</code> - (Optional) The RAM size (e.g., <code>16GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#total_memory","title":"<code>total_memory</code> - (Optional) The total RAM size (e.g., <code>32GB</code>). Can be set to a range (e.g. <code>16GB..</code>, or <code>16GB..80GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#compute_capability","title":"<code>compute_capability</code> - (Optional) The minimum compute capability of the GPU (e.g., <code>7.5</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#resources-disk","title":"<code>resouces.disk</code>","text":""},{"location":"docs/reference/dstack.yml/task/#size","title":"<code>size</code> -  The disk size. Can be set to a range (e.g., <code>100GB..</code> or <code>100GB..200GB</code>).","text":""},{"location":"docs/reference/dstack.yml/task/#registry_auth","title":"<code>registry_auth</code>","text":""},{"location":"docs/reference/dstack.yml/task/#username","title":"<code>username</code> -  The username.","text":""},{"location":"docs/reference/dstack.yml/task/#password","title":"<code>password</code> -  The password or access token.","text":""},{"location":"docs/reference/dstack.yml/task/#_volumes","title":"<code>volumes[n]</code>","text":"Network volumesInstance volumes Short syntax <p>The short syntax for volumes is a colon-separated string in the form of <code>source:destination</code></p> <ul> <li><code>volume-name:/container/path</code> for network volumes</li> <li><code>/instance/path:/container/path</code> for instance volumes</li> </ul>"},{"location":"docs/reference/dstack.yml/task/#name","title":"<code>name</code> -  The network volume name or the list of network volume names to mount. If a list is specified, one of the volumes in the list will be mounted. Specify volumes from different backends/regions to increase availability..","text":""},{"location":"docs/reference/dstack.yml/task/#path","title":"<code>path</code> -  The absolute container path to mount the volume at.","text":""},{"location":"docs/reference/dstack.yml/task/#instance_path","title":"<code>instance_path</code> -  The absolute path on the instance (host).","text":""},{"location":"docs/reference/dstack.yml/task/#path","title":"<code>path</code> -  The absolute path in the container.","text":""},{"location":"docs/reference/dstack.yml/task/#optional","title":"<code>optional</code> - (Optional) Allow running without this volume in backends that do not support instance volumes.","text":""},{"location":"docs/reference/dstack.yml/volume/","title":"<code>volume</code>","text":"<p>The <code>volume</code> configuration type allows creating, registering, and updating volumes.</p>"},{"location":"docs/reference/dstack.yml/volume/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/dstack.yml/volume/#name","title":"<code>name</code> - (Optional) The volume name.","text":""},{"location":"docs/reference/dstack.yml/volume/#backend","title":"<code>backend</code> -  The volume backend.","text":""},{"location":"docs/reference/dstack.yml/volume/#region","title":"<code>region</code> -  The volume region.","text":""},{"location":"docs/reference/dstack.yml/volume/#size","title":"<code>size</code> - (Optional) The volume size. Must be specified when creating new volumes.","text":""},{"location":"docs/reference/dstack.yml/volume/#volume_id","title":"<code>volume_id</code> - (Optional) The volume ID. Must be specified when registering external volumes.","text":""},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":"<p>The <code>~/.dstack/server/config.yml</code> file is used to configure backends and other sever-level settings.</p>"},{"location":"docs/reference/server/config.yml/#root-reference","title":"Root reference","text":""},{"location":"docs/reference/server/config.yml/#_projects","title":"<code>projects</code> -  The list of projects.","text":""},{"location":"docs/reference/server/config.yml/#_encryption","title":"<code>encryption</code> - (Optional) The encryption config.","text":""},{"location":"docs/reference/server/config.yml/#_default_permissions","title":"<code>default_permissions</code> - (Optional) The default user permissions.","text":""},{"location":"docs/reference/server/config.yml/#projects","title":"<code>projects[n]</code>","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The name of the project.","text":""},{"location":"docs/reference/server/config.yml/#backends","title":"<code>backends</code> - (Optional) The list of backends.","text":""},{"location":"docs/reference/server/config.yml/#backends","title":"<code>projects[n].backends</code>","text":""},{"location":"docs/reference/server/config.yml/#aws","title":"<code>projects[n].backends[type=aws]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>aws</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of AWS regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#vpc_name","title":"<code>vpc_name</code> - (Optional) The name of custom VPCs. All configured regions must have a VPC with this name. If your custom VPCs don't have names or have different names in different regions, use <code>vpc_ids</code> instead..","text":""},{"location":"docs/reference/server/config.yml/#vpc_ids","title":"<code>vpc_ids</code> - (Optional) The mapping from AWS regions to VPC IDs. If <code>default_vpcs: true</code>, omitted regions will use default VPCs.","text":""},{"location":"docs/reference/server/config.yml/#default_vpcs","title":"<code>default_vpcs</code> - (Optional) A flag to enable/disable using default VPCs in regions not configured by <code>vpc_ids</code>. Set to <code>false</code> if default VPCs should never be used. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#public_ips","title":"<code>public_ips</code> - (Optional) A flag to enable/disable public IP assigning on instances. <code>public_ips: false</code> requires at least one private subnet with outbound internet connectivity provided by a NAT Gateway or a Transit Gateway. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#tags","title":"<code>tags</code> - (Optional) The tags that will be assigned to resources created by <code>dstack</code>.","text":""},{"location":"docs/reference/server/config.yml/#_os_images","title":"<code>os_images</code> - (Optional) The mapping of instance categories (CPU, NVIDIA GPU) to AMI configurations.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#aws-creds","title":"<code>projects[n].backends[type=aws].creds</code>","text":"Access keyDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>access_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#access_key","title":"<code>access_key</code> -  The access key.","text":""},{"location":"docs/reference/server/config.yml/#secret_key","title":"<code>secret_key</code> -  The secret key.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#aws-os_images","title":"<code>projects[n].backends[type=aws].os_images</code>","text":""},{"location":"docs/reference/server/config.yml/#_cpu","title":"<code>cpu</code> - (Optional) The AMI used for CPU instances.","text":""},{"location":"docs/reference/server/config.yml/#_nvidia","title":"<code>nvidia</code> - (Optional) The AMI used for NVIDIA GPU instances.","text":""},{"location":"docs/reference/server/config.yml/#aws-os_images-cpu","title":"<code>projects[n].backends[type=aws].os_images.cpu</code>","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The AMI name.","text":""},{"location":"docs/reference/server/config.yml/#owner","title":"<code>owner</code> - (Optional) The AMI owner, account ID or <code>self</code>. Defaults to <code>self</code>.","text":""},{"location":"docs/reference/server/config.yml/#user","title":"<code>user</code> -  The OS user for provisioning.","text":""},{"location":"docs/reference/server/config.yml/#aws-os_images-nvidia","title":"<code>projects[n].backends[type=aws].os_images.nvidia</code>","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The AMI name.","text":""},{"location":"docs/reference/server/config.yml/#owner","title":"<code>owner</code> - (Optional) The AMI owner, account ID or <code>self</code>. Defaults to <code>self</code>.","text":""},{"location":"docs/reference/server/config.yml/#user","title":"<code>user</code> -  The OS user for provisioning.","text":""},{"location":"docs/reference/server/config.yml/#azure","title":"<code>projects[n].backends[type=azure]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the backend. Must be <code>azure</code>.","text":""},{"location":"docs/reference/server/config.yml/#tenant_id","title":"<code>tenant_id</code> -  The tenant ID.","text":""},{"location":"docs/reference/server/config.yml/#subscription_id","title":"<code>subscription_id</code> -  The subscription ID.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of Azure regions (locations). Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#vpc_ids","title":"<code>vpc_ids</code> - (Optional) The mapping from configured Azure locations to network IDs. A network ID must have a format <code>networkResourceGroup/networkName</code> If not specified, <code>dstack</code> will create a new network for every configured region.","text":""},{"location":"docs/reference/server/config.yml/#public_ips","title":"<code>public_ips</code> - (Optional) A flag to enable/disable public IP assigning on instances. <code>public_ips: false</code> requires <code>vpc_ids</code> that specifies custom networks with outbound internet connectivity provided by NAT Gateway or other mechanism. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#tags","title":"<code>tags</code> - (Optional) The tags that will be assigned to resources created by <code>dstack</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#azure-creds","title":"<code>projects[n].backends[type=azure].creds</code>","text":"ClientDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>client</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#gcp","title":"<code>projects[n].backends[type=gcp]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>gcp</code>.","text":""},{"location":"docs/reference/server/config.yml/#project_id","title":"<code>project_id</code> -  The project ID.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of GCP regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#vpc_name","title":"<code>vpc_name</code> - (Optional) The name of a custom VPC.","text":""},{"location":"docs/reference/server/config.yml/#vpc_project_id","title":"<code>vpc_project_id</code> - (Optional) The shared VPC hosted project ID. Required for shared VPC only.","text":""},{"location":"docs/reference/server/config.yml/#public_ips","title":"<code>public_ips</code> - (Optional) A flag to enable/disable public IP assigning on instances. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#nat_check","title":"<code>nat_check</code> - (Optional) A flag to enable/disable a check that Cloud NAT is configured for the VPC. This should be set to <code>false</code> when <code>public_ips: false</code> and outbound internet connectivity is provided by a mechanism other than Cloud NAT such as a third-party NAT appliance. Defaults to <code>true</code>.","text":""},{"location":"docs/reference/server/config.yml/#vm_service_account","title":"<code>vm_service_account</code> - (Optional) The service account associated with provisioned VMs.","text":""},{"location":"docs/reference/server/config.yml/#tags","title":"<code>tags</code> - (Optional) The tags (labels) that will be assigned to resources created by <code>dstack</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#gcp-creds","title":"<code>projects[n].backends[type=gcp].creds</code>","text":"Service accountDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>service_account</code>.","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"<code>filename</code> -  The path to the service account file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the service account file. When configuring via <code>server/config.yml</code>, it's automatically filled from <code>filename</code>. When configuring via UI, it has to be specified explicitly.","text":"Specifying <code>data</code> <p>To specify service account file contents as a string, use <code>jq</code>:</p> <pre><code>cat my-service-account-file.json | jq -c | jq -R\n</code></pre>"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#lambda","title":"<code>projects[n].backends[type=lambda]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>lambda</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of Lambda regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#lambda-creds","title":"<code>projects[n].backends[type=lambda].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#runpod","title":"<code>projects[n].backends[type=runpod]</code>","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of RunPod regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#runpod-creds","title":"<code>projects[n].backends[type=runpod].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#vastai","title":"<code>projects[n].backends[type=vastai]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>vastai</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of VastAI regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#vastai-creds","title":"<code>projects[n].backends[type=vastai].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#tensordock","title":"<code>projects[n].backends[type=tensordock]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>tensordock</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of TensorDock regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#tensordock-creds","title":"<code>projects[n].backends[type=tensordock].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#api_token","title":"<code>api_token</code> -  The API token.","text":""},{"location":"docs/reference/server/config.yml/#oci","title":"<code>projects[n].backends[type=oci]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>oci</code>.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of OCI regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#compartment_id","title":"<code>compartment_id</code> - (Optional) Compartment where <code>dstack</code> will create all resources. Omit to instruct <code>dstack</code> to create a new compartment.","text":""},{"location":"docs/reference/server/config.yml/#oci-creds","title":"<code>projects[n].backends[type=oci].creds</code>","text":"ClientDefault"},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>client</code>.","text":""},{"location":"docs/reference/server/config.yml/#user","title":"<code>user</code> -  User OCID.","text":""},{"location":"docs/reference/server/config.yml/#tenancy","title":"<code>tenancy</code> -  Tenancy OCID.","text":""},{"location":"docs/reference/server/config.yml/#key_file","title":"<code>key_file</code> - (Optional) Path to the user's private PEM key. Either this or <code>key_content</code> should be set.","text":""},{"location":"docs/reference/server/config.yml/#key_content","title":"<code>key_content</code> - (Optional) Content of the user's private PEM key. Either this or <code>key_file</code> should be set.","text":""},{"location":"docs/reference/server/config.yml/#pass_phrase","title":"<code>pass_phrase</code> - (Optional) Passphrase for the private PEM key if it is encrypted.","text":""},{"location":"docs/reference/server/config.yml/#fingerprint","title":"<code>fingerprint</code> -  User's public key fingerprint.","text":""},{"location":"docs/reference/server/config.yml/#region","title":"<code>region</code> -  Name or key of any region the tenancy is subscribed to.","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>default</code>.","text":""},{"location":"docs/reference/server/config.yml/#file","title":"<code>file</code> - (Optional) Path to the OCI CLI-compatible config file. Defaults to <code>~/.oci/config</code>.","text":""},{"location":"docs/reference/server/config.yml/#profile","title":"<code>profile</code> - (Optional) Profile to load from the config file. Defaults to <code>DEFAULT</code>.","text":""},{"location":"docs/reference/server/config.yml/#cudo","title":"<code>projects[n].backends[type=cudo]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>cudo</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of Cudo regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#project_id","title":"<code>project_id</code> -  The project ID.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#cudo-creds","title":"<code>projects[n].backends[type=cudo].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#datacrunch","title":"<code>projects[n].backends[type=datacrunch]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>datacrunch</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of DataCrunch regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#datacrunch-creds","title":"<code>projects[n].backends[type=datacrunch].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#client_id","title":"<code>client_id</code> -  The client ID.","text":""},{"location":"docs/reference/server/config.yml/#client_secret","title":"<code>client_secret</code> -  The client secret.","text":""},{"location":"docs/reference/server/config.yml/#kubernetes","title":"<code>projects[n].backends[type=kubernetes]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>kubernetes</code>.","text":""},{"location":"docs/reference/server/config.yml/#_kubeconfig","title":"<code>kubeconfig</code> -  The kubeconfig configuration.","text":""},{"location":"docs/reference/server/config.yml/#_networking","title":"<code>networking</code> - (Optional) The networking configuration.","text":""},{"location":"docs/reference/server/config.yml/#kubernetes-kubeconfig","title":"<code>projects[n].backends[type=kubernetes].kubeconfig</code>","text":""},{"location":"docs/reference/server/config.yml/#filename","title":"# <code>filename</code> -  The path to the kubeconfig file.","text":""},{"location":"docs/reference/server/config.yml/#data","title":"<code>data</code> - (Optional) The contents of the kubeconfig file. When configuring via <code>server/config.yml</code>, it's automatically filled from <code>filename</code>. When configuring via UI, it has to be specified explicitly.","text":"Specifying <code>data</code> <p>To specify service account file contents as a string, use <code>jq</code>:</p> <pre><code>cat my-service-account-file.json | jq -c | jq -R\n</code></pre>"},{"location":"docs/reference/server/config.yml/#kubernetes-networking","title":"<code>projects[n].backends[type=kubernetes].networking</code>","text":""},{"location":"docs/reference/server/config.yml/#ssh_host","title":"# <code>ssh_host</code> - (Optional) The external IP address of any node.","text":""},{"location":"docs/reference/server/config.yml/#ssh_port","title":"<code>ssh_port</code> - (Optional) Any port accessible outside of the cluster.","text":""},{"location":"docs/reference/server/config.yml/#vultr","title":"<code>projects[n].backends[type=vultr]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of backend. Must be <code>vultr</code>.","text":""},{"location":"docs/reference/server/config.yml/#regions","title":"<code>regions</code> - (Optional) The list of Vultr regions. Omit to use all regions.","text":""},{"location":"docs/reference/server/config.yml/#_creds","title":"<code>creds</code> -  The credentials.","text":""},{"location":"docs/reference/server/config.yml/#vultr-creds","title":"<code>projects[n].backends[type=vultr].creds</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of credentials. Must be <code>api_key</code>.","text":""},{"location":"docs/reference/server/config.yml/#api_key","title":"<code>api_key</code> -  The API key.","text":""},{"location":"docs/reference/server/config.yml/#encryption","title":"<code>encryption</code>","text":""},{"location":"docs/reference/server/config.yml/#keys","title":"<code>keys</code> -  The encryption keys.","text":""},{"location":"docs/reference/server/config.yml/#encryption-keys","title":"<code>encryption.keys</code>","text":""},{"location":"docs/reference/server/config.yml/#encryption-keys-identity","title":"<code>encryption.keys[n][type=identity]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the key. Must be <code>identity</code>.","text":""},{"location":"docs/reference/server/config.yml/#encryption-keys-aes","title":"<code>encryption.keys[n][type=aes]</code>","text":""},{"location":"docs/reference/server/config.yml/#type","title":"<code>type</code> -  The type of the key. Must be <code>aes</code>.","text":""},{"location":"docs/reference/server/config.yml/#name","title":"<code>name</code> -  The key name for key identification.","text":""},{"location":"docs/reference/server/config.yml/#secret","title":"<code>secret</code> -  Base64-encoded AES-256 key.","text":""},{"location":"docs/reference/server/config.yml/#default_permissions","title":"<code>default_permissions</code>","text":""},{"location":"docs/reference/server/config.yml/#allow_non_admins_create_projects","title":"<code>allow_non_admins_create_projects</code> - (Optional) This flag controls whether regular users (non-global admins) can create and manage their own projects. Defaults to <code>True</code>.","text":""},{"location":"docs/reference/server/config.yml/#allow_non_admins_manage_ssh_fleets","title":"<code>allow_non_admins_manage_ssh_fleets</code> - (Optional) This flag controls whether regular project members (i.e. Users) can add and delete SSH fleets. Defaults to <code>True</code>.","text":""},{"location":"examples/accelerators/amd/","title":"AMD","text":"<p><code>dstack</code> supports running dev environments, tasks, and services on AMD GPUs. You can do that by setting up an SSH fleet  with on-prem AMD GPUs or configuring a backend that offers AMD GPUs such as the <code>runpod</code> backend.</p>"},{"location":"examples/accelerators/amd/#deployment","title":"Deployment","text":"<p>Most serving frameworks including vLLM and TGI have AMD support. Here's an example of a service that deploys  Llama 3.1 70B in FP16 using TGI  and vLLM .</p> TGIvLLM <p> <pre><code>type: service\nname: amd-service-tgi\n\n# Using the official TGI's ROCm Docker image\nimage: ghcr.io/huggingface/text-generation-inference:sha-a379d55-rocm\n\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct\n  - TRUST_REMOTE_CODE=true\n  - ROCM_USE_FLASH_ATTN_V2_TRITON=true\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-70B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n</code></pre> <p> <p><pre><code>type: service\nname: llama31-service-vllm-amd\n\n# Using RunPod's ROCm Docker image\nimage: runpod/pytorch:2.4.0-py3.10-rocm6.1.0-ubuntu22.04\n# Required environment variables\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct\n  - MAX_MODEL_LEN=126192\n# Commands of the task\ncommands:\n  - export PATH=/opt/conda/envs/py_3.10/bin:$PATH\n  - wget https://github.com/ROCm/hipBLAS/archive/refs/tags/rocm-6.1.0.zip\n  - unzip rocm-6.1.0.zip\n  - cd hipBLAS-rocm-6.1.0\n  - python rmake.py\n  - cd ..\n  - git clone https://github.com/vllm-project/vllm.git\n  - cd vllm\n  - pip install triton\n  - pip uninstall torch -y\n  - pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1\n  - pip install /opt/rocm/share/amd_smi\n  - pip install --upgrade numba scipy huggingface-hub[cli]\n  - pip install \"numpy&lt;2\"\n  - pip install -r requirements-rocm.txt\n  - wget -N https://github.com/ROCm/vllm/raw/fa78403/rocm_patch/libamdhip64.so.6 -P /opt/rocm/lib\n  - rm -f \"$(python3 -c 'import torch; print(torch.__path__[0])')\"/lib/libamdhip64.so*\n  - export PYTORCH_ROCM_ARCH=\"gfx90a;gfx942\"\n  - wget https://dstack-binaries.s3.amazonaws.com/vllm-0.6.0%2Brocm614-cp310-cp310-linux_x86_64.whl\n  - pip install vllm-0.6.0+rocm614-cp310-cp310-linux_x86_64.whl\n  - vllm serve $MODEL_ID --max-model-len $MAX_MODEL_LEN --port 8000\n# Service port\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-70B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 200GB\n</code></pre> </p> <p>Note, maximum size of vLLM\u2019s <code>KV cache</code> is 126192, consequently we must set <code>MAX_MODEL_LEN</code> to 126192. Adding <code>/opt/conda/envs/py_3.10/bin</code> to PATH ensures we use the Python 3.10 environment necessary for the pre-built binaries compiled specifically for this version.</p> <p>To speed up the <code>vLLM-ROCm</code> installation, we use a pre-built binary from S3.  You can find the task to build and upload the binary in  <code>examples/deployment/vllm/amd/</code> .</p> <p>Docker image</p> <p>If you want to use AMD, specifying <code>image</code> is currently required. This must be an image that includes ROCm drivers.</p> <p>To request multiple GPUs, specify the quantity after the GPU name, separated by a colon, e.g., <code>MI300X:4</code>.</p>"},{"location":"examples/accelerators/amd/#fine-tuning","title":"Fine-tuning","text":"TRLAxolotl <p>Below is an example of LoRA fine-tuning Llama 3.1 8B using TRL   and the <code>mlabonne/guanaco-llama2-1k</code>  dataset.</p> <pre><code>type: task\nname: trl-amd-llama31-train\n\n# Using RunPod's ROCm Docker image\nimage: runpod/pytorch:2.1.2-py3.10-rocm6.1-ubuntu22.04\n\n# Required environment variables\nenv:\n  - HF_TOKEN\n# Commands of the task\ncommands:\n  - export PATH=/opt/conda/envs/py_3.10/bin:$PATH\n  - git clone https://github.com/ROCm/bitsandbytes\n  - cd bitsandbytes\n  - git checkout rocm_enabled\n  - pip install -r requirements-dev.txt\n  - cmake -DBNB_ROCM_ARCH=\"gfx942\" -DCOMPUTE_BACKEND=hip -S  .\n  - make\n  - pip install .\n  - pip install trl\n  - pip install peft\n  - pip install transformers datasets huggingface-hub scipy\n  - cd ..\n  - python examples/fine-tuning/trl/amd/train.py\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n</code></pre> <p>Below is an example of fine-tuning Llama 3.1 8B using Axolotl   and the tatsu-lab/alpaca  dataset.</p> <p><pre><code>type: task\nname: axolotl-amd-llama31-train\n\n# Using RunPod's ROCm Docker image\nimage: runpod/pytorch:2.1.2-py3.10-rocm6.0.2-ubuntu22.04\n# Required environment variables\nenv:\n  - HF_TOKEN\n# Commands of the task\ncommands:\n  - export PATH=/opt/conda/envs/py_3.10/bin:$PATH\n  - pip uninstall torch torchvision torchaudio -y\n  - python3 -m pip install --pre torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0/\n  - git clone https://github.com/OpenAccess-AI-Collective/axolotl\n  - cd axolotl\n  - git checkout d4f6c65\n  - pip install -e .\n  - cd ..\n  - wget https://dstack-binaries.s3.amazonaws.com/flash_attn-2.0.4-cp310-cp310-linux_x86_64.whl\n  - pip install flash_attn-2.0.4-cp310-cp310-linux_x86_64.whl\n  - wget https://dstack-binaries.s3.amazonaws.com/xformers-0.0.26-cp310-cp310-linux_x86_64.whl\n  - pip install xformers-0.0.26-cp310-cp310-linux_x86_64.whl\n  - git clone --recurse https://github.com/ROCm/bitsandbytes\n  - cd bitsandbytes\n  - git checkout rocm_enabled\n  - pip install -r requirements-dev.txt\n  - cmake -DBNB_ROCM_ARCH=\"gfx942\" -DCOMPUTE_BACKEND=hip -S  .\n  - make\n  - pip install .\n  - cd ..\n  - accelerate launch -m axolotl.cli.train axolotl/examples/llama-3/fft-8b.yaml\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: MI300X\n  disk: 150GB\n</code></pre> </p> <p>Note, to support ROCm, we need to checkout to commit <code>d4f6c65</code>. You can find the installation instruction in rocm-blogs .</p> <p>To speed up installation of <code>flash-attention</code> and <code>xformers</code>, we use pre-built binaries uploaded to S3.  You can find the tasks that build and upload the binaries in <code>examples/fine-tuning/axolotl/amd/</code> .</p>"},{"location":"examples/accelerators/amd/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f examples/deployment/vllm/amd/.dstack.yml\n</code></pre>"},{"location":"examples/accelerators/amd/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/tgi/amd</code> , <code>examples/deployment/vllm/amd</code> , <code>examples/fine-tuning/axolotl/amd</code>  and <code>examples/fine-tuning/trl/amd</code> </p>"},{"location":"examples/accelerators/amd/#whats-next","title":"What's next?","text":"<ol> <li>Browse TGI ,    vLLM ,    Axolotl ,    TRL  and    ROCm Bitsandbytes </li> <li>Check dev environments, tasks, and    services.</li> </ol>"},{"location":"examples/accelerators/tpu/","title":"TPU","text":"<p>If you've configured the <code>gcp</code> backend in <code>dstack</code>, you can run dev environments, tasks, and services on TPUs. Choose a TPU instance by specifying the TPU version and the number of cores (e.g. <code>v5litepod-8</code>) in the <code>gpu</code> property under <code>resources</code>, or request TPUs by specifying <code>tpu</code> as <code>vendor</code> (see examples).</p> <p>Below are a few examples on using TPUs for deployment and fine-tuning.</p> <p>Multi-host TPUs</p> <p>Currently, <code>dstack</code> supports only single-host TPUs, which means that  the maximum supported number of cores is <code>8</code> (e.g. <code>v2-8</code>, <code>v3-8</code>, <code>v5litepod-8</code>, <code>v5p-8</code>, <code>v6e-8</code>).  Multi-host TPU support is on the roadmap.</p> <p>TPU storage</p> <p>By default, each TPU VM contains a 100GB boot disk and its size cannot be changed. If you need more storage, attach additional disks using Volumes.</p>"},{"location":"examples/accelerators/tpu/#deployment","title":"Deployment","text":"<p>Many serving frameworks including vLLM and TGI have TPU support. Here's an example of a service that deploys Llama 3.1 8B using  Optimum TPU  and vLLM .</p> Optimum TPUvLLM <p> <p><pre><code>type: service\nname: llama31-service-optimum-tpu\n\nimage: dstackai/optimum-tpu:llama31\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_TOTAL_TOKENS=4096\n  - MAX_BATCH_PREFILL_TOKENS=4095\ncommands:\n  - text-generation-launcher --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\nresources:\n  gpu: v5litepod-4\n</code></pre> </p> <p>Note that for Optimum TPU <code>MAX_INPUT_TOKEN</code> is set to 4095 by default. We must also set <code>MAX_BATCH_PREFILL_TOKENS</code> to 4095.</p> Docker image <p>The official Docker image <code>huggingface/optimum-tpu:latest</code> doesn\u2019t support Llama 3.1-8B.  We\u2019ve created a custom image with the fix: <code>dstackai/optimum-tpu:llama31</code>.  Once the pull request  is merged,  the official Docker image can be used.</p> <p> <p><pre><code>type: service\nname: llama31-service-vllm-tpu\n\nenv:\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - HF_TOKEN\n  - DATE=20240828\n  - TORCH_VERSION=2.5.0\n  - VLLM_TARGET_DEVICE=tpu\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-${TORCH_VERSION}.dev${DATE}-cp311-cp311-linux_x86_64.whl\n  - pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n  - git clone https://github.com/vllm-project/vllm.git\n  - cd vllm\n  - pip install -r requirements-tpu.txt\n  - apt-get install -y libopenblas-base libopenmpi-dev libomp-dev\n  - python setup.py develop\n  - vllm serve $MODEL_ID \n      --tensor-parallel-size 4 \n      --max-model-len $MAX_MODEL_LEN\n      --port 8000\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu: v5litepod-4\n</code></pre> </p> <p>Note, when using Llama 3.1 8B with a <code>v5litepod</code> which has 16GB memory per core, we must limit the context size to 4096 tokens to fit the memory.</p>"},{"location":"examples/accelerators/tpu/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for serving LLMs with the minimal required TPU configuration:</p> Model size bfloat16 TPU int8 TPU 8B 16GB v5litepod-4 8GB v5litepod-4 70B 140GB v5litepod-16 70GB v5litepod-16 405B 810GB v5litepod-64 405GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for serving transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"examples/accelerators/tpu/#supported-frameworks","title":"Supported frameworks","text":"Framework Quantization Note TGI bfloat16 To deploy with TGI, Optimum TPU must be used. vLLM int8, bfloat16 int8 quantization still requires the same memory because the weights are first moved to the TPU in bfloat16, and then converted to int8. See the pull request  for more details."},{"location":"examples/accelerators/tpu/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p>"},{"location":"examples/accelerators/tpu/#fine-tuning-with-optimum-tpu","title":"Fine-tuning with Optimum TPU","text":"<p>Below is an example of fine-tuning Llama 3.1 8B using Optimum TPU   and the <code>Abirate/english_quotes</code>  dataset.</p> <pre><code>type: task\nname: optimum-tpu-llama-train\n\npython: \"3.11\"\nenv:\n  - HF_TOKEN\ncommands:\n  - git clone -b add_llama_31_support https://github.com/dstackai/optimum-tpu.git\n  - mkdir -p optimum-tpu/examples/custom/\n  - cp examples/fine-tuning/optimum-tpu/llama31/train.py optimum-tpu/examples/custom/train.py\n  - cp examples/fine-tuning/optimum-tpu/llama31/config.yaml optimum-tpu/examples/custom/config.yaml\n  - cd optimum-tpu\n  - pip install -e . -f https://storage.googleapis.com/libtpu-releases/index.html\n  - pip install datasets evaluate\n  - pip install accelerate -U\n  - pip install peft\n  - python examples/custom/train.py examples/custom/config.yaml\n\nresources:\n  gpu: v5litepod-8\n</code></pre>"},{"location":"examples/accelerators/tpu/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning LLMs with the minimal required TPU configuration:</p> Model size LoRA TPU 8B 16GB v5litepod-8 70B 160GB v5litepod-16 405B 950GB v5litepod-64 <p>Note, <code>v5litepod</code> is optimized for fine-tuning transformer-based models. Each core is equipped with 16GB of memory.</p>"},{"location":"examples/accelerators/tpu/#supported-frameworks_1","title":"Supported frameworks","text":"Framework Quantization Note TRL bfloat16 To fine-tune using TRL, Optimum TPU is recommended. TRL doesn't support Llama 3.1 out of the box. Pytorch XLA bfloat16"},{"location":"examples/accelerators/tpu/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/tgi/tpu</code> , <code>examples/deployment/vllm/tpu</code> , and <code>examples/fine-tuning/optimum-tpu</code> .</p>"},{"location":"examples/accelerators/tpu/#whats-next","title":"What's next?","text":"<ol> <li>Browse Optimum TPU ,    Optimum TPU TGI  and    vLLM .</li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol>"},{"location":"examples/deployment/nim/","title":"NVIDIA NIM","text":"<p>This example shows how to deploy LLama 3.1 using NVIDIA NIM  and <code>dstack</code>.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/deployment/nim/#deployment","title":"Deployment","text":"<p>Here's an example of a service that deploys Llama 3.1 8B using vLLM.</p> <pre><code>type: service\nname: llama31\n\nimage: nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\nenv:\n  - NGC_API_KEY\n  - NIM_MAX_MODEL_LEN=4096\nregistry_auth:\n  username: $oauthtoken\n  password: ${{ env.NGC_API_KEY }}\nport: 8000\n# Register the model\nmodel: meta/llama-3.1-8b-instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Cache downloaded models\nvolumes:\n  - /root/.cache/nim:/opt/nim/.cache\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre>"},{"location":"examples/deployment/nim/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command. </p> <pre><code>$ NGC_API_KEY=...\n$ dstack apply -f examples/deployment/nim/.dstack.yml\n\n #  BACKEND  REGION             RESOURCES                 SPOT  PRICE       \n 1  gcp      asia-northeast3    4xCPU, 16GB, 1xL4 (24GB)  yes   $0.17   \n 2  gcp      asia-east1         4xCPU, 16GB, 1xL4 (24GB)  yes   $0.21   \n 3  gcp      asia-northeast3    8xCPU, 32GB, 1xL4 (24GB)  yes   $0.21 \n\nSubmit the run llama3-nim-task? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>If no gateway is created, the model will be available via the OpenAI-compatible endpoint  at <code>&lt;dstack server URL&gt;/proxy/models/&lt;project name&gt;/</code>.</p> <pre><code>$ curl http://127.0.0.1:3000/proxy/models/main/chat/completions \\\n    -X POST \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"model\": \"meta/llama3-8b-instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is Deep Learning?\"\n        }\n      ],\n      \"max_tokens\": 128\n    }'\n</code></pre> <p>When a gateway is configured, the OpenAI-compatible endpoint  is available at <code>https://gateway.&lt;gateway domain&gt;/</code>.</p>"},{"location":"examples/deployment/nim/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/nim</code> .</p> Limitations <p>NIM doesn't work yet with the <code>runpod</code> backend.  Track the issue  for progress.</p>"},{"location":"examples/deployment/nim/#whats-next","title":"What's next?","text":"<ol> <li>Check services</li> <li>Browse the Llama 3.1, TGI,     and vLLM examples</li> <li>See also AMD and    TPU</li> </ol>"},{"location":"examples/deployment/tgi/","title":"HuggingFace TGI","text":"<p>This example shows how to deploy Llama 3.1 8B with <code>dstack</code> using HuggingFace TGI .</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/deployment/tgi/#deployment","title":"Deployment","text":"<p>Here's an example of a service that deploys Llama 3.1 8B using TGI.</p> <pre><code>type: service\nname: llama31\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - NUM_SHARD=$DSTACK_GPUS_NUM text-generation-launcher\nport: 80\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Uncomment to cache downloaded models  \n#volumes:\n#  - /data:/data\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre>"},{"location":"examples/deployment/tgi/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command. </p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f examples/deployment/tgi/.dstack.yml\n\n #  BACKEND     REGION        RESOURCES                      SPOT  PRICE    \n 1  tensordock  unitedstates  2xCPU, 10GB, 1xRTX3090 (24GB)  no    $0.231   \n 2  tensordock  unitedstates  2xCPU, 10GB, 1xRTX3090 (24GB)  no    $0.242   \n 3  tensordock  india         2xCPU, 38GB, 1xA5000 (24GB)    no    $0.283  \n\nSubmit a new run? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>If no gateway is created, the model will be available via the OpenAI-compatible endpoint  at <code>&lt;dstack server URL&gt;/proxy/models/&lt;project name&gt;/</code>.</p> <pre><code>$ curl http://127.0.0.1:3000/proxy/models/main/chat/completions \\\n    -X POST \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is Deep Learning?\"\n        }\n      ],\n      \"max_tokens\": 128\n    }'\n</code></pre> <p>When a gateway is configured, the OpenAI-compatible endpoint  is available at <code>https://gateway.&lt;gateway domain&gt;/</code>.</p>"},{"location":"examples/deployment/tgi/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/tgi</code> .</p>"},{"location":"examples/deployment/tgi/#whats-next","title":"What's next?","text":"<ol> <li>Check services</li> <li>Browse the Llama 3.1, vLLM,    and NIM examples</li> <li>See also AMD and    TPU</li> </ol>"},{"location":"examples/deployment/vllm/","title":"vLLM","text":"<p>This example shows how to deploy Llama 3.1 8B with <code>dstack</code> using vLLM .</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/deployment/vllm/#deployment","title":"Deployment","text":"<p>Here's an example of a service that deploys Llama 3.1 8B using vLLM.</p> <pre><code>type: service\nname: llama31\n\npython: \"3.11\"\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL_ID\n    --max-model-len $MAX_MODEL_LEN\n    --tensor-parallel-size $DSTACK_GPUS_NUM\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Uncomment to cache downloaded models\n#volumes:\n#  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre>"},{"location":"examples/deployment/vllm/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command. </p> <pre><code>$ dstack apply -f examples/deployment/vllm/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE     \n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB    yes   $0.12\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB    yes   $0.12\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:2  yes   $0.23\n\nSubmit a new run? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>If no gateway is created, the model will be available via the OpenAI-compatible endpoint  at <code>&lt;dstack server URL&gt;/proxy/models/&lt;project name&gt;/</code>.</p> <pre><code>$ curl http://127.0.0.1:3000/proxy/models/main/chat/completions \\\n    -X POST \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is Deep Learning?\"\n        }\n      ],\n      \"max_tokens\": 128\n    }'\n</code></pre> <p>When a gateway is configured, the OpenAI-compatible endpoint  is available at <code>https://gateway.&lt;gateway domain&gt;/</code>.</p>"},{"location":"examples/deployment/vllm/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/deployment/vllm</code> .</p>"},{"location":"examples/deployment/vllm/#whats-next","title":"What's next?","text":"<ol> <li>Check services</li> <li>Browse the Llama 3.1, TGI    and NIM examples</li> <li>See also AMD and    TPU</li> </ol>"},{"location":"examples/fine-tuning/axolotl/","title":"Axolotl","text":"<p>This example shows how use Axolotl   with <code>dstack</code> to fine-tune Llama3 8B using FSDP and QLoRA.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/fine-tuning/axolotl/#training-configuration-recipe","title":"Training configuration recipe","text":"<p>Axolotl reads the model, LoRA, and dataset arguments, as well as trainer configuration from a YAML file. This file can be found at <code>examples/fine-tuning/axolotl/config.yaml</code> . You can modify it as needed.</p> <p>Before you proceed with training, make sure to update the <code>hub_model_id</code> in <code>examples/fine-tuning/axolotl/config.yaml</code>  with your HuggingFace username.</p>"},{"location":"examples/fine-tuning/axolotl/#single-node-training","title":"Single-node training","text":"<p>The easiest way to run a training script with <code>dstack</code> is by creating a task configuration file. This file can be found at <code>examples/fine-tuning/axolotl/train.dstack.yml</code> .</p> <pre><code>type: task\nname: axolotl-train\n\n# Using the official Axolotl's Docker image\nimage: winglian/axolotl-cloud:main-20240429-py3.11-cu121-2.2.1\n\n# Required environment variables\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\n# Commands of the task\ncommands:\n  - accelerate launch -m axolotl.cli.train examples/fine-tuning/axolotl/config.yaml\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # Two or more GPU\n    count: 2..\n</code></pre> <p>The task uses Axolotl's Docker image, where Axolotl is already pre-installed.</p> <p>AMD</p> <p>The example above uses NVIDIA accelerators. To use it with AMD, check out AMD.</p>"},{"location":"examples/fine-tuning/axolotl/#running-a-configuration","title":"Running a configuration","text":"<p>Once the configuration is ready, run <code>dstack apply -f &lt;configuration file&gt;</code>, and <code>dstack</code> will automatically provision the cloud resources and run the configuration.</p> <pre><code>$ HF_TOKEN=...\n$ WANDB_API_KEY=...\n$ dstack apply -f examples/fine-tuning/axolotl/.dstack.yml\n</code></pre>"},{"location":"examples/fine-tuning/axolotl/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in <code>examples/fine-tuning/axolotl</code> .</p>"},{"location":"examples/fine-tuning/axolotl/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and fleets.</li> <li>See AMD. </li> <li>Browse Axolotl .</li> </ol>"},{"location":"examples/fine-tuning/trl/","title":"TRL","text":"<p>This example walks you through how to fine-tune Llama 3.1 with <code>dstack</code>, whether in the cloud or on-prem.</p>"},{"location":"examples/fine-tuning/trl/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning Llama 3.1.</p> Model size Full fine-tuning LoRA QLoRA 8B 60GB 16GB 6GB 70B 500GB 160GB 48GB 405B 3.25TB 950GB 250GB <p>The requirements can be significantly reduced with certain optimizations.</p>"},{"location":"examples/fine-tuning/trl/#running-on-multiple-gpus","title":"Running on multiple GPUs","text":"<p>Below is an example for fine-tuning Llama 3.1 8B using the <code>OpenAssistant/oasst_top1_2023-08-25</code>  dataset:</p> <pre><code>type: task\nname: trl-train\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/multi_gpu.yaml\n    --num_processes $DSTACK_GPUS_PER_NODE \n    examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n\nresources:\ngpu:\n  # 24GB or more vRAM\n  memory: 24GB..\n  # One or more GPU\n  count: 1..\n# Shared memory (for multi-gpu)\nshm_size: 24GB\n</code></pre> <p>Change the <code>resources</code> property to specify more GPUs.</p> <p>AMD</p> <p>The example above uses NVIDIA accelerators. To use it with AMD, check out AMD.</p>"},{"location":"examples/fine-tuning/trl/#deepspeed","title":"DeepSpeed","text":"<p>For more memory-efficient use of multiple GPUs, consider using DeepSpeed and ZeRO Stage 3.</p> <p>To do this, use the <code>examples/accelerate_configs/deepspeed_zero3.yaml</code> configuration file instead of  <code>examples/accelerate_configs/multi_gpu.yaml</code>.</p>"},{"location":"examples/fine-tuning/trl/#running-on-multiple-nodes","title":"Running on multiple nodes","text":"<p>In case the model doesn't feet into a single GPU, consider running a <code>dstack</code> task on multiple nodes.</p> <pre><code>type: task\nname: trl-train-distrib\n\n# Size of the cluster\nnodes: 2\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/fsdp_qlora.yaml \n    --main_process_ip=$DSTACK_MASTER_NODE_IP\n    --main_process_port=8008\n    --machine_rank=$DSTACK_NODE_RANK\n    --num_processes=$DSTACK_GPUS_NUM\n    --num_machines=$DSTACK_NODES_NUM\n      examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n    --torch_dtype bfloat16\n    --use_bnb_nested_quant\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n  # Shared memory (for multi-gpu)\n  shm_size: 24GB\n</code></pre>"},{"location":"examples/fine-tuning/trl/#fleets","title":"Fleets","text":"<p>By default, <code>dstack run</code> reuses <code>idle</code> instances from one of the existing fleets. If no <code>idle</code> instances meet the requirements, it creates a new fleet using one of the configured backends.</p> <p>Use fleets configurations to create fleets manually. This reduces startup time for dev environments, tasks, and services, and is very convenient if you want to reuse fleets across runs.</p>"},{"location":"examples/fine-tuning/trl/#dev-environments","title":"Dev environments","text":"<p>Before running a task or service, it's recommended that you first start with a dev environment. Dev environments allow you to run commands interactively.</p>"},{"location":"examples/fine-tuning/trl/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/llms/llama31</code>  and <code>examples/fine-tuning/trl</code> .</p>"},{"location":"examples/fine-tuning/trl/#whats-next","title":"What's next?","text":"<ol> <li>Browse the Axolotl     and Alignment Handbook examples</li> <li>See AMD. </li> <li>Check dev environments, tasks,     services, and fleets.</li> </ol>"},{"location":"examples/llms/llama31/","title":"Llama 3.1","text":"<p>This example walks you through how to deploy and fine-tuning Llama 3.1 with <code>dstack</code>.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/llms/llama31/#deployment","title":"Deployment","text":"<p>You can use any serving frameworks. Here's an example of a service that deploys Llama 3.1 8B using vLLM, TGI, and NIM.</p> vLLMTGINIM <p> <pre><code>type: service\nname: llama31\n\npython: \"3.11\"\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_MODEL_LEN=4096\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL_ID\n    --max-model-len $MAX_MODEL_LEN\n    --tensor-parallel-size $DSTACK_GPUS_NUM\nport: 8000\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Uncomment to cache downloaded models\n#volumes:\n#  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre> <p> <pre><code>type: service\nname: llama31\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct\n  - MAX_INPUT_LENGTH=4000\n  - MAX_TOTAL_TOKENS=4096\ncommands:\n  - NUM_SHARD=$DSTACK_GPUS_NUM text-generation-launcher\nport: 80\n# Register the model\nmodel: meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Uncomment to cache downloaded models  \n#volumes:\n#  - /data:/data\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre> <p> <pre><code>type: service\nname: llama31\n\nimage: nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\nenv:\n  - NGC_API_KEY\n  - NIM_MAX_MODEL_LEN=4096\nregistry_auth:\n  username: $oauthtoken\n  password: ${{ env.NGC_API_KEY }}\nport: 8000\n# Register the model\nmodel: meta/llama-3.1-8b-instruct\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\n# Cache downloaded models\nvolumes:\n  - /root/.cache/nim:/opt/nim/.cache\n\nresources:\n  gpu: 24GB\n  # Uncomment if using multiple GPUs\n  #shm_size: 24GB\n</code></pre> <p>Note, when using Llama 3.1 8B with a 24GB GPU, we must limit the context size to 4096 tokens to fit the memory.</p>"},{"location":"examples/llms/llama31/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for loading the model.  This excludes memory for the model context and CUDA kernel reservations.</p> Model size FP16 FP8 INT4 8B 16GB 8GB 4GB 70B 140GB 70GB 35GB 405B 810GB 405GB 203GB <p>For example, the FP16 version of Llama 3.1 405B won't fit into a single machine with eight 80GB GPUs, so we'd need at least two nodes.</p>"},{"location":"examples/llms/llama31/#quantization","title":"Quantization","text":"<p>The INT4 version of Llama 3.1 70B, can fit into two 40GB GPUs.</p> <p>The INT4 version of Llama 3.1 405B can fit into eight 40GB GPUs.</p> <p>Useful links:</p> <ul> <li>Meta's official FP8 quantized version of Llama 3.1 405B (with minimal accuracy degradation)</li> <li>Llama 3.1 Quantized Models with quantized checkpoints</li> </ul>"},{"location":"examples/llms/llama31/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f examples/llms/llama31/vllm/.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB    yes   $0.12\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB    yes   $0.12\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:2  yes   $0.23\n\nSubmit the run llama31? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>Once the service is up, the model will be available via the OpenAI-compatible endpoint at `/proxy/models//. <pre><code>$ curl http://127.0.0.1:3000/proxy/models/main/chat/completions \\\n    -X POST \\\n    -H 'Authorization: Bearer &amp;lt;dstack token&amp;gt;' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"model\": \"llama3.1\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is Deep Learning?\"\n        }\n      ],\n      \"max_tokens\": 128\n    }'\n</code></pre> <p>When a gateway is configured, the OpenAI-compatible endpoint  is available at <code>https://gateway.&lt;gateway domain&gt;/</code>.</p>"},{"location":"examples/llms/llama31/#fine-tuning","title":"Fine-tuning","text":""},{"location":"examples/llms/llama31/#running-on-multiple-gpus","title":"Running on multiple GPUs","text":"<p>Below is the task configuration file of fine-tuning Llama 3.1 8B using TRL on the <code>OpenAssistant/oasst_top1_2023-08-25</code>  dataset.</p> <pre><code>type: task\nname: trl-train\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/multi_gpu.yaml\n    --num_processes $DSTACK_GPUS_PER_NODE \n    examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n\nresources:\ngpu:\n  # 24GB or more vRAM\n  memory: 24GB..\n  # One or more GPU\n  count: 1..\n# Shared memory (for multi-gpu)\nshm_size: 24GB\n</code></pre> <p>Change the <code>resources</code> property to specify more GPUs. </p>"},{"location":"examples/llms/llama31/#memory-requirements_1","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for fine-tuning Llama 3.1.</p> Model size Full fine-tuning LoRA QLoRA 8B 60GB 16GB 6GB 70B 500GB 160GB 48GB 405B 3.25TB 950GB 250GB <p>The requirements can be significantly reduced with certain optimizations.</p>"},{"location":"examples/llms/llama31/#deepspeed","title":"DeepSpeed","text":"<p>For more memory-efficient use of multiple GPUs, consider using DeepSpeed and ZeRO Stage 3.</p> <p>To do this, use the <code>examples/accelerate_configs/deepspeed_zero3.yaml</code> configuration file instead of  <code>examples/accelerate_configs/multi_gpu.yaml</code>.</p>"},{"location":"examples/llms/llama31/#running-on-multiple-nodes","title":"Running on multiple nodes","text":"<p>In case the model doesn't feet into a single GPU, consider running a <code>dstack</code> task on multiple nodes. Below is the corresponding task configuration file.</p> <pre><code>type: task\nname: trl-train-distrib\n\n# Size of the cluster\nnodes: 2\n\npython: \"3.10\"\n# Ensure nvcc is installed (req. for Flash Attention) \nnvcc: true\n\nenv:\n  - HF_TOKEN\n  - WANDB_API_KEY\ncommands:\n  - pip install \"transformers&gt;=4.43.2\"\n  - pip install bitsandbytes\n  - pip install flash-attn --no-build-isolation\n  - pip install peft\n  - pip install wandb\n  - git clone https://github.com/huggingface/trl\n  - cd trl\n  - pip install .\n  - accelerate launch\n    --config_file=examples/accelerate_configs/fsdp_qlora.yaml \n    --main_process_ip=$DSTACK_MASTER_NODE_IP\n    --main_process_port=8008\n    --machine_rank=$DSTACK_NODE_RANK\n    --num_processes=$DSTACK_GPUS_NUM\n    --num_machines=$DSTACK_NODES_NUM\n      examples/scripts/sft.py\n    --model_name meta-llama/Meta-Llama-3.1-8B\n    --dataset_name OpenAssistant/oasst_top1_2023-08-25\n    --dataset_text_field=\"text\"\n    --per_device_train_batch_size 1\n    --per_device_eval_batch_size 1\n    --gradient_accumulation_steps 4\n    --learning_rate 2e-4\n    --report_to wandb\n    --bf16\n    --max_seq_length 1024\n    --lora_r 16 --lora_alpha 32\n    --lora_target_modules q_proj k_proj v_proj o_proj\n    --load_in_4bit\n    --use_peft\n    --attn_implementation \"flash_attention_2\"\n    --logging_steps=10\n    --output_dir models/llama31\n    --hub_model_id peterschmidt85/FineLlama-3.1-8B\n    --torch_dtype bfloat16\n    --use_bnb_nested_quant\n\nresources:\n  gpu:\n    # 24GB or more vRAM\n    memory: 24GB..\n    # One or more GPU\n    count: 1..\n  # Shared memory (for multi-gpu)\n  shm_size: 24GB\n</code></pre>"},{"location":"examples/llms/llama31/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/llms/llama31</code>  and <code>examples/fine-tuning/trl</code> .</p>"},{"location":"examples/llms/llama31/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and protips.</li> <li>Browse Llama 3.1 on HuggingFace ,     HuggingFace's Llama recipes ,     Meta's Llama recipes      and Llama Agentic System .</li> </ol>"},{"location":"examples/llms/llama32/","title":"Llama 3.2","text":"<p>This example walks you through how to deploy Llama 3.2 vision model with <code>dstack</code> using <code>vLLM</code>.</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/llms/llama32/#deployment","title":"Deployment","text":"<p>Here's an example of a service that deploys Llama 3.2 11B using vLLM.</p> <pre><code>type: service\nname: llama32\n\nimage: vllm/vllm-openai:latest\nenv:\n  - HF_TOKEN\n  - MODEL_ID=meta-llama/Llama-3.2-11B-Vision-Instruct\n  - MAX_MODEL_LEN=4096\n  - MAX_NUM_SEQS=8\ncommands:\n  - vllm serve $MODEL_ID\n    --max-model-len $MAX_MODEL_LEN\n    --max-num-seqs $MAX_NUM_SEQS\n    --enforce-eager\n    --disable-log-requests\n    --limit-mm-per-prompt \"image=1\"\n    --tensor-parallel-size $DSTACK_GPUS_NUM\nport: 8000\n# Register the model\nmodel: meta-llama/Llama-3.2-11B-Vision-Instruct\n\n# Uncomment to cache downloaded models\n#volumes:\n#  - /root/.cache/huggingface/hub:/root/.cache/huggingface/hub\n\nresources:\n  gpu: 40GB..48GB\n</code></pre>"},{"location":"examples/llms/llama32/#memory-requirements","title":"Memory requirements","text":"<p>Below are the approximate memory requirements for loading the model.  This excludes memory for the model context and CUDA kernel reservations.</p> Model size FP16 11B 40GB 90B 180GB"},{"location":"examples/llms/llama32/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f examples/llms/llama32/vllm/.dstack.yml\n\n #  BACKEND  REGION     RESOURCES                    SPOT  PRICE   \n 1  runpod   CA-MTL-1   9xCPU, 50GB, 1xA40 (48GB)    yes   $0.24   \n 2  runpod   EU-SE-1    9xCPU, 50GB, 1xA40 (48GB)    yes   $0.24   \n 3  runpod   EU-SE-1    9xCPU, 50GB, 1xA6000 (48GB)  yes   $0.25   \n\n\nSubmit the run llama32? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre> <p>Once the service is up, it will be available via the service endpoint at <code>&lt;dstack server URL&gt;/proxy/services/&lt;project name&gt;/&lt;run name&gt;/</code>.</p> <pre><code>$ curl http://127.0.0.1:3000/proxy/services/main/llama32/v1/chat/completions \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n        \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\" : \"text\", \"text\": \"Describe the image.\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/e/ea/Bento_at_Hanabishi%2C_Koyasan.jpg\"}}\n            ]\n        }],\n        \"max_tokens\": 2048\n    }'\n</code></pre> <p>When a gateway is configured, the service endpoint  is available at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;/</code>.</p>"},{"location":"examples/llms/llama32/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/llms/llama32</code> .</p>"},{"location":"examples/llms/llama32/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and protips.</li> <li>Browse Llama 3.2 on HuggingFace     and LLama 3.2 on vLLM .</li> </ol>"},{"location":"examples/misc/docker-compose/","title":"Docker Compose","text":"<p>All backends except <code>runpod</code>, <code>vastai</code>, and <code>kubernetes</code> allow using Docker and Docker Compose inside <code>dstack</code> runs.</p> <p>This example shows how to deploy Hugging Face Chat UI  with TGI  serving Llama-3.2-3B-Instruct  using Docker Compose .</p> Prerequisites <p>Once <code>dstack</code> is installed, go ahead clone the repo, and run <code>dstack init</code>.</p> <pre><code>$ git clone https://github.com/dstackai/dstack\n$ cd dstack\n$ dstack init\n</code></pre>"},{"location":"examples/misc/docker-compose/#deployment","title":"Deployment","text":""},{"location":"examples/misc/docker-compose/#running-as-a-task","title":"Running as a task","text":"<code>task.dstack.yml</code><code>compose.yaml</code> <p> <pre><code>type: task\nname: chat-ui-task\n\nprivileged: true\nimage: dstackai/dind\nenv:\n  - MODEL_ID=meta-llama/Llama-3.2-3B-Instruct\n  - HF_TOKEN\nworking_dir: examples/misc/docker-compose\ncommands:\n  - start-dockerd\n  - docker compose up\nports:\n  - 9000\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  # Required resources\n  gpu: \"nvidia:24GB\"\n</code></pre> <p> <pre><code>services:\n  app:\n    image: ghcr.io/huggingface/chat-ui:sha-bf0bc92\n    command:\n      - bash\n      - -c\n      - |\n        echo MONGODB_URL=mongodb://db:27017 &gt; .env.local\n        echo MODELS='`[{\n          \"name\": \"${MODEL_ID?}\",\n          \"endpoints\": [{\"type\": \"tgi\", \"url\": \"http://tgi:8000\"}]\n        }]`' &gt;&gt; .env.local\n        exec ./entrypoint.sh\n    ports:\n      - 127.0.0.1:9000:3000\n    depends_on:\n      - tgi\n      - db\n\n  tgi:\n    image: ghcr.io/huggingface/text-generation-inference:sha-704a58c\n    volumes:\n      - tgi_data:/data\n    environment:\n      HF_TOKEN: ${HF_TOKEN?}\n      MODEL_ID: ${MODEL_ID?}\n      PORT: 8000\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n  db:\n    image: mongo:latest\n    volumes:\n      - db_data:/data/db\n\nvolumes:\n  tgi_data:\n  db_data:\n</code></pre>"},{"location":"examples/misc/docker-compose/#deploying-as-a-service","title":"Deploying as a service","text":"<p>If you'd like to deploy Chat UI as an auto-scalable and secure endpoint, use the service configuration. You can find it at <code>examples/misc/docker-compose/service.dstack.yml</code> </p>"},{"location":"examples/misc/docker-compose/#running-a-configuration","title":"Running a configuration","text":"<p>To run a configuration, use the <code>dstack apply</code> command.</p> <pre><code>$ HF_TOKEN=...\n$ dstack apply -f examples/examples/misc/docker-compose/task.dstack.yml\n\n #  BACKEND  REGION    RESOURCES                    SPOT  PRICE\n 1  runpod   CA-MTL-1  18xCPU, 100GB, A5000:24GB    yes   $0.12\n 2  runpod   EU-SE-1   18xCPU, 100GB, A5000:24GB    yes   $0.12\n 3  gcp      us-west4  27xCPU, 150GB, A5000:24GB:2  yes   $0.23\n\nSubmit the run chat-ui-task? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n</code></pre>"},{"location":"examples/misc/docker-compose/#persisting-data","title":"Persisting data","text":"<p>To persist data between runs, create a volume and attach it to the run configuration.</p> <pre><code>type: task\nname: chat-ui-task\n\nprivileged: true\nimage: dstackai/dind\nenv:\n  - MODEL_ID=meta-llama/Llama-3.2-3B-Instruct\n  - HF_TOKEN\nworking_dir: examples/misc/docker-compose\ncommands:\n  - start-dockerd\n  - docker compose up\nports:\n  - 9000\n\n# Uncomment to leverage spot instances\n#spot_policy: auto\n\nresources:\n  # Required resources\n  gpu: \"nvidia:24GB\"\n\nvolumes:\n  - name: my-dind-volume\n    path: /var/lib/docker\n</code></pre> <p>With this change, all Docker data\u2014pulled images, containers, and crucially, volumes for database and model storage\u2014will be persisted.</p>"},{"location":"examples/misc/docker-compose/#source-code","title":"Source code","text":"<p>The source-code of this example can be found in  <code>examples/misc/docker-compose</code> .</p>"},{"location":"examples/misc/docker-compose/#whats-next","title":"What's next?","text":"<ol> <li>Check dev environments, tasks,     services, and protips.</li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/community/","title":"Community","text":""},{"location":"blog/category/amd/","title":"AMD","text":""},{"location":"blog/category/nvidia/","title":"NVIDIA","text":""},{"location":"blog/category/volumes/","title":"Volumes","text":""},{"location":"blog/category/fleets/","title":"Fleets","text":""},{"location":"blog/category/benchmarks/","title":"Benchmarks","text":""},{"location":"blog/category/observability/","title":"Observability","text":""},{"location":"blog/category/tpu/","title":"TPU","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}