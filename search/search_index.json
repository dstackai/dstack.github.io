{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"blog/","title":"Blog","text":""},{"location":"blog/archive/say-goodbye-to-managed-notebooks/","title":"Say goodbye to managed notebooks","text":"<p>Data science and ML tools have made significant advancements in recent years. This blog post aims to examine the advantages of cloud dev environments (CDE) for ML engineers and compare them with web-based managed notebooks.</p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#notebooks-are-here-to-stay","title":"Notebooks are here to stay","text":"<p>Jupyter notebooks are instrumental for interactive work with data. They provide numerous advantages such as high interactivity, visualization support, remote accessibility, and effortless sharing.</p> <p>Managed notebook platforms, like Google Colab and AWS SageMaker have become popular thanks to their easy integration with clouds. With pre-configured environments, managed notebooks remove the need to worry about infrastructure.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#reproducibility-challenge","title":"Reproducibility challenge","text":"<p>As the code evolves, it needs to be converted into Python scripts and stored in Git for improved organization and version control. Notebooks alone cannot handle this task, which is why they must be a part of a developer environment that also supports Python scripts and Git.</p> <p>The JupyterLab project attempts to address this by turning notebooks into an IDE by adding a file browser, terminal, and Git support.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#ides-get-equipped-for-ml","title":"IDEs get equipped for ML","text":"<p>Recently, IDEs have improved in their ability to support machine learning. They have started to combine the benefits of traditional IDEs and managed notebooks. </p> <p>IDEs have upgraded their remote capabilities, with better SSH support. Additionally, they now offer built-in support for editing notebooks.</p> <p>Two popular IDEs, VS Code and PyCharm, have both integrated remote capabilities and seamless notebook editing features.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#the-rise-of-app-ecosystem","title":"The rise of app ecosystem","text":"<p>Notebooks have been beneficial for their interactivity and sharing features. However, there are new alternatives like Streamlit and Gradio that allow developers to build data apps using Python code. These frameworks not only simplify app-building but also enhance reproducibility by integrating with Git. </p> <p>Hugging Face Spaces, for example, is a popular tool today for sharing Streamlit and Gradio apps with others.</p> <p></p>"},{"location":"blog/archive/say-goodbye-to-managed-notebooks/#say-hello-to-cloud-dev-environments","title":"Say hello to cloud dev environments!","text":"<p>Remote development within IDEs is becoming increasingly popular, and as a result, cloud dev environments have emerged as a new concept. Various managed services, such as Codespaces and GitPod, offer scalable infrastructure while maintaining the familiar IDE experience.</p> <p>One such open-source tool is <code>dstack</code>, which enables you to define your dev environment declaratively as code and run it on any cloud.</p> <pre><code>type: dev-environment\nbuild:\n  - apt-get update\n  - apt-get install -y ffmpeg\n  - pip install -r requirements.txt\nide: vscode\n</code></pre> <p>With this tool, provisioning the required hardware, setting up the pre-built environment (no Docker is needed), and fetching your local code is automated.</p> <pre><code>$ dstack run .\n\n RUN                 CONFIGURATION  USER   PROJECT  INSTANCE       SPOT POLICY\n honest-jellyfish-1  .dstack.yml    peter  gcp      a2-highgpu-1g  on-demand\n\nStarting SSH tunnel...\n\nTo open in VS Code Desktop, use one of these link:\n  vscode://vscode-remote/ssh-remote+honest-jellyfish-1/workflow\n\nTo exit, press Ctrl+C.\n</code></pre> <p>You can securely access the cloud development environment with the desktop IDE of your choice.</p> <p></p> <p>Learn more</p> <p>Check out our guide for running dev environments in your cloud.</p>"},{"location":"blog/dstack-sky/","title":"Introducing dstack Sky","text":"<p>Today we're previewing <code>dstack Sky</code>, a service built on top of  <code>dstack</code> that enables you to get GPUs at competitive rates from a wide pool of providers.</p> <p></p>"},{"location":"blog/dstack-sky/#tldr","title":"TL;DR","text":"<ul> <li>GPUs at competitive rates from multiple providers</li> <li>No need for your own cloud accounts</li> <li>Compatible with <code>dstack</code>'s CLI and API</li> <li>A pre-configured gateway for deploying services</li> </ul>"},{"location":"blog/dstack-sky/#introduction","title":"Introduction","text":"<p><code>dstack</code> is an open-source tool designed for managing AI infrastructure across various cloud platforms. It's lighter and more specifically geared towards AI tasks compared to Kubernetes.</p> <p>Due to its support for multiple cloud providers, <code>dstack</code> is frequently used to access on-demand and spot GPUs  across multiple clouds.  From our users, we've learned that managing various cloud accounts, quotas, and billing can be cumbersome.</p> <p>To streamline this process, we introduce <code>dstack Sky</code>, a managed service that enables users to access GPUs from multiple providers through <code>dstack</code> \u2013 without needing an account in each cloud provider. </p>"},{"location":"blog/dstack-sky/#what-is-dstack-sky","title":"What is dstack Sky?","text":"<p>Instead of running <code>dstack server</code> yourself, you point <code>dstack config</code> to a project set up with <code>dstack Sky</code>.</p> <pre><code>$ dstack config --url https://sky.dstack.ai \\\n    --project my-awesome-project \\\n    --token ca1ee60b-7b3f-8943-9a25-6974c50efa75\n</code></pre> <p>Now, you can use <code>dstack</code>'s CLI or API \u2013 just like you would with your own cloud accounts.</p> <pre><code>$ dstack run . -b tensordock -b vastai\n\n #  BACKEND     REGION  RESOURCES                    SPOT  PRICE \n 1  vastai      canada  16xCPU/64GB/1xRTX4090/1TB    no    $0.35\n 2  vastai      canada  16xCPU/64GB/1xRTX4090/400GB  no    $0.34\n 3  tensordock  us      8xCPU/48GB/1xRTX4090/480GB   no    $0.74\n    ...\n Shown 3 of 50 offers, $0.7424 max\n\nContinue? [y/n]:\n</code></pre> <p>Backends</p> <p><code>dstack Sky</code> supports the same backends as the open-source version, except that you don't need to set them up. By default, it uses all supported backends.</p> <p>You can use both on-demand and spot instances without needing to manage quotas, as they are automatically handled for you.</p> <p>With <code>dstack Sky</code> you can use all of <code>dstack</code>'s features, incl. dev environments,  tasks, services, and  pools.</p> <p>To use services, the open-source version requires setting up a gateway with your own domain.  <code>dstack Sky</code> comes with a pre-configured gateway.</p> <pre><code>$ dstack gateway list\n BACKEND  REGION     NAME    ADDRESS       DOMAIN                            DEFAULT\n aws      eu-west-1  dstack  3.252.79.143  my-awesome-project.sky.dstack.ai  \u2713\n</code></pre> <p>If you run it with <code>dstack Sky</code>, the service's endpoint will be available at <code>https://&lt;run name&gt;.&lt;project name&gt;.sky.dstack.ai</code>.</p> <p>Let's say we define a service:</p> <pre><code>type: service\n# Deploys Mixtral 8x7B with Ollama\n\n# Serve model using Ollama's Docker image\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull mixtral\n  - fg\nport: 11434\n\n# Configure hardware requirements\nresources:\n  gpu: 48GB..80GB\n\n# Enable OpenAI compatible endpoint\nmodel:\n  type: chat\n  name: mixtral\n  format: openai\n</code></pre> <p>If it has a <code>model</code> mapping, the model will be accessible at <code>https://gateway.&lt;project name&gt;.sky.dstack.ai</code> via the OpenAI compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.&lt;project name&gt;.sky.dstack.ai\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mixtral\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Now, you can choose \u2014 either use <code>dstack</code> via the open-source version or via <code>dstack Sky</code>,  or even use them side by side.</p> <p>Credits</p> <p>Are you an active contributor to the AI community? Request free <code>dstack Sky</code> credits.</p> <p><code>dstack Sky</code>  is live on Product Hunt. Support it by giving it your vote!</p> <p>Join Discord</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/0.10.5/","title":"dstack 0.10.5: Lambda integration, Docker support, and more","text":"<p>In the previous update, we added initial integration with Lambda Cloud. With today's release, this integration has significantly improved and finally goes generally available. Additionally, the latest release adds support for custom Docker images.</p>"},{"location":"changelog/0.10.5/#lambda-cloud","title":"Lambda Cloud","text":"<p>In this update, we've added a possibility to create Lambda Cloud projects via the user interface.</p> <p></p> <p>All you need to do is provide your Lambda Cloud API key, and specify an S3 bucket and AWS credentials  for storing state and artifacts.</p> <p>Learn more \u2192</p> <p>Once the project is configured, feel free to run dev environments and tasks in Lambda Cloud using the <code>dstack</code> CLI.</p>"},{"location":"changelog/0.10.5/#custom-docker-images","title":"Custom Docker images","text":"<p>By default, <code>dstack</code> uses its own base Docker images to run  dev environments and tasks. These base images come pre-configured with Python, Conda, and essential CUDA drivers.  However, there may be times when you need additional dependencies that you don't want to install every time you run your dev environment or task.</p> <p>To address this, <code>dstack</code> now allows specifying custom Docker images. Here's an example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nenv:\n  - MODEL_ID=tiiuae/falcon-7b\n\nports:\n - 3000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 3000 --trust-remote-code\n</code></pre> Existing limitations <p>Dev environments require the Docker image to have <code>openssh-server</code> pre-installed. If you want to use a custom Docker image with a dev environment and it does not include <code>openssh-server</code>, you can install it using the following  method:</p> <pre><code>type: dev-environment\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9\n\nbuild:\n  - apt-get update\n  - DEBIAN_FRONTEND=noninteractive apt-get install -y openssh-server\n  - rm -rf /var/lib/apt/lists/*\n\nide: vscode\n</code></pre> <p>The documentation and examples are updated to reflect the changes in the release.</p>"},{"location":"changelog/0.10.5/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"changelog/0.10.7/","title":"dstack 0.10.7: Services","text":"<p>Until now, <code>dstack</code> has supported <code>dev-environment</code> and <code>task</code> as configuration types. Even though <code>task</code>  may be used for basic serving use cases, it lacks crucial serving features. With the new update, we introduce <code>service</code>, a dedicated configuration type for serving.</p> <p>Consider the following example:</p> <pre><code>type: task\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nports: \n  - 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>When running it, the <code>dstack</code> CLI forwards traffic to <code>127.0.0.1:8000</code>. This is convenient for development but unsuitable for production.</p> <p>In production, you need your endpoint available on the external network, preferably behind authentication  and a load balancer. </p> <p>This is why we introduce the <code>service</code> configuration type.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:0.9.3\n\nport: 8000\n\ncommands: \n  - text-generation-launcher --hostname 0.0.0.0 --port 8000 --trust-remote-code\n</code></pre> <p>As you see, there are two differences compared to <code>task</code>.</p> <ol> <li>The <code>gateway</code> property: the address of a special cloud instance that wraps the running service with a public    endpoint. Currently, you must specify it manually. In the future, <code>dstack</code> will assign it automatically.</li> <li>The <code>port</code> property: A service must always configure one port on which it's running.</li> </ol> <p>When running, <code>dstack</code> forwards the traffic to the gateway, providing you with a public endpoint that you can use to access the running service.</p> Existing limitations <ol> <li>Currently, you must create a gateway manually using the <code>dstack gateway</code> command  and specify its address via YAML (e.g. using secrets). In the future, <code>dstack</code> will assign it automatically.</li> <li>Gateways do not support HTTPS yet. When you run a service, its endpoint URL is <code>&lt;the address of the gateway&gt;:80</code>.  The port can be overridden via the port property: instead of <code>8000</code>, specify <code>&lt;gateway port&gt;:8000</code>.</li> <li>Gateways do not provide authorization and auto-scaling. In the future, <code>dstack</code> will support them as well.</li> </ol> <p>This initial support for services is the first step towards providing multi-cloud and cost-effective inference.</p> <p>Give it a try and share feedback</p> <p>Even though the current support is limited in many ways, we encourage you to give it a try and share your feedback with us!</p> <p>More details on how to use services can be found in a dedicated guide in our docs.  Questions and requests for help are very much welcome in our Discord server.</p>"},{"location":"changelog/0.11.0/","title":"dstack 0.11.0: Multi-cloud and multi-region projects","text":"<p>The latest release of <code>dstack</code> enables the automatic discovery of the best GPU price and availability across multiple configured cloud providers and regions.</p>"},{"location":"changelog/0.11.0/#multiple-backends-per-project","title":"Multiple backends per project","text":"<p>Now, <code>dstack</code> leverages price data from multiple configured cloud providers and regions to automatically suggest the most cost-effective options.</p> <pre><code>$ dstack run . -f llama-2/train.dstack.yml --gpu A100\n\n Configuration       llama-2/train.dstack.yml\n Min resources       2xCPUs, 8GB, 1xA100\n Max price           no\n Spot policy         auto\n Max duration        72h\n\n #  BACKEND  RESOURCES                      SPOT  PRICE\n 2  lambda   30xCPUs, 200GB, 1xA100 (80GB)  yes   $1.1\n 3  gcp      12xCPUs, 85GB, 1xA100 (40GB)   yes   $1.20582\n 1  azure    24xCPUs, 220GB, 1xA100 (80GB)  yes   $1.6469\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>The default behavior of <code>dstack</code> is to first attempt the most cost-effective options, provided they are available. You have the option to set a maximum price limit either through <code>max_price</code> in <code>.dstack/profiles.yml</code> or by using <code>--max-price</code> in the <code>dstack run</code> command.</p> <p>To implement this change, we have modified the way projects are configured. You can now configure multiple clouds and regions within a single project.</p> <p></p> <p>Why this matter?</p> <p>The ability to run LLM workloads across multiple cloud GPU providers allows for a significant reduction in costs and an increase in availability, while also remaining independent of any particular cloud vendor.</p> <p>We hope that the value of <code>dstack</code> will continue to grow as we expand our support for additional cloud GPU providers. If you're interested in a specific provider, please message us on Discord.</p>"},{"location":"changelog/0.11.0/#custom-domains-and-https","title":"Custom domains and HTTPS","text":"<p>In other news, it is now possible to deploy services using HTTPS. All you need to do is configure a wildcard domain (e.g., <code>*.mydomain.com</code>), point it to the gateway IP address, and then pass the subdomain you want to use (e.g., <code>myservice.mydomain.com</code>) to the <code>gateway</code> property in YAML (instead of the gateway IP address).</p>"},{"location":"changelog/0.11.0/#other-changes","title":"Other changes","text":""},{"location":"changelog/0.11.0/#dstackprofilesyml","title":".dstack/profiles.yml","text":"<ul> <li>The <code>project</code> property is no longer supported.</li> <li>You can now use <code>max_price</code> to set the maximum price per hour in dollars.</li> </ul>"},{"location":"changelog/0.11.0/#dstack-run","title":"dstack run","text":"<p>Using the dstack run command, you are now able to utilize options such as <code>--gpu</code>, <code>--memory</code>, <code>--env</code>, <code>--max-price</code>, and several other arguments to override the profile settings.</p> <p>Lastly, the local backend is no longer supported. Now, you can run everything using only a cloud backend.</p> <p>The documentation is updated to reflect the changes in the release.</p> <p>Migration to 0.11</p> <p>The <code>dstack</code> version 0.11 update brings significant changes that break backward compatibility. If you used prior <code>dstack</code> versions, after updating to <code>dstack==0.11</code>, you'll need to log in to the UI and reconfigure clouds. </p> <p>We apologize for any inconvenience and aim to ensure future updates maintain backward compatibility.</p>"},{"location":"changelog/0.11.0/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[aws,gcp,azure,lambda]\" -U\n$ dstack start\n</code></pre>"},{"location":"changelog/0.12.0/","title":"dstack 0.12.0: Simplified cloud setup, and refined API","text":"<p>For the past six weeks, we've been diligently overhauling <code>dstack</code> with the aim of significantly simplifying the process of configuring clouds and enhancing the functionality of the API. Please take note of the breaking changes, as they necessitate careful migration.</p>"},{"location":"changelog/0.12.0/#cloud-setup","title":"Cloud setup","text":"<p>Previously, the only way to configure clouds for a project was through the UI. Additionally, you had to specify not only the credentials but also set up a storage bucket for each cloud to store metadata.</p> <p>Now, you can configure clouds for a project via <code>~/.dstack/server/config.yml</code>. Example:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Regions and other settings are optional. Learn more on what credential types are supported  via Clouds.</p>"},{"location":"changelog/0.12.0/#enhanced-api","title":"Enhanced API","text":"<p>The earlier introduced Python API is now greatly refined.</p> <p>Creating a <code>dstack</code> client is as easy as this: </p> <pre><code>from dstack.api import Client, ClientError\n\ntry:\n    client = Client.from_config()\nexcept ClientError:\n    print(\"Can't connect to the server\")\n</code></pre> <p>Now, you can submit a task or a service:</p> <pre><code>from dstack.api import Task, Resources, GPU\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",\n    configuration=task,\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n</code></pre> <p>The <code>dstack.api.Run</code> instance provides methods for various operations including attaching to the run,  forwarding ports to <code>localhost</code>, retrieving status, stopping, and accessing logs. For more details, refer to  the reference.</p>"},{"location":"changelog/0.12.0/#other-changes","title":"Other changes","text":"<ul> <li>Because we've prioritized CLI and API UX over the UI, the UI is no longer bundled.  Please inform us if you experience any significant inconvenience related to this.</li> <li>Gateways should now be configured using the <code>dstack gateway</code> command, and their usage requires you to specify a domain.   Learn more about how to set up a gateway.</li> <li>The <code>dstack start</code> command is now <code>dstack server</code>.</li> <li>The Python API classes were moved from the <code>dstack</code> package to <code>dstack.api</code>.</li> </ul>"},{"location":"changelog/0.12.0/#migration","title":"Migration","text":"<p>Unfortunately, when upgrading to 0.12.0, there is no automatic migration for data. This means you'll need to delete <code>~/.dstack</code> and configure <code>dstack</code> from scratch.</p> <ol> <li><code>pip install \"dstack[all]==0.12.0\"</code></li> <li>Delete <code>~/.dstack</code></li> <li>Configure clouds via <code>~/.dstack/server/config.yml</code> (see the new guide)</li> <li>Run <code>dstack server</code></li> </ol> <p>The documentation and examples are updated.</p>"},{"location":"changelog/0.12.0/#give-it-a-try","title":"Give it a try","text":"<p>Getting started with <code>dstack</code> takes less than a minute. Go ahead and give it a try.</p> <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n</code></pre> <p>Feedback and support</p> <p>Questions and requests for help are very much welcome in our  Discord server.</p>"},{"location":"changelog/0.12.2/","title":"dstack 0.12.2: TensorDock integration","text":"<p>At <code>dstack</code>, we remain committed to our mission of building the most convenient tool for orchestrating generative AI workloads in the cloud. In today's release, we have added support for TensorDock, making it easier for you to leverage cloud GPUs at highly competitive prices.</p> <p>Configuring your TensorDock account with <code>dstack</code> is very easy. Simply generate an authorization key in your TensorDock API settings and set it up in <code>~/.dstack/server/config.yml</code>:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>Now you can restart the server and proceed to using the CLI or API for running development environments, tasks, and services.</p> <pre><code>$ dstack run . -f .dstack.yml --gpu 40GB\n\n Min resources  1xGPU (40GB)\n Max price      -\n Max duration   6h\n Retry policy   no\n\n #  REGION        INSTANCE  RESOURCES                     SPOT  PRICE\n 1  unitedstates  ef483076  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6235\n 2  canada        0ca177e7  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n 3  canada        45d0cabd  10xCPU, 80GB, 1xA6000 (48GB)  no    $0.6435\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>TensorDock offers cloud GPUs on top of servers from dozens of independent hosts, providing some of the most affordable GPU pricing you can find on the internet.</p> <p>With <code>dstack</code>, you can now utilize TensorDock's GPUs through a highly convenient interface, which includes the developer-friendly CLI and API.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p>"},{"location":"changelog/0.12.3/","title":"dstack 0.12.3: Vast.ai integration","text":"<p><code>dstack</code> simplifies gen AI model development and deployment through its developer-friendly CLI and API.  It eliminates cloud infrastructure hassles while supporting top cloud providers (such as AWS, GCP, Azure, among others).</p> <p>While <code>dstack</code> streamlines infrastructure challenges, GPU costs can still hinder development. To address this,  we've integrated <code>dstack</code> with Vast.ai, a marketplace providing GPUs from independent hosts at  notably lower prices compared to other providers.</p> <p>With the <code>dstack</code> 0.12.3 release, it's now possible use Vast.ai alongside other cloud providers.</p> <pre><code>$ dstack run . --gpu 24GB --backend vastai --max-price 0.4\n\n #  REGION            INSTANCE  RESOURCES                       PRICE\n 1  pl-greaterpoland  6244171   16xCPU, 32GB, 1xRTX3090 (24GB)  $0.18478\n 2  ee-harjumaa       6648481   16xCPU, 64GB, 1xA5000 (24GB)    $0.29583\n 3  pl-greaterpoland  6244172   32xCPU, 64GB, 2xRTX3090 (24GB)  $0.36678\n    ...\n\nContinue? [y/n]:\n</code></pre> <p>By default, it suggests GPU instances based on their quality score. If you want to, you can control the maximum price.</p> <p>Configuring Vast.ai for use with <code>dstack</code> is easy. Log into your Vast AI account, click <code>Account</code> in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>Now you can restart the server and proceed to using <code>dstack</code>'s CLI and API.</p> <p>If you want an easy way to  develop, train and deploy gen AI models using affordable cloud GPUs,  give <code>dstack</code> with Vast.ai a try.</p> <p>Feedback and support</p> <p>Feel free to ask questions or seek help in our  Discord server.</p>"},{"location":"changelog/0.13.0/","title":"dstack 0.13.0: Disk size, CUDA 12.1, Mixtral, and more","text":"<p>As we wrap up this year, we're releasing a new update and publishing a guide on deploying Mixtral 8x7B with <code>dstack</code>.</p>"},{"location":"changelog/0.13.0/#configurable-disk-size","title":"Configurable disk size","text":"<p>Previously, <code>dstack</code> set the disk size to <code>100GB</code> regardless of the cloud provider. Now, to accommodate larger language models and datasets, <code>dstack</code> enables setting a custom disk size using <code>--disk</code> in <code>dstack run</code> or via the <code>disk</code> property in <code>.dstack/profiles.yml</code>.</p>"},{"location":"changelog/0.13.0/#default-docker-image","title":"Default Docker image","text":"<p>With <code>dstack</code>, whether you're using dev environments, tasks, or services, you can opt for a custom Docker image (for self-installed dependencies) or stick with the default Docker image (<code>dstack</code> pre-installs CUDA drivers, Conda, Python, etc.).</p> <p>We've upgraded the default Docker image's CUDA drivers to 12.1 (for better compatibility with modern libraries).</p> <p>nvcc</p> <p>If you're using the default Docker image and need the CUDA compiler (<code>nvcc</code>), you'll have to install it manually using <code>conda install cuda</code>. The image comes pre-configured with the  <code>nvidia/label/cuda-12.1.0</code> Conda channel.</p>"},{"location":"changelog/0.13.0/#mixtral-8x7b","title":"Mixtral 8x7B","text":"<p>Lastly, and most importantly, we've added a guide on deploying Mixtral 8x7B as a service. This guide allows you to effortlessly deploy a Mixtral endpoint on any cloud platform of your preference.</p> <p>Deploying Mixtral 8x7B is easy, especailly when using vLLM:</p> <pre><code>type: service\n\npython: \"3.11\"\n\ncommands:\n  - conda install cuda # (required by megablocks)\n  - pip install torch # (required by megablocks)\n  - pip install vllm megablocks\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size 2 # should match the number of GPUs\n\nport: 8000\n</code></pre> <p>Once the configuration is defined, goahead and run it:</p> <pre><code>$ dstack run . -f llms/mixtral.dstack.yml --gpu \"80GB:2\" --disk 200GB\n</code></pre> <p>It will deploy the endpoint at <code>https://&lt;run-name&gt;.&lt;gateway-domain&gt;</code>.</p> <p>Because vLLM provides an OpenAI-compatible endpoint, feel free to access it using various OpenAI-compatible tools like Chat UI, LangChain, Llama Index, etc.</p> <p></p> <p>Check the complete guide for more details.</p> <p>Don't forget, with <code>dstack</code>, you can use spot instances across different clouds and regions.</p>"},{"location":"changelog/0.13.0/#feedback-and-support","title":"Feedback and support","text":"<p>That's all! Feel free to try out the update and the new guide, and share your feedback with us.</p> <p>For updates or assistance, join our Discord.</p>"},{"location":"changelog/0.14.0/","title":"dstack 0.14.0: OpenAI-compatible endpoints preview","text":"<p>The <code>service</code> configuration deploys any application as a public endpoint. For instance, you can use HuggingFace's  TGI or other frameworks to deploy custom LLMs.  While this is simple and customizable, using different frameworks and LLMs complicates the integration of LLMs.</p> <p>With <code>dstack 0.14.0</code>, we are extending the <code>service</code> configuration in <code>dstack</code> to enable you to optionally map your custom LLM to an OpenAI-compatible endpoint.</p> <p>Here's how it works: you define a <code>service</code> (as before) and include the <code>model</code> property with  the model's <code>type</code>, <code>name</code>, <code>format</code>, and other settings.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\n\n# Optional mapping for OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: mistralai/Mistral-7B-Instruct-v0.1\n  format: tgi\n</code></pre> <p>When you deploy the service using <code>dstack run</code>, <code>dstack</code> will automatically publish the OpenAI-compatible endpoint, converting the prompt and response format between your LLM and OpenAI interface.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://gateway.&lt;your gateway domain&gt;\",\n    api_key=\"none\"\n)\n\ncompletion = client.chat.completions.create(\n    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Here's a live demo of how it works:</p> <p></p> <p>For more details on how to use the new feature, be sure to check the updated documentation on services, and the TGI example.</p>"},{"location":"changelog/0.14.0/#migration-guide","title":"Migration guide","text":"<p>Note: After you update to <code>0.14.0</code>, it's important to delete your existing gateways (if any) using <code>dstack gateway delete</code> and create them again with <code>dstack gateway create</code>.</p>"},{"location":"changelog/0.14.0/#feedback","title":"Feedback","text":"<p>In case you have any questions, experience bugs, or need help,  drop us a message on our Discord server or submit it as a  GitHub issue.</p>"},{"location":"changelog/0.15.0/","title":"dstack 0.15.0: Resources, authentication, and more","text":"<p>The latest update brings many improvements, enabling the configuration of resources in YAML files, requiring authentication in services, supporting OpenAI-compatible endpoints for vLLM, and more. </p>"},{"location":"changelog/0.15.0/#resource-configuration","title":"Resource configuration","text":"<p>Previously, if you wanted to request hardware resources, you had to either use the corresponding arguments with <code>dstack run</code> (e.g. <code>--gpu GPU_SPEC</code>) or use <code>.dstack/profiles.yml</code>.</p> <p>With <code>0.15.0</code>, it is now possible to configure resources in the YAML configuration file:</p> <pre><code>type: dev-environment\n\npython: 3.11\nide: vscode\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc \nresources:\n  gpu: 24GB\n</code></pre> <p>Supported properties include: <code>gpu</code>, <code>cpu</code>, <code>memory</code>, <code>disk</code>, and <code>shm_size</code>.</p> <p>If you specify memory size, you can either specify an explicit size (e.g. <code>24GB</code>) or a  range (e.g. <code>24GB..</code>, or <code>24GB..80GB</code>, or <code>..80GB</code>).</p> <p>The <code>gpu</code> property allows specifying not only memory size but also GPU names and their quantity. Examples: <code>A100</code> (one A100), <code>A10G,A100</code> (either A10G or A100),  <code>A100:80GB</code> (one A100 of 80GB), <code>A100:2</code> (two A100), <code>24GB..40GB:2</code> (two GPUs between 24GB and 40GB), etc.</p> <p>It's also possible to configure <code>gpu</code> as an object:</p> <pre><code>type: dev-environment\n\npython: 3.11\nide: vscode\n\n# Require 2 GPUs of at least 40GB with CUDA compute compatibility of 7.5\nresources:\n  gpu:\n    count: 2\n    memory: 40GB..\n    compute_capability: 7.5\n</code></pre> <p>For more details on <code>resources</code> schema, refer to the Reference.</p>"},{"location":"changelog/0.15.0/#authentication-in-services","title":"Authentication in services","text":"<p>Previously, when deploying a service, the public endpoint didn't support authentication,  meaning anyone with access to the gateway could call it.</p> <p>With <code>0.15.0</code>, by default, service endpoints require the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>. </p> <pre><code>$ curl https://yellow-cat-1.example.com/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"&amp;lt;s&amp;gt;[INST] What is your favourite condiment?[/INST]\"}' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authentication: \"Bearer &amp;lt;dstack token&amp;gt;\"'\n</code></pre> <p>Authentication can be disabled by setting <code>auth</code> to <code>false</code> in the service configuration file.</p>"},{"location":"changelog/0.15.0/#openai-interface","title":"OpenAI interface","text":"<p>In case the service has model mapping configured,  the OpenAI-compatible endpoint requires authentication.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.example.com\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre>"},{"location":"changelog/0.15.0/#model-mapping-for-vllm","title":"Model mapping for vLLM","text":"<p>Last but not least, we've added one more format for model mapping: <code>openai</code>.</p> <p>For example, if you run vLLM using the OpenAI mode, it's possible to configure model mapping for it.</p> <pre><code>type: service\n\npython: \"3.11\"\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nresources:\n  gpu: 24GB\n\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre> <p>When we run such a service, it will be possible to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface  and using your <code>dstack</code> user token.</p>"},{"location":"changelog/0.15.0/#feedback","title":"Feedback","text":"<p>In case you have any questions, experience bugs, or need help,  drop us a message on our Discord server or submit it as a  GitHub issue.</p>"},{"location":"changelog/0.15.1/","title":"dstack 0.15.1: Kubernetes integration","text":"<p>In addition to a few bug fixes, the latest update brings initial integration with Kubernetes (experimental) and adds the possibility to configure a custom VPC for AWS. Read below for more details.</p>"},{"location":"changelog/0.15.1/#configuring-a-kubernetes-backend","title":"Configuring a Kubernetes backend","text":"<p>With the latest update, it's now possible to configure a Kubernetes backend. In this case, if you run a workload, <code>dstack</code> will provision infrastructure within your Kubernetes cluster. This may work with both self-managed and managed clusters.</p> Prerequisite <p>To use GPUs with Kubernetes, the cluster must be installed with the NVIDIA GPU Operator.</p> <p>To configure a Kubernetes backend, you need to specify the path to the kubeconfig file, and the port that <code>dstack</code> can use for proxying SSH traffic. In case of a self-managed cluster, also specify the IP address of any node in the cluster.</p> Self-managedManaged <p>Here's how to configure the backend to use a self-managed cluster.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_host: localhost # The external IP address of any node\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using Kind, make sure to add it via <code>extraPortMappings</code>:</p> <p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32000 # Must be same as `ssh_port`\n    hostPort: 32000 # Must be same as `ssh_port`\n</code></pre> <p>Here's how to configure the backend to use a managed cluster (AWS, GCP, Azure).</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using EKS, make sure to add it via an ingress rule of the corresponding security group:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;cluster-security-group-id&gt; --protocol tcp --port 32000 --cidr 0.0.0.0/0\n</code></pre> <p>NOTE:</p> <p>While <code>dstack</code> supports both self-managed and managed clusters, if you're using AWS, GCP, or Azure, it's generally recommended to corresponding backends directly for greater efficiency and ease of use.</p>"},{"location":"changelog/0.15.1/#specifying-a-custom-vpc-for-aws","title":"Specifying a custom VPC for AWS","text":"<p>If you're using <code>dstack</code> with AWS, it's now possible to configure a custom VPC via <code>~/.dstack/server/config.yml</code>:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    vpc_name: my-custom-vpc\n    regions: [us-west-2, eu-west-1]\n    creds:\n      type: default\n</code></pre> <p>In this case, <code>dstack</code> will attempt to utilize the VPC with the configured name in each region. If any region lacks a VPC with that name, it will be skipped.</p> <p>NOTE:</p> <p>All subnets of the configured VPC should be public; otherwise, <code>dstack</code> won't be able to manage workloads.</p>"},{"location":"changelog/0.15.1/#feedback","title":"Feedback","text":"<p>Have questions or need help? Drop us a message on our Discord server. See a bug? Report it to GitHub issues.</p>"},{"location":"changelog/0.16.0/","title":"dstack 0.16.0: Pools","text":"<p>Previously, when running a dev environment, task, or service, <code>dstack</code> provisioned an instance in a configured backend, and upon completion of the run, deleted the instance.</p> <p>In the latest update, we introduce pools, a significantly more efficient way to manage instance lifecycles and reuse instances across runs.</p>"},{"location":"changelog/0.16.0/#dstack-run","title":"<code>dstack run</code>","text":"<p>Now, when using the <code>dstack run</code> command, it tries to reuse an instance from a pool. If no ready instance meets the requirements, <code>dstack</code> automatically provisions a new one and adds it to the pool.</p> <p>Once the workload finishes, the instance is marked as ready (to run other workloads). If the instance remains idle for the configured duration, <code>dstack</code> tears it down.</p> <p>Idle duration</p> <p>By default, if <code>dstack run</code> provisions a new instance, its idle duration is set to <code>5m</code>. This means the instance waits for a new workload for only five minutes before getting torn down. To override it, use the <code>--idle-duration DURATION</code> argument.</p>"},{"location":"changelog/0.16.0/#dstack-pool","title":"<code>dstack pool</code>","text":"<p>The <code>dstack pool</code> command allows for managing instances within pools.</p> <p>To manually add an instance to a pool, use <code>dstack pool add</code>:</p> <pre><code>$ dstack pool add --gpu 80GB --idle-duration 1d\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n</code></pre> <p>The <code>dstack pool add</code> command allows specifying resource requirements, along with the spot policy, idle duration, max price, retry policy, and other policies.</p> <p>If no idle duration is configured, by default, <code>dstack</code> sets it to <code>72h</code>.  To override it, use the <code>--idle-duration DURATION</code> argument.</p> Limitations <p>The <code>dstack pool add</code> command is not yet supported for Lambda, Azure, TensorDock, Kubernetes, and VastAI backends. Support for them is coming in version <code>0.16.1</code>.</p> <p>Refer to pools for more details on the new feature and how to use it.</p>"},{"location":"changelog/0.16.0/#why-does-this-matter","title":"Why does this matter?","text":"<p>With this new feature, using the cloud can be a lot more predictable and convenient:</p> <ol> <li>Now, you can provision instances in advance and ensure they are available for the entire duration of the project.    This saves you from the risk of not having a GPU when you need it most.</li> <li>If you reuse an instance from a pool, <code>dstack run</code> starts much faster.    For example, you can provision an instance and reuse it for running a dev environment, task, or service.</li> </ol>"},{"location":"changelog/0.16.0/#feedback","title":"Feedback","text":"<p>Have questions or need help? Drop us a message on our Discord server. See a bug? Report it to GitHub issues.</p>"},{"location":"changelog/0.16.1/","title":"dstack 0.16.1: Improvements to <code>dstack pool</code> and bug-fixes","text":"<p>The latest update enhances the <code>dstack pool</code> command introduced earlier,  and it fixes a number of important bugs.</p>"},{"location":"changelog/0.16.1/#improvements-to-dstack-pool","title":"Improvements to <code>dstack pool</code>","text":"<p>The <code>dstack pool</code> command, that allows you to manually add instances to the pool,  has received several improvements:</p> <ul> <li>The <code>dstack pool add</code> command now works with all VM-based backends (which means all backends except <code>vastai</code>   and <code>kubernetes</code>).</li> <li>The <code>dstack pool add</code> command now accepts the arguments to configure the spot policy   (via <code>--spot-auto</code>, <code>--spot</code>, <code>--on-demand</code>) and idle duration (via <code>--idle-duration DURATION</code>).   By default, the spot policy is set to <code>on-demand</code>, while the idle duration is set to <code>72h</code>.</li> </ul> <p>Didn't try <code>dstack pool</code> yet? Give it a try now. It significantly improves the predictability and convenience of using cloud GPUs.</p>"},{"location":"changelog/0.16.1/#bug-fixes","title":"Bug-fixes","text":"<p>The major bug-fixes include:</p> <ul> <li>The <code>0.16.0</code> update broke the <code>vastai</code> backend (the <code>dstack run</code> command didn't show offers). </li> <li>If you submitted runs via the API, the default idle duration was not applied, leading to instances staying in the pool   and not being automatically removed.</li> <li><code>dstack</code> couldn't connect to the instance via SSH due to a number of issues related to not properly handling the user'   s default SSH config.</li> <li>When connecting to a run via <code>ssh &lt;run name&gt;</code> (while using the default Docker image),    <code>python</code>, <code>pip</code>, and <code>conda</code> couldn't be found due to the broken <code>PATH</code>.</li> </ul> <p>All of these issues have been fixed now.</p>"},{"location":"changelog/0.16.1/#feedback","title":"Feedback","text":"<p>Have questions or need help? Drop us a message on our Discord server. See a bug? Report it to GitHub issues.</p>"},{"location":"changelog/0.16.4/","title":"dstack 0.16.4: CUDO Compute integration","text":"<p>On our journey to provide an open-source, cloud-agnostic platform for orchestrating GPU workloads, we are proud to announce another step forward \u2013 the integration with CUDO Compute.</p> <p>CUDO Compute is a GPU marketplace that offers cloud resources at an affordable cost in a number of locations. Currently, the available GPUs include A40, RTX A6000,  RTX A4000, RTX A5000, and RTX 3080.</p> <p>To use it with <code>dstack,</code> you only need to configure the <code>cudo</code> backend with your CUDO Compute project ID and API key:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: cudo\n    project_id: my-cudo-project\n    creds:\n      type: api_key\n      api_key: 7487240a466624b48de22865589\n</code></pre> <p>Once it's done, you can restart the <code>dstack server</code> and use the <code>dstack</code> CLI or API to run  workloads.</p> <pre><code>$ dstack run . -b cudo \n #  BACKEND  REGION       RESOURCES              SPOT  PRICE\n 1  cudo     no-luster-1  25xCPU, 96GB, 1xA6000  no    $1.17267\n                          (48GB), 100GB (disk)\n 2  cudo     no-luster-1  26xCPU, 100GB, 1xA6000  no   $1.17477\n                          (48GB), 100GB (disk)\n 3  cudo     no-luster-1  27xCPU, 100GB, 1xA6000  no   $1.17687\n                          (48GB), 100GB (disk)\n    ...\n Shown 3 of 8 offers, $1.18737 max\n\n Continue? [y/n]:\n</code></pre> <p>Just like with other backends, the <code>cudo</code> backend allows you to launch dev environments, run tasks, and deploy services with <code>dstack run</code>, and manage your pool of instances via <code>dstack pool</code>.</p> Limitations <p>The <code>dstack gateway</code> feature is not yet compatible with <code>cudo</code>, but it is expected to be supported in version <code>0.17.0</code>, planned for release within a week.</p> <p>The <code>cudo</code> backend cannot yet be used with dstack Sky, but it will also be enabled within a week.</p> <p>Haven't tried <code>dstack</code> yet? You're very welcome to do so now. With <code>dstack</code>,  orchestrating GPU workloads over any cloud is very easy!</p>"},{"location":"changelog/0.16.4/#feedback","title":"Feedback","text":"<p>Have questions or need help? Drop us a message on our Discord server. See a bug? Report it to GitHub issues.</p>"},{"location":"changelog/0.17.0/","title":"dstack 0.17.0: Service auto-scaling, and other improvements","text":"<p>The latest update previews service replicas and auto-scaling, and brings many other improvements.</p>"},{"location":"changelog/0.17.0/#service-auto-scaling","title":"Service auto-scaling","text":"<p>Previously, <code>dstack</code> always served services as single replicas. While this is suitable for development, in production, the service must automatically scale based on the load.</p> <p>That's why in <code>0.17.0</code>, we extended <code>dstack</code> with the capability to configure the number of  replicas as well as the auto-scaling policy.</p> <pre><code>type: service\n\npython: \"3.11\"\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nreplicas: 1..4\nscaling:\n  metric: rps\n  target: 10\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre> <p>The <code>replicas</code> property can be set either to a number or to a range. In the case of a range, the <code>scaling</code> property is required to configure the auto-scaling policy.  The auto-scaling policy requires specifying <code>metric</code> (such as <code>rps</code>, i.e. \"requests per second\") and its <code>target</code>  (the metric value).</p>"},{"location":"changelog/0.17.0/#regions-and-instance-types","title":"Regions and instance types","text":"<p>Also, the update brings a simpler way to configure regions and instance types.</p> <p>For example, if you'd like to use only a subset of specific regions or instance types, you can now configure them via <code>.dstack/profiles.yml</code>.</p> <pre><code>profiles:\n  - name: custom\n    default: false\n\n    regions:\n      - us-east-1\n      - us-east-2\n\n    instance_types:\n      - p3.2xlarge\n      - p3.8xlarge\n      - p3.16xlarge\n</code></pre> <p>Then you can pass it to <code>dstack run</code> with <code>--profile custom</code>. Alternatively, you can set <code>default</code> to <code>true</code>, and then  <code>dstack run</code> will apply it automatically.</p> <p>If you don't want to define a profile, you can use the <code>--region</code> and <code>--instance-type</code> options directly with <code>dstack run</code>.</p>"},{"location":"changelog/0.17.0/#environment-variables","title":"Environment variables","text":"<p>Previously, environment variables had to be hardcoded in the configuration file or passed via the CLI. The update brings two major improvements.</p> <p>Firstly, it's now possible to configure an environment variable in the configuration without hardcoding its value.  Secondly, <code>dstack run</code> now inherits environment variables from the current process.</p> <p>Together, these features allow users to define environment variables separately from the configuration and pass them to <code>dstack run</code> conveniently, such as by using a <code>.env</code> file.</p> <pre><code>type: task\n\npython: \"3.11\"\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python fine-tuning/qlora/train.py --merge_and_push ${{ run.args }}\nports:\n  - 6006\n\nresources:\n  gpu: 16GB..24GB\n</code></pre> <p>Now, if you run this configuration, <code>dstack</code> will ensure that you've set <code>HUGGING_FACE_HUB_TOKEN</code> (either via <code>HUGGING_FACE_HUB_TOKEN=&lt;value&gt; dstack run ...</code>, <code>dstack run -e HUGGING_FACE_HUB_TOKEN=&lt;value&gt; ...</code>, or by using other tools such as <code>direnv</code> or similar.</p>"},{"location":"changelog/0.17.0/#feedback","title":"Feedback","text":"<p>Have questions or need help? Drop us a message on our Discord server!</p>"},{"location":"changelog/0.2/","title":"dstack 0.2: GCP integration","text":"<p>With the release of version 0.2 of <code>dstack</code>, it is now possible to configure GCP as a remote. All features that were previously available for AWS, except real-time artifacts, are now available for GCP as well.</p> <p>This means that you can define your ML workflows in code and easily run them locally or remotely in your GCP account.</p> <p><code>dstack</code> automatically creates and deletes cloud instances as needed, and assists in setting up the environment, including pipeline dependencies, and saving/loading artifacts. </p> <p>No code changes are required since ML workflows are described in YAML. You won't need to deal with Docker, Kubernetes, or stateful UI.</p> <p>This article will explain how to use <code>dstack</code> to run remote ML workflows on GCP.</p>"},{"location":"changelog/0.2/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the latest version of <code>dstack</code> before proceeding.</p> <pre><code>$ pip install dstack --upgrade\n</code></pre> <p>By default, workflows run locally. To run workflows remotely, e.g. on a GCP account), you must configure a remote using the <code>dstack config</code> command. Follow the steps below to do so.</p>"},{"location":"changelog/0.2/#1-create-a-project","title":"1. Create a project","text":"<p>First you have to create a project in your GCP account, link a billing to it, and make sure that the required APIs and enabled for it.</p> <pre><code>cloudapis.googleapis.com\ncompute.googleapis.com \nlogging.googleapis.com\nsecretmanager.googleapis.com\nstorage-api.googleapis.com\nstorage-component.googleapis.com \nstorage.googleapis.com \n</code></pre>"},{"location":"changelog/0.2/#2-create-a-storage-bucket","title":"2. Create a storage bucket","text":"<p>Once the project is set up, you can proceed and create a storage bucket. This bucket will be used to store workflow artifacts and metadata.</p> <p>NOTE:</p> <p>Make sure to create the bucket in the sane location where you'd like to run your workflows.</p>"},{"location":"changelog/0.2/#3-create-a-service-account","title":"3. Create a service account","text":"<p>The next step is to create a service account in the created project and configure the following roles for it: <code>Service Account User</code>, <code>Compute Admin</code>, <code>Storage Admin</code>, <code>Secret Manager Admin</code>, and <code>Logging Admin</code>.</p> <p>Once the service account is set up, create a key for it and download the corresponding JSON file to your local machine (e.g. to <code>~/Downloads/my-awesome-project-d7735ca1dd53.json</code>).</p>"},{"location":"changelog/0.2/#4-configure-the-cli","title":"4. Configure the CLI","text":"<p>Once the service account key JSON file is on your machine, you can configure the CLI using the <code>dstack config</code> command.</p> <p>The command will ask you for a path to the key, GCP region and zone, and storage bucket name.</p> <pre><code>$ dstack config\n\n? Choose backend: gcp\n? Enter path to credentials file: ~/Downloads/dstack-d7735ca1dd53.json\n? Choose GCP geographic area: North America\n? Choose GCP region: us-west1\n? Choose GCP zone: us-west1-b\n? Choose storage bucket: dstack-dstack-us-west1\n? Choose VPC subnet: no preference\n</code></pre> <p>That's it! Now you can run remote workflows on GCP.</p>"},{"location":"changelog/0.7.0/","title":"dstack 0.7.0: Introducing dstack server","text":"<p>Last October, we open-sourced the <code>dstack</code> CLI for defining ML workflows as code and running them easily on any cloud or locally. The tool abstracts ML engineers from vendor APIs and infrastructure, making it convenient to run scripts, development environments, and applications.</p> <p>Today, we are excited to announce a preview of <code>Hub</code>, a new way to use dstack for teams to manage their model development workflows effectively on any cloud platform.</p>"},{"location":"changelog/0.7.0/#how-does-it-work","title":"How does it work?","text":"<p>Previously, the <code>dstack</code> CLI configured a cloud account as a remote to use local cloud credentials for direct requests to the cloud. Now, the CLI allows configuration of Hub as a remote, enabling requests to the cloud using user credentials stored in Hub.</p> <pre><code>sequenceDiagram\n  autonumber\n  participant CLI\n  participant Hub\n  participant Cloud\n  %  Note right of Cloud: AWS, GCP, etc\n  CLI-&gt;&gt;Hub: Run a workflow\n  activate Hub\n      Hub--&gt;&gt;Hub: User authentication\n      loop Workflow provider\n        Hub--&gt;&gt;Cloud: Submit workflow jobs\n      end\n  Hub--&gt;&gt;CLI: Return the workflow status\n  deactivate Hub\n  loop Workflow scheduler\n    Hub--&gt;&gt;Cloud: Re-submit workflow jobs\n  end</code></pre> <p>The Hub not only provides basic features such as authentication and credential storage, but it also has built-in workflow scheduling capabilities. For instance, it can monitor the availability of spot instances and automatically resubmit jobs.</p>"},{"location":"changelog/0.7.0/#why-does-it-matter","title":"Why does it matter?","text":"<p>As you start developing models more regularly, you'll encounter the challenge of automating your ML workflows to reduce time spent on infrastructure and manual work.</p> <p>While many cloud vendors offer tools to automate ML workflows, they do so through opinionated UIs and APIs, leading to a suboptimal developer experience and vendor lock-in.</p> <p>In contrast, <code>dstack</code> aims to provide a non-opinionated and developer-friendly interface that can work across any  vendor.</p>"},{"location":"changelog/0.7.0/#try-the-preview","title":"Try the preview","text":"<p>Here's a quick guide to get started with Hub:</p> <ol> <li>Start the Hub application</li> <li>Visit the URL provided in the output to log in as an administrator</li> <li>Create a project and configure its backend (AWS or GCP)</li> <li>Configure the CLI to use the project as a remote</li> </ol> <p>For more details, visit the Hub documentation. </p>"},{"location":"changelog/0.7.0/#whats-next","title":"What's next?","text":"<p>Currently, the only way to run or manage workflows is through the <code>dstack</code> CLI. There are scenarios when you'd prefer to run workflows other ways, e.g. from Python code or programmatically via API. To support these scenarios, we plan to release soon Python SDK and REST API.</p> <p>The built-in scheduler currently monitors spot instance availability and automatically resubmits jobs. Our plan is to enhance this feature and include additional capabilities. Users will be able to track cloud compute usage, and manage quotes per team via the user interface.</p> <p>Lastly, and of utmost importance, we plan to extend support to other cloud platforms, not limiting ourselves to AWS, GCP, and Azure.</p>"},{"location":"changelog/0.7.0/#contribution","title":"Contribution","text":"<p>You are encouraged to report any bugs, suggest new features, and provide feedback to improve Hub through GitHub issues.</p>"},{"location":"changelog/0.9.1/","title":"dstack 0.9.1: Azure integration","text":"<p>At <code>dstack</code>, our goal is to create a simple and unified interface for ML engineers to run dev environments, pipelines, and apps on any cloud. With the latest update, we take another significant step in this direction.</p> <p>We are thrilled to announce that the latest update introduces Azure support, among other things, making it incredibly easy to run dev environments, pipelines, and apps in Azure. Read on for more details.</p>"},{"location":"changelog/0.9.1/#azure-support","title":"Azure support","text":"<p>Using Azure with <code>dstack</code> is very straightforward. All you need to do is create the corresponding project via the UI and  provide your Azure credentials.</p> <p></p> <p>NOTE:</p> <p>For detailed instructions on setting up <code>dstack</code> for Azure, refer to the documentation.</p> <p>Once the project is set up, you can define dev environments, pipelines, and apps as code, and easily run them with just a single command. <code>dstack</code> will automatically provision the infrastructure for you.</p>"},{"location":"changelog/0.9.1/#logs-and-artifacts-in-ui","title":"Logs and artifacts in UI","text":"<p>Secondly, with the new update, you now have the ability to browse the logs and artifacts of any run through the user interface.</p> <p></p>"},{"location":"changelog/0.9.1/#better-documentation","title":"Better documentation","text":"<p>Last but not least, with the update, we have reworked the documentation to provide a greater emphasis on specific use cases: dev environments,  tasks, and services.</p>"},{"location":"changelog/0.9.1/#try-it-out","title":"Try it out","text":"<p>Please note that when installing <code>dstack</code> via <code>pip</code>, you now need to specify the exact list of cloud providers you intend to use:</p> <pre><code>$ pip install \"dstack[aws,gcp,azure]\" -U\n</code></pre> <p>This requirement applies only when you start the server locally. If you connect to a server hosted elsewhere,  you can use the shorter syntax:<code>pip install dstack</code>.</p> <p>Feedback</p> <p>If you have any feedback, including issues or questions, please share them in our Discord community or file it as a GitHub issue.</p>"},{"location":"docs/","title":"What is dstack?","text":"<p><code>dstack</code> is an open-source engine for running GPU workloads on any cloud. It works with a wide range of cloud GPU providers (AWS, GCP, Azure, Lambda, TensorDock, Vast.ai, etc.) as well as on-premises servers.</p>"},{"location":"docs/#why-use-dstack","title":"Why use dstack?","text":"<ol> <li>Designed for development, training, and deployment of gen AI models.</li> <li>Efficiently utilizes GPUs across regions and cloud providers.</li> <li>Compatible with any frameworks.</li> <li>100% open-source.</li> </ol>"},{"location":"docs/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Install the open-source server and configure backends (or sign up with the dstack Sky) </li> <li>Define configurations such as dev environments, tasks, and services.</li> <li>Run configurations via the CLI or API. The <code>dstack</code> server automatically provisions cloud resources, handles     containers, logs, network, and everything else.</li> </ol>"},{"location":"docs/#where-do-i-start","title":"Where do I start?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord</li> </ol>"},{"location":"docs/quickstart/","title":"Quickstart","text":"Prerequisites <p>To use the open-source version, make sure to install the server and configure backends.</p> <p>If you're using dstack Sky, install the CLI and run the <code>dstack config</code> command:</p> <p></p> <p>Once the CLI is set up, follow the quickstart.</p>"},{"location":"docs/quickstart/#initialize-a-repo","title":"Initialize a repo","text":"<p>To use <code>dstack</code>'s CLI in a folder, first run <code>dstack init</code> within that folder.</p> <pre><code>$ mkdir quickstart &amp;&amp; cd quickstart\n$ dstack init\n</code></pre> <p>Your folder can be a regular local folder or a Git repo.</p>"},{"location":"docs/quickstart/#define-a-configuration","title":"Define a configuration","text":"<p>Define what you want to run as a YAML file. The filename must end with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> Dev environmentTaskService <p>Dev environments allow you to quickly provision a machine with a pre-configured environment, resources, IDE, code, etc.</p> <p> <pre><code>type: dev-environment\n\n# Use either `python` or `image` to configure environment\npython: \"3.11\"\n# image: ghcr.io/huggingface/text-generation-inference:latest\n\nide: vscode\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>Tasks make it very easy to run any scripts, be it for training, data processing, or web apps. They allow you to pre-configure the environment, resources, code, etc.</p> <p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>Ensure <code>requirements.txt</code> and <code>train.py</code> are in your folder. You can take them from <code>dstack-examples</code>.</p> <p>Services make it easy to deploy models and apps cost-effectively as public endpoints, allowing you to use any frameworks.</p> <p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre>"},{"location":"docs/quickstart/#run-configuration","title":"Run configuration","text":"<p>Run a configuration using the <code>dstack run</code> command, followed by the working directory path (e.g., <code>.</code>), the path to the configuration file, and run options (e.g., configuring hardware resources, spot policy, etc.)</p> <pre><code>$ dstack run . -f train.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>The <code>dstack run</code> command automatically uploads your code, including any local uncommitted changes.  To exclude any files from uploading, use <code>.gitignore</code>.</p>"},{"location":"docs/quickstart/#whats-next","title":"What's next?","text":"<ol> <li>Read about dev environments, tasks,      services, and pools </li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"docs/concepts/dev-environments/","title":"Dev environments","text":"<p>Before submitting a task or deploying a model, you may want to run code interactively. Dev environments allow you to do exactly that. </p> <p>You specify the required environment and resources, then run it. <code>dstack</code> provisions the dev environment in the configured backend and enables access via your desktop IDE.</p>"},{"location":"docs/concepts/dev-environments/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable).</p> <pre><code>type: dev-environment\n\n# Use either `python` or `image` to configure environment\npython: \"3.11\"\n\n# image: ghcr.io/huggingface/text-generation-inference:latest\nide: vscode\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>The YAML file allows you to specify your own Docker image, environment variables,  resource requirements, etc. If image is not specified, <code>dstack</code> uses its own (pre-configured with Python, Conda, and essential CUDA drivers).</p> Environment variables <p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <p>If you only specify the name of the environment variable without specifying the value,  <code>dstack</code> will require that the value is passed via the CLI or set for the current process.</p> <pre><code>type: dev-environment\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\n\npython: \"3.11\"\nide: vscode\n\nresources:\n  gpu: 80GB\n</code></pre> <p>This way, you can define environment variables in a <code>.env</code> file and also use tools such as <code>direnv</code> and similar.</p> <p>For more details on the file syntax, refer to <code>.dstack.yml</code>.</p>"},{"location":"docs/concepts/dev-environments/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and other options.</p> <pre><code>$ dstack run . -f .dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning `fast-moth-1`...\n---&gt; 100%\n\nTo open in VS Code Desktop, use this link:\n  vscode://vscode-remote/ssh-remote+fast-moth-1/workflow\n</code></pre> <p>When <code>dstack</code> provisions the dev environment, it uses the current folder contents.</p> <p>Exclude files</p> <p>If there are large files or folders you'd like to avoid uploading,  you can list them in either <code>.gitignore</code> or <code>.dstackignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/dev-environments/#ide","title":"IDE","text":"<p>To open the dev environment in your desktop IDE, use the link from the output  (such as <code>vscode://vscode-remote/ssh-remote+fast-moth-1/workflow</code>).</p> <p></p>"},{"location":"docs/concepts/dev-environments/#ssh","title":"SSH","text":"<p>Alternatively, you can connect to the dev environment via SSH:</p> <pre><code>$ ssh fast-moth-1\n</code></pre>"},{"location":"docs/concepts/dev-environments/#configure-profiles","title":"Configure profiles","text":"<p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration,  max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/dev-environments/#manage-runs","title":"Manage runs","text":""},{"location":"docs/concepts/dev-environments/#stop-a-run","title":"Stop a run","text":"<p>Once the run exceeds the max duration, or when you use <code>dstack stop</code>,  the dev environment and its cloud resources are deleted.</p>"},{"location":"docs/concepts/dev-environments/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/dev-environments/#whats-next","title":"What's next?","text":"<ol> <li>Check out <code>.dstack.yml</code>, <code>dstack run</code>,     and <code>profiles.yml</code></li> <li>Read about tasks, services, and pools</li> </ol>"},{"location":"docs/concepts/pools/","title":"Pools","text":"<p>Pools simplify managing the lifecycle of cloud instances and enable their efficient reuse across runs.</p> <p>You can have instances provisioned in the configured backend automatically when you run a workload, or add them manually, configuring the required resources, idle duration, etc.</p>"},{"location":"docs/concepts/pools/#add-instances","title":"Add instances","text":""},{"location":"docs/concepts/pools/#dstack-run","title":"<code>dstack run</code>","text":"<p>By default, when using the <code>dstack run</code> command, it tries to reuse an instance from a pool. If no idle instance meets the requirements, <code>dstack</code> automatically provisions a new one and adds it to the pool.</p> <p>To avoid provisioning new instances with <code>dstack run</code>, use <code>--reuse</code>. Your run will be assigned to an idle instance in  the pool.</p> <p>Idle duration</p> <p>By default, <code>dstack run</code> sets the idle duration of a newly provisioned instance to <code>5m</code>. This means that if the run is finished and the instance remains idle for longer than five minutes, it is automatically removed from the pool. To override the default idle duration, use  <code>--idle-duration DURATION</code> with <code>dstack run</code>.</p>"},{"location":"docs/concepts/pools/#dstack-pool-add","title":"<code>dstack pool add</code>","text":"<p>To manually add an instance to a pool, use <code>dstack pool add</code>:</p> <pre><code>$ dstack pool add --gpu 80GB\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n</code></pre> <p>The <code>dstack pool add</code> command allows specifying resource requirements, along with the spot policy, idle duration, max price, retry policy, and other policies.</p> <p>The default idle duration if you're using <code>dstack pool add</code> is <code>72h</code>. To override it, use the <code>--idle-duration DURATION</code> argument.</p> <p>You can also specify the policies via <code>.dstack/profiles.yml</code> instead of passing them as arguments. For more details on policies and their defaults, refer to <code>.dstack/profiles.yml</code>.</p> Limitations <p>The <code>dstack pool add</code> command is not supported for Kubernetes, and VastAI backends yet.</p>"},{"location":"docs/concepts/pools/#remove-instances","title":"Remove instances","text":"<p>Idle duration</p> <p>If the instance remains idle for the configured duration, <code>dstack</code> removes it and deletes all cloud resources.</p>"},{"location":"docs/concepts/pools/#dstack-pool-remove","title":"<code>dstack pool remove</code>","text":"<p>To remove an instance from the pool manually, use the <code>dstack pool remove</code> command. </p> <pre><code>$ dstack pool remove &amp;lt;instance name&amp;gt;\n</code></pre>"},{"location":"docs/concepts/pools/#list-instances","title":"List instances","text":"<p>The <code>dstack pool ps</code> command lists active instances and their status (<code>busy</code> or <code>idle</code>).</p>"},{"location":"docs/concepts/services/","title":"Services","text":"<p>Services make it very easy to deploy any kind of model or web application as public endpoints.</p> <p>Use any serving frameworks and specify required resources. <code>dstack</code> deploys it in the configured backend, handles authentication, auto-scaling, and provides an OpenAI-compatible interface if needed.</p> Prerequisites <p>If you're using the open-source server, you first have to set up a gateway.</p>"},{"location":"docs/concepts/services/#set-up-a-gateway","title":"Set up a gateway","text":"<p>For example, if your domain is <code>example.com</code>, go ahead and run the  <code>dstack gateway create</code> command:</p> <pre><code>$ dstack gateway create --domain example.com --region eu-west-1 --backend aws\n\nCreating gateway...\n---&gt; 100%\n\n BACKEND  REGION     NAME          ADDRESS        DOMAIN       DEFAULT\n aws      eu-west-1  sour-fireant  52.148.254.14  example.com  \u2713\n</code></pre> <p>Afterward, in your domain's DNS settings, add an <code>A</code> DNS record for <code>*.example.com</code>  pointing to the IP address of the gateway.</p> <p>Now, if you run a service, <code>dstack</code> will make its endpoint available at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>In case your service has the model mapping configured, <code>dstack</code> will  automatically make your model available at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <p>If you're using dstack Sky, the gateway is set up for you.</p>"},{"location":"docs/concepts/services/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>The YAML file allows you to specify your own Docker image, environment variables,  resource requirements, etc. If image is not specified, <code>dstack</code> uses its own (pre-configured with Python, Conda, and essential CUDA drivers).</p> Environment variables <p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <p>If you only specify the name of the environment variable without specifying the value,  <code>dstack</code> will require that the value is passed via the CLI or set for the current process.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>This way, you can define environment variables in a <code>.env</code> file and also use tools such as <code>direnv</code> and similar.</p> <p>For more details on the file syntax, refer to <code>.dstack.yml</code>.</p>"},{"location":"docs/concepts/services/#configure-model-mapping","title":"Configure model mapping","text":"<p>By default, if you run a service, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>If you run a model, you can optionally configure the mapping to make it accessible via the  OpenAI-compatible interface.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n\n# (Optional) Enable the OpenAI-compatible endpoint   \nmodel:\n  type: chat\n  name: mistralai/Mistral-7B-Instruct-v0.1\n  format: tgi\n</code></pre> <p>In this case, with such a configuration, once the service is up, you'll be able to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <p>The <code>format</code> supports only <code>tgi</code> (Text Generation Inference)  and <code>openai</code> (if you are using Text Generation Inference or vLLM with OpenAI-compatible mode).</p> Chat template <p>By default, <code>dstack</code> loads the chat template  from the model's repository. If it is not present there, manual configuration is required.</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code --quantize gptq\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Llama-2-13B-chat-GPTQ\n  format: tgi\n  chat_template: \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\\n' + system_message + '\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '&lt;s&gt;[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' &lt;/s&gt;' }}{% endif %}{% endfor %}\"\n  eos_token: \"&lt;/s&gt;\"\n</code></pre>"},{"location":"docs/concepts/services/#limitations","title":"Limitations","text":"<p>Please note that model mapping is an experimental feature with the following limitations:</p> <ol> <li>Doesn't work if your <code>chat_template</code> uses <code>bos_token</code>. As a workaround, replace <code>bos_token</code> inside <code>chat_template</code> with the token content itself.</li> <li>Doesn't work if <code>eos_token</code> is defined in the model repository as a dictionary. As a workaround, set <code>eos_token</code> manually, as shown in the example above (see Chat template).</li> </ol> <p>If you encounter any other issues, please make sure to file a GitHub issue.</p>"},{"location":"docs/concepts/services/#configure-replicas-and-auto-scaling","title":"Configure replicas and auto-scaling","text":"<p>By default, <code>dstack</code> runs a single replica of the service. You can configure the number of replicas as well as the auto-scaling policy.</p> <pre><code>type: service\n\npython: \"3.11\"\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nreplicas: 1..4\nscaling:\n  metric: rps\n  target: 10\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre> <p>If you specify the minimum number of replicas as <code>0</code>, the service will scale down to zero when there are no requests.</p>"},{"location":"docs/concepts/services/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and any other options.</p> <pre><code>$ dstack run . -f serve.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nService is published at https://yellow-cat-1.example.com\n</code></pre> <p>When <code>dstack</code> submits the task, it uses the current folder contents.</p> <p>Exclude files</p> <p>If there are large files or folders you'd like to avoid uploading,  you can list them in either <code>.gitignore</code> or <code>.dstackignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/services/#service-endpoint","title":"Service endpoint","text":"<p>One the service is up, its endpoint is accessible at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p>"},{"location":"docs/concepts/services/#authentication","title":"Authentication","text":"<p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>. </p> <pre><code>$ curl https://yellow-cat-1.example.com/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"&amp;lt;s&amp;gt;[INST] What is your favourite condiment?[/INST]\"}' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authentication: \"Bearer &amp;lt;dstack token&amp;gt;\"'\n</code></pre> <p>Authentication can be disabled by setting <code>auth</code> to <code>false</code> in the service configuration file.</p>"},{"location":"docs/concepts/services/#openai-interface","title":"OpenAI interface","text":"<p>In case the service has the model mapping configured, you will also be able  to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.example.com\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre>"},{"location":"docs/concepts/services/#configure-profiles","title":"Configure profiles","text":"<p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration,  max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/services/#manage-runs","title":"Manage runs","text":""},{"location":"docs/concepts/services/#stop-a-run","title":"Stop a run","text":"<p>When you use <code>dstack stop</code>, the service and its cloud resources are deleted.</p>"},{"location":"docs/concepts/services/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/services/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about dev environments, tasks, and pools</li> <li>Browse examples</li> <li>Check the reference</li> </ol>"},{"location":"docs/concepts/tasks/","title":"Tasks","text":"<p>Tasks allow for convenient scheduling of any kind of batch jobs, such as training, fine-tuning, or data processing, as well as running web applications.</p> <p>You simply specify the commands, required environment, and resources, and then submit it. <code>dstack</code> provisions the required resources in a configured backend and runs the task.</p>"},{"location":"docs/concepts/tasks/#define-a-configuration","title":"Define a configuration","text":"<p>First, create a YAML file in your project folder. Its name must end with <code>.dstack.yml</code> (e.g. <code>.dstack.yml</code> or <code>train.dstack.yml</code> are both acceptable).</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>The YAML file allows you to specify your own Docker image, environment variables,  resource requirements, etc. If image is not specified, <code>dstack</code> uses its own (pre-configured with Python, Conda, and essential CUDA drivers).</p> Environment variables <p>Environment variables can be set either within the configuration file or passed via the CLI.</p> <p>If you only specify the name of the environment variable without specifying the value,  <code>dstack</code> will require that the value is passed via the CLI or set for the current process.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>This way, you can define environment variables in a <code>.env</code> file and also use tools such as <code>direnv</code> and similar.</p> <p>For more details on the file syntax, refer to <code>.dstack.yml</code>.</p>"},{"location":"docs/concepts/tasks/#configure-ports","title":"Configure ports","text":"<p>A task can configure ports. In this case, if the task is running an application on a port, <code>dstack run</code>  will securely allow you to access this port from your local machine through port forwarding.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python fine-tuning/qlora/train.py\nports:\n  - 6000\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>When running it, <code>dstack run</code> forwards <code>6000</code> port to <code>localhost:6000</code>, enabling secure access. </p> Override port mapping <p>By default, <code>dstack</code> uses the same ports on your local machine for port forwarding. However, you can override local ports using <code>--port</code>:</p> <pre><code>$ dstack run . -f train.dstack.yml --port 6000:6001\n</code></pre> <p>This will forward the task's port <code>6000</code> to <code>localhost:6001</code>.</p>"},{"location":"docs/concepts/tasks/#parametrize-tasks","title":"Parametrize tasks","text":"<p>You can parameterize tasks with user arguments using <code>${{ run.args }}</code> in the configuration.</p> <pre><code>type: task\n\npython: \"3.11\"\nenv:\n  - HF_HUB_ENABLE_HF_TRANSFER=1\ncommands:\n  - pip install -r fine-tuning/qlora/requirements.txt\n  - python fine-tuning/qlora/train.py ${{ run.args }}\n\n# (Optional) Configure `gpu`, `memory`, `disk`, etc\nresources:\n  gpu: 80GB\n</code></pre> <p>Now, you can pass your arguments to the <code>dstack run</code> command:</p> <pre><code>$ dstack run . -f train.dstack.yml --train_batch_size=1 --num_train_epochs=100\n</code></pre> <p>The <code>dstack run</code> command will pass <code>--train_batch_size=1</code> and <code>--num_train_epochs=100</code> as arguments to <code>train.py</code>.</p>"},{"location":"docs/concepts/tasks/#run-the-configuration","title":"Run the configuration","text":"<p>To run a configuration, use the <code>dstack run</code> command followed by the working directory path,  configuration file path, and other options.</p> <pre><code>$ dstack run . -f train.dstack.yml\n\n BACKEND     REGION         RESOURCES                     SPOT  PRICE\n tensordock  unitedkingdom  10xCPU, 80GB, 1xA100 (80GB)   no    $1.595\n azure       westus3        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n azure       westus2        24xCPU, 220GB, 1xA100 (80GB)  no    $3.673\n\nContinue? [y/n]: y\n\nProvisioning...\n---&gt; 100%\n\nEpoch 0:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 1:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\nEpoch 2:  100% 1719/1719 [00:18&lt;00:00, 92.32it/s, loss=0.0981, acc=0.969]\n</code></pre> <p>When <code>dstack</code> submits the task, it uses the current folder contents.</p> <p>Exclude files</p> <p>If there are large files or folders you'd like to avoid uploading,  you can list them in either <code>.gitignore</code> or <code>.dstackignore</code>.</p> <p>The <code>dstack run</code> command allows specifying many things, including spot policy, retry and max duration,  max price, regions, instance types, and much more.</p>"},{"location":"docs/concepts/tasks/#configure-profiles","title":"Configure profiles","text":"<p>In case you'd like to reuse certain parameters (such as spot policy, retry and max duration,  max price, regions, instance types, etc.) across runs, you can define them via <code>.dstack/profiles.yml</code>.</p>"},{"location":"docs/concepts/tasks/#manage-runs","title":"Manage runs","text":""},{"location":"docs/concepts/tasks/#stop-a-run","title":"Stop a run","text":"<p>Once the run exceeds the max duration, or when you use <code>dstack stop</code>,  the task and its cloud resources are deleted.</p>"},{"location":"docs/concepts/tasks/#list-runs","title":"List runs","text":"<p>The <code>dstack ps</code> command lists all running runs and their status.</p>"},{"location":"docs/concepts/tasks/#whats-next","title":"What's next?","text":"<ol> <li>Check the QLoRA example</li> <li>Read about dev environments and services</li> <li>Browse examples</li> <li>Check the reference</li> <li>Check Python API and REST API</li> </ol>"},{"location":"docs/installation/","title":"Installation","text":"<p>Follow this guide to install the open-source version of <code>dstack</code> server.</p>"},{"location":"docs/installation/#set-up-the-server","title":"Set up the server","text":"pipDocker <pre><code>$ pip install \"dstack[all]\" -U\n$ dstack server\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/\n</code></pre> <pre><code>$ docker run -p 3000:3000 -v $HOME/.dstack/server/:/root/.dstack/server dstackai/dstack\n\nApplying ~/.dstack/server/config.yml...\n\nThe admin token is \"bbae0f28-d3dd-4820-bf61-8f4bb40815da\"\nThe server is running at http://127.0.0.1:3000/.\n</code></pre> <p>NOTE:</p> <p>For flexibility, <code>dstack</code> server allows you to configure multiple project. The default project is <code>main</code>.</p>"},{"location":"docs/installation/#configure-backends","title":"Configure backends","text":"<p>To let <code>dstack</code> run workloads in your cloud account(s), you need to configure cloud credentials  in <code>~/.dstack/server/config.yml</code> under the <code>backends</code> property of the respective project.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: AIZKISCVKUKO5AAKLAEH\n      secret_key: QSbmpqJIUBn1V5U3pyM9S6lwwiu8/fOJ2dgfwFdW\n</code></pre> <p>Default credentials</p> <p>If you have default AWS, GCP, or Azure credentials on your machine, the <code>dstack</code> server will pick them up automatically. Otherwise, you have to configure them manually.</p>"},{"location":"docs/installation/#aws","title":"AWS","text":"<p>There are two ways to configure AWS: using an access key or using the default credentials.</p> Access keyDefault credentials <p>Create an access key by following the this guide. Once you've downloaded the <code>.csv</code> file with your IAM user's Access key ID and Secret access key, proceed to  configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: access_key\n      access_key: KKAAUKLIZ5EHKICAOASV\n      secret_key: pn158lMqSBJiySwpQ9ubwmI6VUU3/W2fdJdFwfgO\n</code></pre> <p>If you have default credentials set up (e.g. in <code>~/.aws/credentials</code>), configure the backend like this:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: aws\n    creds:\n      type: default\n</code></pre> Required AWS permissions <p>The following AWS policy permissions are sufficient for <code>dstack</code> to work:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"servicequotas:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetRole\",\n                \"iam:CreateRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:TagRole\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreatePolicy\",\n                \"iam:TagPolicy\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetInstanceProfile\",\n                \"iam:CreateInstanceProfile\",\n                \"iam:AddRoleToInstanceProfile\",\n                \"iam:TagInstanceProfile\",\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"docs/installation/#azure","title":"Azure","text":"<p>There are two ways to configure Azure: using a client secret or using the default credentials.</p> Client secretDefault credentials <p>A client secret can be created using the Azure CLI:</p> <pre><code>SUBSCRIPTION_ID=...\naz ad sp create-for-rbac \n    --name dstack-app \\\n    --role Owner \\ \n    --scopes /subscriptions/$SUBSCRIPTION_ID \\ \n    --query \"{ tenant_id: tenant, client_id: appId, client_secret: password }\"\n</code></pre> <p>Once you have <code>tenant_id</code>, <code>client_id</code>, and <code>client_secret</code>, go ahead and configure the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: client\n      client_id: acf3f73a-597b-46b6-98d9-748d75018ed0\n      client_secret: 1Kb8Q~o3Q2hdEvrul9yaj5DJDFkuL3RG7lger2VQ\n</code></pre> <p>Obtain the <code>subscription_id</code> and <code>tenant_id</code> via the Azure CLI:</p> <pre><code>az account show --query \"{subscription_id: id, tenant_id: tenantId}\"\n</code></pre> <p>Then proceed to configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: azure\n    subscription_id: 06c82ce3-28ff-4285-a146-c5e981a9d808\n    tenant_id: f84a7584-88e4-4fd2-8e97-623f0a715ee1\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your <code>subscription_id</code>, run</p> <pre><code>az account show --query \"{subscription_id: id}\"\n</code></pre> Required Azure permissions <p>You must have the <code>Owner</code> permission for the Azure subscription. Please, let us know if your use case requires more granular Azure permissions.</p>"},{"location":"docs/installation/#gcp","title":"GCP","text":"Enable APIs <p>First, ensure the required APIs are enabled in your GCP <code>project_id</code>.</p> <pre><code>PROJECT_ID=...\ngcloud config set project $PROJECT_ID\ngcloud services enable cloudapis.googleapis.com\ngcloud services enable compute.googleapis.com \n</code></pre> <p>There are two ways to configure GCP: using a service account or using the default credentials.</p> Service accountDefault credentials <p>To create a service account, follow this guide. Make sure to grant it the <code>Service Account User</code> and <code>Compute Admin</code> roles.</p> <p>After setting up the service account create a key for it  and download the corresponding JSON file.</p> <p>Then go ahead and configure the backend by specifying the downloaded file path.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: service_account\n      filename: ~/.dstack/server/gcp-024ed630eab5.json\n</code></pre> <pre><code>projects:\n- name: main\n  backends:\n  - type: gcp\n    project_id: gcp-project-id\n    creds:\n      type: default\n</code></pre> <p>NOTE:</p> <p>If you don't know your GCP project ID, run </p> <pre><code>gcloud projects list --format=\"json(projectId)\"\n</code></pre> Required GCP permissions <p>The <code>Service Account User</code> and <code>Compute Admin</code> roles are sufficient for <code>dstack</code> to work.</p>"},{"location":"docs/installation/#lambda","title":"Lambda","text":"<p>Log into your Lambda Cloud account, click API keys in the sidebar, and then click the <code>Generate API key</code> button to create a new API key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: lambda\n    creds:\n      type: api_key\n      api_key: eersct_yrpiey-naaeedst-tk-_cb6ba38e1128464aea9bcc619e4ba2a5.iijPMi07obgt6TZ87v5qAEj61RVxhd0p\n</code></pre>"},{"location":"docs/installation/#tensordock","title":"TensorDock","text":"<p>Log into your TensorDock account, click API in the sidebar, and use the <code>Create an Authorization</code> section to create a new authorization key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: tensordock\n    creds:\n      type: api_key\n      api_key: 248e621d-9317-7494-dc1557fa5825b-98b\n      api_token: FyBI3YbnFEYXdth2xqYRnQI7hiusssBC\n</code></pre> <p>NOTE:</p> <p>The <code>tensordock</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#vast-ai","title":"Vast AI","text":"<p>Log into your Vast AI account, click Account in the sidebar, and copy your API Key.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: vastai\n    creds:\n      type: api_key\n      api_key: d75789f22f1908e0527c78a283b523dd73051c8c7d05456516fc91e9d4efd8c5\n</code></pre> <p>NOTE:</p> <p>Also, the <code>vastai</code> backend supports on-demand instances only. Spot instance support coming soon.</p>"},{"location":"docs/installation/#cudo","title":"CUDO","text":"<p>Log into your CUDO Compute account, click API keys in the sidebar, and click the <code>Create an API key</code> button.</p> <p>Ensure you've created a project with CUDO Compute, then proceed to configuring the backend.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: cudo\n    project_id: my-cudo-project\n    creds:\n      type: api_key\n      api_key: 7487240a466624b48de22865589\n</code></pre>"},{"location":"docs/installation/#datacrunch","title":"DataCrunch","text":"<p>Log into your DataCrunch account, click Account Settings in the sidebar, find <code>REST API Credentials</code> area and then click the <code>Generate Credentials</code> button.</p> <p>Then, go ahead and configure the backend:</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: datacrunch\n    creds:\n      type: api_key\n      client_id: xfaHBqYEsArqhKWX-e52x3HH7w8T\n      client_secret: B5ZU5Qx9Nt8oGMlmMhNI3iglK8bjMhagTbylZy4WzncZe39995f7Vxh8\n</code></pre>"},{"location":"docs/installation/#kubernetes","title":"Kubernetes","text":"<p><code>dstack</code> supports both self-managed, and managed Kubernetes clusters.</p> Prerequisite <p>To use GPUs with Kubernetes, the cluster must be installed with the  NVIDIA GPU Operator.</p> <p>To configure a Kubernetes backend, specify the path to the kubeconfig file,  and the port that <code>dstack</code> can use for proxying SSH traffic. In case of a self-managed cluster, also specify the IP address of any node in the cluster.</p> Self-managedManaged <p>Here's how to configure the backend to use a self-managed cluster.</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_host: localhost # The external IP address of any node\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using Kind, make sure to add it via <code>extraPortMappings</code>:</p> <p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32000 # Must be same as `ssh_port`\n    hostPort: 32000 # Must be same as `ssh_port`\n</code></pre> <p>Here's how to configure the backend to use a managed cluster (AWS, GCP, Azure).</p> <pre><code>projects:\n- name: main\n  backends:\n  - type: kubernetes\n    kubeconfig:\n      filename: ~/.kube/config\n    networking:\n      ssh_port: 32000 # Any port accessible outside of the cluster\n</code></pre> <p>The port specified to <code>ssh_port</code> must be accessible outside of the cluster.</p> <p>For example, if you are using EKS, make sure to add it via an ingress rule of the corresponding security group:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;cluster-security-group-id&gt; --protocol tcp --port 32000 --cidr 0.0.0.0/0\n</code></pre> <p>After you update <code>~/.dstack/server/config.yml</code>, make sure to restart the server.</p>"},{"location":"docs/installation/#set-up-the-client","title":"Set up the client","text":"<p>The client is configured via <code>~/.dstack/config.yml</code> with the server address, user token, and the project name. </p> <p>If you run <code>dstack server</code> on the same machine, it automatically updates the client configuration for the default project (<code>main</code>).</p> <p>To configure the client on a different machine or for other projects, use <code>dstack config</code>.</p> <pre><code>$ dstack config --url &amp;lt;your server adddress&amp;gt; --project &amp;lt;your project name&amp;gt; --token &amp;lt;your user token&amp;gt;\n\nConfigurated is updated at ~/.dstack/config.yml\n</code></pre>"},{"location":"docs/installation/#whats-next","title":"What's next?","text":"<ol> <li>Follow quickstart</li> <li>Browse examples</li> <li>Join the community via Discord</li> </ol>"},{"location":"docs/reference/dstack.yml/","title":".dstack.yml","text":"<p>With <code>dstack</code>, you can define what you want to run as YAML configuration files  and run them using the <code>dstack run</code> command. </p> <p>Filename</p> <p>Configuration files must have a name ending with <code>.dstack.yml</code> (e.g., <code>.dstack.yml</code> or <code>dev.dstack.yml</code> are both acceptable). Configuration files can be placed either in the project's root directory or in any nested folder. </p> <p>Configurations can be of three types: <code>dev-environment</code>, <code>task</code>, and <code>service</code>.</p> <p>Below, you'll find the complete reference detailing all available properties for each type of configuration.</p>"},{"location":"docs/reference/dstack.yml/#dev-environment","title":"dev-environment","text":"<p>This configuration type allows you to provision a dev environment with the required cloud resources,  code, and environment.</p> Property Description Type Default value <code>type</code> <code>Literal['dev-environment']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, Union[str,EnvSentinel]]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>resources</code> The resources requirements to run the configuration <code>ResourcesSpec</code> <code>cpu=Range[int](min=2, max=None) memory=Range[Memory](min=8GB, max=None) shm_size=None gpu=None disk=None</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>ide</code> The IDE to run <code>Literal['vscode']</code> required <code>version</code> The version of the IDE <code>Optional[str]</code> <code>None</code> <code>init</code> The bash commands to run <code>List[str]</code> <code>[]</code>"},{"location":"docs/reference/dstack.yml/#task","title":"task","text":"<p>This configuration type allows you to run tasks like training scripts, batch jobs, or web apps.</p> Property Description Type Default value <code>type</code> <code>Literal['task']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, Union[str,EnvSentinel]]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>resources</code> The resources requirements to run the configuration <code>ResourcesSpec</code> <code>cpu=Range[int](min=2, max=None) memory=Range[Memory](min=8GB, max=None) shm_size=None gpu=None disk=None</code> <code>ports</code> Port numbers/mapping to expose <code>List[Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]]</code> <code>[]</code> <code>commands</code> The bash commands to run <code>List[str]</code> required"},{"location":"docs/reference/dstack.yml/#service","title":"service","text":"<p>This configuration type allows you to deploy models or web apps as services.</p> Property Description Type Default value <code>type</code> <code>Literal['service']</code> required <code>image</code> The name of the Docker image to run <code>Optional[str]</code> <code>None</code> <code>entrypoint</code> The Docker entrypoint <code>Optional[str]</code> <code>None</code> <code>home_dir</code> The absolute path to the home directory inside the container <code>str</code> <code>/root</code> <code>registry_auth</code> Credentials for pulling a private Docker image <code>Optional[RegistryAuth]</code> <code>None</code> <code>python</code> The major version of PythonMutually exclusive with the image <code>Optional[PythonVersion]</code> <code>None</code> <code>env</code> The mapping or the list of environment variables <code>Union[List[ConstrainedStrValue],Dict[str, Union[str,EnvSentinel]]]</code> <code>{}</code> <code>setup</code> The bash commands to run on the boot <code>List[str]</code> <code>[]</code> <code>resources</code> The resources requirements to run the configuration <code>ResourcesSpec</code> <code>cpu=Range[int](min=2, max=None) memory=Range[Memory](min=8GB, max=None) shm_size=None gpu=None disk=None</code> <code>commands</code> The bash commands to run <code>List[str]</code> required <code>port</code> The port, that application listens to or the mapping <code>Union[ConstrainedIntValue,ConstrainedStrValue,PortMapping]</code> required <code>model</code> Mapping of the model for the OpenAI-compatible endpoint <code>Optional[Union[TGIChatModel,OpenAIChatModel]]</code> <code>None</code> <code>auth</code> Enable the authorization <code>bool</code> <code>True</code> <code>replicas</code> The range <code>Range[int]</code> <code>1</code> <code>scaling</code> The auto-scaling configuration <code>Optional[ScalingSpec]</code> <code>None</code>"},{"location":"docs/reference/profiles.yml/","title":"profiles.yml","text":"<p>Instead of configuring run options through<code>dstack run</code>,  you can do so via <code>.dstack/profiles.yml</code> in the root folder of the project. </p>"},{"location":"docs/reference/profiles.yml/#example","title":"Example","text":"<pre><code>profiles:\n  - name: large\n\n    spot_policy: auto # (Optional) The spot policy. Supports `spot`, `on-demand, and `auto`.\n\n    max_price: 1.5 # (Optional) The maximum price per instance per hour\n\n    max_duration: 1d # (Optional) The maximum duration of the run.\n\n    retry:\n      retry-limit: 3h # (Optional) To wait for capacity\n\n    backends: [azure, lambda]  # (Optional) Use only listed backends \n\n    default: true # (Optional) Activate the profile by default\n</code></pre> <p>You can mark any profile as default or pass its name via <code>--profile</code> to <code>dstack run</code>.</p>"},{"location":"docs/reference/profiles.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/profiles.yml/#profile","title":"Profile","text":"Property Description Type Default value <code>name</code> The name of the profile that can be passed as <code>--profile</code> to <code>dstack run</code> <code>str</code> required <code>backends</code> The backends to consider for provisionig (e.g., \"[aws, gcp]\") <code>Optional[List[BackendType]]</code> <code>None</code> <code>regions</code> The regions to consider for provisionig (e.g., \"[eu-west-1, us-west4, westeurope]\") <code>Optional[List[str]]</code> <code>None</code> <code>instance_types</code> The cloud-specific instance types to consider for provisionig (e.g., \"[p3.8xlarge, n1-standard-4]\") <code>Optional[List[str]]</code> <code>None</code> <code>spot_policy</code> The policy for provisioning spot or on-demand instances: spot, on-demand, or auto <code>Optional[SpotPolicy]</code> <code>None</code> <code>retry_policy</code> The policy for re-submitting the run <code>Optional[ProfileRetryPolicy]</code> <code>None</code> <code>max_duration</code> The maximum duration of a run (e.g., 2h, 1d, etc). After it elapses, the run is forced to stop. <code>Union[Literal['off'],str,int,NoneType]</code> <code>None</code> <code>max_price</code> The maximum price per hour, in dollars <code>Optional[float]</code> <code>None</code> <code>default</code> If set to true, <code>dstack run</code> will use this profile by default. <code>bool</code> <code>False</code> <code>pool_name</code> The name of the pool. If not set, dstack will use the default name. <code>Optional[str]</code> <code>None</code> <code>instance_name</code> The name of the instance <code>Optional[str]</code> <code>None</code> <code>creation_policy</code> The policy for using instances from the pool <code>Optional[CreationPolicy]</code> <code>CreationPolicy.REUSE_OR_CREATE</code> <code>termination_policy</code> The policy for termination instances <code>Optional[TerminationPolicy]</code> <code>TerminationPolicy.DESTROY_AFTER_IDLE</code> <code>termination_idle_time</code> Time to wait before destroying the idle instance <code>Union[int,str,NoneType]</code> <code>300</code>"},{"location":"docs/reference/profiles.yml/#profileretrypolicy","title":"ProfileRetryPolicy","text":"Property Description Type Default value <code>retry</code> Whether to retry the run on failure or not <code>bool</code> <code>False</code> <code>limit</code> The maximum period of retrying the run, e.g., 4h or 1d <code>Union[int,str,NoneType]</code> <code>None</code>"},{"location":"docs/reference/api/python/","title":"Python API","text":"<p>The Python API enables running tasks, services, and managing runs programmatically.</p>"},{"location":"docs/reference/api/python/#usage-example","title":"Usage example","text":"<p>Below is a quick example of submitting a task for running and displaying its logs.</p> <pre><code>import sys\n\nfrom dstack.api import Task, GPU, Client, Resources\n\nclient = Client.from_config()\n\ntask = Task(\n    image=\"ghcr.io/huggingface/text-generation-inference:latest\",\n    env={\"MODEL_ID\": \"TheBloke/Llama-2-13B-chat-GPTQ\"},\n    commands=[\n        \"text-generation-launcher --trust-remote-code --quantize gptq\",\n    ],\n    ports=[\"80\"],\n    resources=Resources(gpu=GPU(memory=\"24GB\")),\n)\n\nrun = client.runs.submit(\n    run_name=\"my-awesome-run\",  # If not specified, a random name is assigned \n    configuration=task,\n    repo=None, # Specify to mount additional files\n)\n\nrun.attach()\n\ntry:\n    for log in run.logs():\n        sys.stdout.buffer.write(log)\n        sys.stdout.buffer.flush()\nexcept KeyboardInterrupt:\n    run.stop(abort=True)\nfinally:\n    run.detach()\n</code></pre> <p>NOTE:</p> <ol> <li>The <code>configuration</code> argument in the <code>submit</code> method can be either <code>dstack.api.Task</code> or <code>dstack.api.Service</code>. </li> <li>If you create <code>dstack.api.Task</code> or <code>dstack.api.Service</code>, you may specify the <code>image</code> argument. If <code>image</code> isn't    specified, the default image will be used. For a private Docker registry, ensure you also pass the <code>registry_auth</code> argument.</li> <li>The <code>repo</code> argument in the <code>submit</code> method allows the mounting of a local folder, a remote repo, or a    programmatically created repo. In this case, the <code>commands</code> argument can refer to the files within this repo.</li> <li>The <code>attach</code> method waits for the run to start and, for <code>dstack.api.Task</code> sets up an SSH tunnel and forwards configured <code>ports</code> to <code>localhost</code>.</li> </ol>"},{"location":"docs/reference/api/python/#dstack.api","title":"<code>dstack.api</code>","text":""},{"location":"docs/reference/api/python/#dstack.api.Client","title":"<code>dstack.api.Client</code>","text":"<p>High-level API client for interacting with dstack server</p> <p>Attributes:</p> Name Type Description <code>runs</code> <code>RunCollection</code> <p>Operations with runs.</p> <code>repos</code> <code>RepoCollection</code> <p>Operations with repositories.</p> <code>backends</code> <code>BackendCollection</code> <p>Operations with backends.</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.from_config","title":"<code>from_config(project_name=None, server_url=None, user_token=None, ssh_identity_file=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Client using the default configuration from <code>~/.dstack/config.yml</code> if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project, required if <code>server_url</code> and <code>user_token</code> are specified</p> <code>None</code> <code>server_url</code> <code>Optional[str]</code> <p>The dstack server URL (e.g. <code>http://localhost:3000/</code> or <code>https://sky.dstack.ai</code>)</p> <code>None</code> <code>user_token</code> <code>Optional[str]</code> <p>The dstack user token</p> <code>None</code> <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for SSH tunneling</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>A client instance</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.runs","title":"<code>dstack.api.RunCollection</code>","text":"<p>Operations with runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.get","title":"<code>get(run_name)</code>","text":"<p>Get run by run name</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>run name</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The run or <code>None</code> if not found</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.list","title":"<code>list(all=False)</code>","text":"<p>List runs</p> <p>Parameters:</p> Name Type Description Default <code>all</code> <code>bool</code> <p>show all runs (active and finished) if <code>True</code></p> <code>False</code> <p>Returns:</p> Type Description <code>List[Run]</code> <p>list of runs</p>"},{"location":"docs/reference/api/python/#dstack.api.RunCollection.submit","title":"<code>submit(configuration, configuration_path=None, repo=None, backends=None, regions=None, instance_types=None, resources=None, spot_policy=None, retry_policy=None, max_duration=None, max_price=None, working_dir=None, run_name=None, reserve_ports=True)</code>","text":"<p>Submit a run</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>Union[Task, Service]</code> <p>A run configuration.</p> required <code>configuration_path</code> <code>Optional[str]</code> <p>The path to the configuration file, relative to the root directory of the repo.</p> <code>None</code> <code>repo</code> <code>Union[LocalRepo, RemoteRepo, VirtualRepo]</code> <p>A repo to mount to the run.</p> <code>None</code> <code>backends</code> <code>Optional[List[BackendType]]</code> <p>A list of allowed backend for provisioning.</p> <code>None</code> <code>regions</code> <code>Optional[List[str]]</code> <p>A list of cloud regions for provisioning.</p> <code>None</code> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration. Overrides the configuration's resources.</p> <code>None</code> <code>spot_policy</code> <code>Optional[SpotPolicy]</code> <p>A spot policy for provisioning.</p> <code>None</code> <code>retry_policy</code> <code>RetryPolicy</code> <p>A retry policy.</p> <code>None</code> <code>max_duration</code> <code>Optional[Union[int, str]]</code> <p>The max instance running duration in seconds.</p> <code>None</code> <code>max_price</code> <code>Optional[float]</code> <p>The max instance price in dollars per hour for provisioning.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>A working directory relative to the repo root directory</p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>A desired name of the run. Must be unique in the project. If not specified, a random name is assigned.</p> <code>None</code> <code>reserve_ports</code> <code>bool</code> <p>Whether local ports should be reserved in advance.</p> <code>True</code> <p>Returns:</p> Type Description <code>Run</code> <p>submitted run</p>"},{"location":"docs/reference/api/python/#dstack.api.Client.repos","title":"<code>dstack.api.RepoCollection</code>","text":"<p>Operations with repos</p>"},{"location":"docs/reference/api/python/#dstack.api.RepoCollection.init","title":"<code>init(repo, git_identity_file=None, oauth_token=None)</code>","text":"<p>Initializes the repo and configures its credentials in the project. Must be invoked before mounting the repo to a run.</p> <p>Example:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\",\n)\nclient.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Once the repo is initialized, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The repo to initialize.</p> required <code>git_identity_file</code> <code>Optional[PathLike]</code> <p>The private SSH key path for accessing the remote repo.</p> <code>None</code> <code>oauth_token</code> <code>Optional[str]</code> <p>The GitHub OAuth token to access the remote repo.</p> <code>None</code>"},{"location":"docs/reference/api/python/#dstack.api.Task","title":"<code>dstack.api.Task</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>ports</code> <code>List[PortMapping]</code> <p>Port numbers/mapping to expose</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration.</p>"},{"location":"docs/reference/api/python/#dstack.api.Service","title":"<code>dstack.api.Service</code>","text":"<p>Attributes:</p> Name Type Description <code>commands</code> <code>List[str]</code> <p>The bash commands to run</p> <code>port</code> <code>PortMapping</code> <p>The port, that application listens to or the mapping</p> <code>env</code> <code>Dict[str, str]</code> <p>The mapping or the list of environment variables</p> <code>image</code> <code>Optional[str]</code> <p>The name of the Docker image to run</p> <code>python</code> <code>Optional[str]</code> <p>The major version of Python</p> <code>entrypoint</code> <code>Optional[str]</code> <p>The Docker entrypoint</p> <code>registry_auth</code> <code>Optional[RegistryAuth]</code> <p>Credentials for pulling a private Docker image</p> <code>home_dir</code> <code>str</code> <p>The absolute path to the home directory inside the container. Defaults to <code>/root</code>.</p> <code>resources</code> <code>Optional[ResourcesSpec]</code> <p>The requirements to run the configuration.</p> <code>model</code> <code>Optional[ModelMapping]</code> <p>Mapping of the model for the OpenAI-compatible endpoint.</p> <code>auth</code> <code>bool</code> <p>Enable the authorization. Defaults to <code>True</code>.</p> <code>replicas</code> <code>Range[int]</code> <p>The range of the number of replicas. Defaults to <code>1</code>.</p> <code>scaling</code> <code>Annotated[Optional[ScalingSpec], Field(description='The auto-scaling configuration')]</code> <p>Optional[ScalingSpec]: The auto-scaling configuration.</p>"},{"location":"docs/reference/api/python/##dstack.api.Run","title":"<code>dstack.api.Run</code>","text":"<p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>run name</p> <code>ports</code> <code>Optional[Dict[int, int]]</code> <p>ports mapping, if run is attached</p> <code>backend</code> <code>Optional[BackendType]</code> <p>backend type</p> <code>status</code> <code>RunStatus</code> <p>run status</p> <code>hostname</code> <code>str</code> <p>instance hostname</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.attach","title":"<code>attach(ssh_identity_file=None)</code>","text":"<p>Establish an SSH tunnel to the instance and update SSH config</p> <p>Parameters:</p> Name Type Description Default <code>ssh_identity_file</code> <code>Optional[PathLike]</code> <p>SSH keypair to access instances</p> <code>None</code> <p>Raises:</p> Type Description <code>PortUsedError</code> <p>If ports are in use or the run is attached by another process.</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.detach","title":"<code>detach()</code>","text":"<p>Stop the SSH tunnel to the instance and update SSH config</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.logs","title":"<code>logs(start_time=None, diagnose=False)</code>","text":"<p>Iterate through run's log messages</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> <p>minimal log timestamp</p> <code>None</code> <code>diagnose</code> <code>bool</code> <p>return runner logs if <code>True</code></p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[bytes]</code> <p>log messages</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.refresh","title":"<code>refresh()</code>","text":"<p>Get up-to-date run info</p>"},{"location":"docs/reference/api/python/#dstack.api.Run.stop","title":"<code>stop(abort=False)</code>","text":"<p>Terminate the instance and detach</p> <p>Parameters:</p> Name Type Description Default <code>abort</code> <code>bool</code> <p>gracefully stop the run if <code>False</code></p> <code>False</code>"},{"location":"docs/reference/api/python/##dstack.api.Resources","title":"<code>dstack.api.Resources</code>","text":"<p>Creates required resources specification.</p> <p>Parameters:</p> Name Type Description Default <code>cpu</code> <code>Optional[Range[int]]</code> <p>The number of CPUs</p> <code>DEFAULT_CPU_COUNT</code> <code>memory</code> <code>Optional[Range[Memory]]</code> <p>The size of RAM memory (e.g., <code>\"16GB\"</code>)</p> <code>DEFAULT_MEMORY_SIZE</code> <code>gpu</code> <code>Optional[GPUSpec]</code> <p>The GPU spec</p> <code>None</code> <code>shm_size</code> <code>Optional[Range[Memory]]</code> <p>The of shared memory (e.g., <code>\"8GB\"</code>). If you are using parallel communicating processes (e.g., dataloaders in PyTorch), you may need to configure this.</p> <code>None</code> <code>disk</code> <code>Optional[DiskSpec]</code> <p>The disk spec</p> <code>None</code> <p>Returns:</p> Type Description <code>ResourcesSpec</code> <p>resources specification</p>"},{"location":"docs/reference/api/python/##dstack.api.GPU","title":"<code>dstack.api.GPU</code>","text":"<p>Creates GPU specification.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[List[str]]</code> <p>The name of the GPU (e.g., <code>\"A100\"</code> or <code>\"H100\"</code>)</p> <code>None</code> <code>count</code> <code>Optional[Range[int]]</code> <p>The number of GPUs</p> <code>DEFAULT_GPU_COUNT</code> <code>memory</code> <code>Optional[Range[Memory]]</code> <p>The size of a single GPU memory (e.g., <code>\"16GB\"</code>)</p> <code>None</code> <code>total_memory</code> <code>Optional[Range[Memory]]</code> <p>The total size of all GPUs memory (e.g., <code>\"32GB\"</code>)</p> <code>None</code> <code>compute_capability</code> <code>Optional[float]</code> <p>The minimum compute capability of the GPU (e.g., <code>7.5</code>)</p> <code>None</code> <p>Returns:</p> Type Description <code>GPUSpec</code> <p>GPU specification</p>"},{"location":"docs/reference/api/python/##dstack.api.Disk","title":"<code>dstack.api.Disk</code>","text":"<p>Creates disk specification.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Range[Memory]</code> <p>The size of the disk (e.g., <code>\"100GB\"</code>)</p> required <p>Returns:</p> Type Description <code>DiskSpec</code> <p>disk specification</p>"},{"location":"docs/reference/api/python/##dstack.api.LocalRepo","title":"<code>dstack.api.LocalRepo</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Example:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=LocalRepo.from_dir(\".\"), # Mount the current folder to the run\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.LocalRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a local repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>LocalRepo</code> <p>A local repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.RemoteRepo","title":"<code>dstack.api.RemoteRepo</code>","text":"<p>Creates an instance of a remote Git repo for mounting to a submitted run.</p> <p>Using a locally checked-out remote Git repo:</p> <pre><code>repo=RemoteRepo.from_dir(repo_dir=\".\")\n</code></pre> <p>Using a remote Git repo by a URL:</p> <pre><code>repo=RemoteRepo.from_url(\n    repo_url=\"https://github.com/dstackai/dstack-examples\",\n    repo_branch=\"main\"\n)\n</code></pre> <p>Initialize the repo before mounting it.</p> <pre><code>client.repos.init(repo)\n</code></pre> <p>By default, it uses the default Git credentials configured on the machine. You can override these credentials via the <code>git_identity_file</code> or <code>oauth_token</code> arguments of the <code>init</code> method.</p> <p>Finally, you can pass the repo object to the run:</p> <pre><code>run = client.runs.submit(\n    configuration=...,\n    repo=repo,\n)\n</code></pre>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_dir","title":"<code>from_dir(repo_dir)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a local path.</p> <p>Parameters:</p> Name Type Description Default <code>repo_dir</code> <code>PathLike</code> <p>The path to a local folder</p> required <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/#dstack.api.RemoteRepo.from_url","title":"<code>from_url(repo_url, repo_branch=None, repo_hash=None)</code>  <code>staticmethod</code>","text":"<p>Creates an instance of a remote repo from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>The URL of a remote Git repo</p> required <code>repo_branch</code> <code>Optional[str]</code> <p>The name of the remote branch. Must be specified if <code>hash</code> is not specified.</p> <code>None</code> <code>repo_hash</code> <code>Optional[str]</code> <p>The hash of the revision. Must be specified if <code>branch</code> is not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>RemoteRepo</code> <p>A remote repo instance</p>"},{"location":"docs/reference/api/python/##dstack.api.VirtualRepo","title":"<code>dstack.api.VirtualRepo</code>","text":"<p>Allows mounting a repo created programmatically.</p> <p>Example:</p> <pre><code>virtual_repo = VirtualRepo(repo_id=\"some-unique-repo-id\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"requirements.txt\")\nvirtual_repo.add_file_from_package(package=some_package, path=\"train.py\")\n\nrun = client.runs.submit(\n    configuration=...,\n    repo=virtual_repo,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>repo_id</code> <p>A unique identifier of the repo</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file","title":"<code>add_file(path, content)</code>","text":"<p>Adds a given file to the repo.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path inside the repo to add the file.</p> <code>content</code> <code>bytes</code> <p>The contents of the file.</p>"},{"location":"docs/reference/api/python/#dstack.api.VirtualRepo.add_file_from_package","title":"<code>add_file_from_package(package, path)</code>","text":"<p>Includes a file from a given package to the repo.</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>Union[ModuleType, str]</code> <p>A package to include the file from.</p> <code>path</code> <code>str</code> <p>The path to the file to include to the repo. Must be relative to the package directory.</p>"},{"location":"docs/reference/api/python/##dstack.api.RegistryAuth","title":"<code>dstack.api.RegistryAuth</code>","text":"<p>Credentials for pulling a private Docker image.</p> <p>Attributes:</p> Name Type Description <code>username</code> <code>str</code> <p>The username</p> <code>password</code> <code>str</code> <p>The password or access token</p>"},{"location":"docs/reference/api/python/#dstack.api.BackendType","title":"<code>dstack.api.BackendType</code>","text":"<p>Attributes:</p> Name Type Description <code>AWS</code> <code>BackendType</code> <p>Amazon Web Services</p> <code>AZURE</code> <code>BackendType</code> <p>Microsoft Azure</p> <code>CUDO</code> <code>BackendType</code> <p>Cudo</p> <code>DSTACK</code> <code>BackendType</code> <p>dstack Sky</p> <code>GCP</code> <code>BackendType</code> <p>Google Cloud Platform</p> <code>DATACRUNCH</code> <code>BackendType</code> <p>DataCrunch</p> <code>KUBERNETES</code> <code>BackendType</code> <p>Kubernetes</p> <code>LAMBDA</code> <code>BackendType</code> <p>Lambda Cloud</p> <code>TENSORDOCK</code> <code>BackendType</code> <p>TensorDock Marketplace</p> <code>VASTAI</code> <code>BackendType</code> <p>Vast.ai Marketplace</p>"},{"location":"docs/reference/api/rest/","title":"REST API","text":""},{"location":"docs/reference/cli/","title":"CLI","text":""},{"location":"docs/reference/cli/#commands","title":"Commands","text":""},{"location":"docs/reference/cli/#dstack-server","title":"dstack server","text":"<p>This command starts the <code>dstack</code> server.</p> <pre><code>$ dstack server --help\nUsage: dstack server [-h] [--host HOST] [-p PORT] [-l LOG_LEVEL] [--default]\n                     [--no-default] [--token TOKEN]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --host HOST           Bind socket to this host. Defaults to 127.0.0.1\n  -p, --port PORT       Bind socket to this port. Defaults to 3000.\n  -l, --log-level LOG_LEVEL\n                        Server logging level. Defaults to WARNING.\n  --default             Update the default project configuration\n  --no-default          Do not update the default project configuration\n  --token TOKEN         The admin user token\n</code></pre>"},{"location":"docs/reference/cli/#dstack-init","title":"dstack init","text":"<p>This command initializes the current folder as a repo.</p> <pre><code>$ dstack init --help\nUsage: dstack init [-h] [--project PROJECT] [-t OAUTH_TOKEN]\n                   [--git-identity SSH_PRIVATE_KEY]\n                   [--ssh-identity SSH_PRIVATE_KEY] [--local]\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project PROJECT     The name of the project\n  -t, --token OAUTH_TOKEN\n                        An authentication token for Git\n  --git-identity SSH_PRIVATE_KEY\n                        The private SSH key path to access the remote repo\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n  --local               Do not use git\n</code></pre> Git credentials <p>If the current folder is a Git repo, the command authorizes <code>dstack</code> to access it. By default, the command uses the default Git credentials configured for the repo.  You can override these credentials via <code>--token</code> (OAuth token) or <code>--git-identity</code>.</p> Custom SSH key <p>By default, this command generates an SSH key that will be used for port forwarding and SSH access to running workloads.  You can override this key via <code>--ssh-identity</code>.</p>"},{"location":"docs/reference/cli/#dstack-run","title":"dstack run","text":"<p>This command runs a given configuration.</p> <pre><code>$ dstack run . --help\nUsage: dstack run [--project NAME] [-h [TYPE]] [-f FILE] [-n RUN_NAME] [-d]\n                  [-y] [--max-offers MAX_OFFERS] [--profile NAME]\n                  [--max-price PRICE] [--max-duration DURATION] [-b NAME]\n                  [-r NAME] [--instance-type NAME]\n                  [--pool POOL_NAME | --reuse | --dont-destroy | --idle-duration IDLE_DURATION | --instance NAME]\n                  [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                  [--retry | --no-retry | --retry-duration DURATION]\n                  [-e KEY=VALUE] [--gpu SPEC] [--disk RANGE]\n                  working_dir\n\nPositional Arguments:\n  working_dir\n\nOptions:\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -h, --help [TYPE]     Show this help message and exit. TYPE is one of task,\n                        dev-environment, service\n  -f, --file FILE       The path to the run configuration file. Defaults to\n                        WORKING_DIR/.dstack.yml\n  -n, --name RUN_NAME   The name of the run. If not specified, a random name\n                        is assigned\n  -d, --detach          Do not poll logs and run status\n  -y, --yes             Do not ask for plan confirmation\n  --max-offers MAX_OFFERS\n                        Number of offers to show in the run plan\n  -e, --env KEY=VALUE   Environment variables\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk RANGE          Request the size range of disk for the run. Example\n                        --disk 100GB...\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  --max-duration DURATION\n                        The maximum duration of the run\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nPools:\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n  --instance NAME       Reuse instance from pool with name NAME\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n</code></pre> .gitignore <p>When running anything via CLI, <code>dstack</code> uses the exact version of code from your project directory.</p> <p>If there are large files, consider creating a <code>.gitignore</code> file to exclude them for better performance.</p>"},{"location":"docs/reference/cli/#dstack-ps","title":"dstack ps","text":"<p>This command shows the status of runs.</p> <pre><code>$ dstack ps --help\nUsage: dstack ps [-h] [--project NAME] [-a] [-v] [-w]\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -a, --all       Show all runs. By default, it only shows unfinished runs or\n                  the last finished.\n  -v, --verbose   Show more information about runs\n  -w, --watch     Watch statuses of runs in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-stop","title":"dstack stop","text":"<p>This command stops run(s) within the current repository.</p> <pre><code>$ dstack stop --help\nUsage: dstack stop [-h] [--project NAME] [-x] [-y] run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help      Show this help message and exit\n  --project NAME  The name of the project. Defaults to $DSTACK_PROJECT\n  -x, --abort\n  -y, --yes\n</code></pre>"},{"location":"docs/reference/cli/#dstack-logs","title":"dstack logs","text":"<p>This command shows the output of a given run within the current repository.</p> <pre><code>$ dstack logs --help\nUsage: dstack logs [-h] [--project NAME] [-d] [-a]\n                   [--ssh-identity SSH_PRIVATE_KEY]\n                   run_name\n\nPositional Arguments:\n  run_name\n\nOptions:\n  -h, --help            Show this help message and exit\n  --project NAME        The name of the project. Defaults to $DSTACK_PROJECT\n  -d, --diagnose\n  -a, --attach          Set up an SSH tunnel, and print logs as they follow.\n  --ssh-identity SSH_PRIVATE_KEY\n                        The private SSH key path for SSH tunneling\n</code></pre>"},{"location":"docs/reference/cli/#dstack-config","title":"dstack config","text":"<p>Both the CLI and API need to be configured with the server address, user token, and project name via <code>~/.dstack/config.yml</code>.</p> <p>At startup, the server automatically configures CLI and API with the server address, user token, and the default project name (<code>main</code>). This configuration is stored via <code>~/.dstack/config.yml</code>.</p> <p>To use CLI and API on different machines or projects, use the <code>dstack config</code> command.</p> <pre><code>$ dstack config --help\nUsage: dstack config [-h] [--project PROJECT] [--url URL] [--token TOKEN]\n                     [--default] [--remove] [--no-default]\n\nOptions:\n  -h, --help         Show this help message and exit\n  --project PROJECT  The name of the project to configure\n  --url URL          Server url\n  --token TOKEN      User token\n  --default          Set the project as default. It will be used when\n                     --project is omitted in commands.\n  --remove           Delete project configuration\n  --no-default       Do not prompt to set the project as default\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool","title":"dstack pool","text":"<p>Pools allow for managing the lifecycle of instances and reusing them across runs.  The default pool is created automatically.</p>"},{"location":"docs/reference/cli/#dstack-pool-add","title":"dstack pool add","text":"<p>The <code>dstack pool add</code> command adds an instance to a pool. If no pool name is specified, the instance goes to the default pool.</p> <pre><code>$ dstack pool add --help\nUsage: dstack pool add [-h] [-y] [--remote] [--remote-host REMOTE_HOST]\n                       [--remote-port REMOTE_PORT] [--name INSTANCE_NAME]\n                       [--profile NAME] [--max-price PRICE] [-b NAME]\n                       [-r NAME] [--instance-type NAME] [--pool POOL_NAME]\n                       [--reuse] [--dont-destroy]\n                       [--idle-duration IDLE_DURATION]\n                       [--spot | --on-demand | --spot-auto | --spot-policy POLICY]\n                       [--retry | --no-retry | --retry-duration DURATION]\n                       [--cpu SPEC] [--memory SIZE] [--shared-memory SIZE]\n                       [--gpu SPEC] [--disk SIZE]\n\nOptions:\n  -h, --help            show this help message and exit\n  -y, --yes             Don't ask for confirmation\n  --remote              Add remote runner as an instance\n  --remote-host REMOTE_HOST\n                        Remote runner host\n  --remote-port REMOTE_PORT\n                        Remote runner port\n  --name INSTANCE_NAME  Set the name of the instance\n  --pool POOL_NAME      The name of the pool. If not set, the default pool\n                        will be used\n  --reuse               Reuse instance from pool\n  --dont-destroy        Do not destroy instance after the run is finished\n  --idle-duration IDLE_DURATION\n                        Time to wait before destroying the idle instance\n\nProfile:\n  --profile NAME        The name of the profile. Defaults to $DSTACK_PROFILE\n  --max-price PRICE     The maximum price per hour, in dollars\n  -b, --backend NAME    The backends that will be tried for provisioning\n  -r, --region NAME     The regions that will be tried for provisioning\n  --instance-type NAME  The cloud-specific instance types that will be tried\n                        for provisioning\n\nSpot Policy:\n  --spot                Consider only spot instances\n  --on-demand           Consider only on-demand instances\n  --spot-auto           Consider both spot and on-demand instances\n  --spot-policy POLICY  One of spot, on-demand, auto\n\nRetry Policy:\n  --retry\n  --no-retry\n  --retry-duration DURATION\n\nResources:\n  --cpu SPEC            Request the CPU count. Default: 2..\n  --memory SIZE         Request the size of RAM. The format is SIZE:MB|GB|TB.\n                        Default: 8GB..\n  --shared-memory SIZE  Request the size of Shared Memory. The format is\n                        SIZE:MB|GB|TB.\n  --gpu SPEC            Request GPU for the run. The format is\n                        NAME:COUNT:MEMORY (all parts are optional)\n  --disk SIZE           Request the size of disk for the run. Example --disk\n                        100GB...\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-ps","title":"dstack pool ps","text":"<p>The <code>dstack pool ps</code> command lists all active instances of a pool. If no pool name is specified, default pool instances are displayed.</p> <pre><code>$ dstack pool ps --help\nUsage: dstack pool ps [-h] [--pool POOL_NAME] [-w]\n\nShow instances in the pool\n\nOptions:\n  -h, --help        show this help message and exit\n  --pool POOL_NAME  The name of the pool. If not set, the default pool will be\n                    used\n  -w, --watch       Watch instances in realtime\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-create","title":"dstack pool create","text":"<p>The <code>dstack pool create</code> command creates a new pool.</p> <pre><code>$ dstack pool create --help\nUsage: dstack pool create [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-list","title":"dstack pool list","text":"<p>The <code>dstack pool list</code> lists all existing pools.</p> <pre><code>$ dstack pool delete --help\nUsage: dstack pool delete [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-pool-delete","title":"dstack pool delete","text":"<p>The <code>dstack pool delete</code> command deletes a specified pool.</p> <pre><code>$ dstack pool delete --help\nUsage: dstack pool delete [-h] -n POOL_NAME\n\nOptions:\n  -h, --help            show this help message and exit\n  -n, --name POOL_NAME  The name of the pool\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway","title":"dstack gateway","text":"<p>A gateway is required for running services. It handles ingress traffic, authentication, domain mapping, model mapping for the OpenAI-compatible endpoint, and so on.</p>"},{"location":"docs/reference/cli/#dstack-gateway-list","title":"dstack gateway list","text":"<p>The <code>dstack gateway list</code> command displays the names and addresses of the gateways configured in the project.</p> <pre><code>$ dstack gateway list --help\nUsage: dstack gateway list [-h] [-v]\n\nOptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Show more information\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-create","title":"dstack gateway create","text":"<p>The <code>dstack gateway create</code> command creates a new gateway instance in the project.</p> <pre><code>$ dstack gateway create --help\nUsage: dstack gateway create [-h] --backend {aws,azure,gcp,kubernetes}\n                             --region REGION [--set-default] [--name NAME]\n                             --domain DOMAIN\n\nOptions:\n  -h, --help            show this help message and exit\n  --backend {aws,azure,gcp,kubernetes}\n  --region REGION\n  --set-default         Set as default gateway for the project\n  --name NAME           Set a custom name for the gateway\n  --domain DOMAIN       Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-delete","title":"dstack gateway delete","text":"<p>The <code>dstack gateway delete</code> command deletes the specified gateway.</p> <pre><code>$ dstack gateway delete --help\nUsage: dstack gateway delete [-h] [-y] name\n\nPositional Arguments:\n  name        The name of the gateway\n\nOptions:\n  -h, --help  show this help message and exit\n  -y, --yes   Don't ask for confirmation\n</code></pre>"},{"location":"docs/reference/cli/#dstack-gateway-update","title":"dstack gateway update","text":"<p>The <code>dstack gateway update</code> command updates the specified gateway.</p> <pre><code>$ dstack gateway update --help\nUsage: dstack gateway update [-h] [--set-default] [--domain DOMAIN] name\n\nPositional Arguments:\n  name             The name of the gateway\n\nOptions:\n  -h, --help       show this help message and exit\n  --set-default    Set it the default gateway for the project\n  --domain DOMAIN  Set the domain for the gateway\n</code></pre>"},{"location":"docs/reference/cli/#environment-variables","title":"Environment variables","text":"Name Description Default <code>DSTACK_CLI_LOG_LEVEL</code> Configures CLI logging level <code>CRITICAL</code> <code>DSTACK_PROFILE</code> Has the same effect as <code>--profile</code> <code>None</code> <code>DSTACK_PROJECT</code> Has the same effect as <code>--project</code> <code>None</code> <code>DSTACK_DEFAULT_CREDS_DISABLED</code> Disables default credentials detection if set <code>None</code> <code>DSTACK_LOCAL_BACKEND_ENABLED</code> Enables local backend for debug if set <code>None</code> <code>DSTACK_RUNNER_VERSION</code> Sets exact runner version for debug <code>latest</code> <code>DSTACK_SERVER_ADMIN_TOKEN</code> Has the same effect as <code>--token</code> <code>None</code> <code>DSTACK_SERVER_DIR</code> Sets path to store data and server configs <code>~/.dstack/server</code> <code>DSTACK_SERVER_HOST</code> Has the same effect as <code>--host</code> <code>127.0.0.1</code> <code>DSTACK_SERVER_LOG_LEVEL</code> Has the same effect as <code>--log-level</code> <code>WARNING</code> <code>DSTACK_SERVER_PORT</code> Has the same effect as <code>--port</code> <code>3000</code> <code>DSTACK_SERVER_ROOT_LOG_LEVEL</code> Sets root logger log level <code>ERROR</code> <code>DSTACK_SERVER_UVICORN_LOG_LEVEL</code> Sets uvicorn logger log level <code>ERROR</code>"},{"location":"docs/reference/server/config.yml/","title":"~/.dstack/server/config.yml","text":""},{"location":"docs/reference/server/config.yml/#yaml-reference","title":"YAML reference","text":""},{"location":"docs/reference/server/config.yml/#serverconfig","title":"ServerConfig","text":"Property Description Type Default value <code>projects</code> <code>List[ProjectConfig]</code> required"},{"location":"docs/reference/server/config.yml/#projectconfig","title":"ProjectConfig","text":"Property Description Type Default value <code>name</code> <code>str</code> required <code>backends</code> <code>Union[AWSConfigInfoWithCreds, AzureConfigInfoWithCreds, GCPConfigInfoWithCreds, LambdaConfigInfoWithCreds, TensorDockConfigInfoWithCreds, VastAIConfigInfoWithCreds, KubernetesConfig]</code> required"},{"location":"docs/reference/server/config.yml/#awsconfig","title":"AWSConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['aws']</code> <code>aws</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>vpc_name</code> <code>Optional[str]</code> <code>None</code> <code>creds</code> <code>Union[AWSAccessKeyCreds,AWSDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#awsdefaultcreds","title":"AWSDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#awsaccesskeycreds","title":"AWSAccessKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['access_key']</code> <code>access_key</code> <code>access_key</code> <code>str</code> required <code>secret_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#azureconfig","title":"AzureConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['azure']</code> <code>azure</code> <code>tenant_id</code> <code>str</code> required <code>subscription_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[AzureClientCreds,AzureDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#azuredefaultcreds","title":"AzureDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#azureclientcreds","title":"AzureClientCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['client']</code> <code>client</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required <code>tenant_id</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#datacrunchconfig","title":"DataCrunchConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['datacrunch']</code> <code>datacrunch</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>DataCrunchAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#datacrunchapikeycreds","title":"DataCrunchAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>client_id</code> <code>str</code> required <code>client_secret</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#gcpconfig","title":"GCPConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['gcp']</code> <code>gcp</code> <code>project_id</code> <code>str</code> required <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>Union[GCPServiceAccountCreds,GCPDefaultCreds]</code> required"},{"location":"docs/reference/server/config.yml/#gcpdefaultcreds","title":"GCPDefaultCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['default']</code> <code>default</code>"},{"location":"docs/reference/server/config.yml/#gcpserviceaccountcreds","title":"GCPServiceAccountCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['service_account']</code> <code>service_account</code> <code>filename</code> <code>str</code> required <code>data</code> <code>Optional[str]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#lambdaconfig","title":"LambdaConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['lambda']</code> <code>lambda</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>LambdaAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#lambdaapikeycreds","title":"LambdaAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#tensordockconfig","title":"TensorDockConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['tensordock']</code> <code>tensordock</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>TensorDockAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#tensordockapikeycreds","title":"TensorDockAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required <code>api_token</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#vastaiconfig","title":"VastAIConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['vastai']</code> <code>vastai</code> <code>regions</code> <code>Optional[List[str]]</code> <code>None</code> <code>creds</code> <code>VastAIAPIKeyCreds</code> required"},{"location":"docs/reference/server/config.yml/#vastaiapikeycreds","title":"VastAIAPIKeyCreds","text":"Property Description Type Default value <code>type</code> <code>Literal['api_key']</code> <code>api_key</code> <code>api_key</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#kubernetesconfig","title":"KubernetesConfig","text":"Property Description Type Default value <code>type</code> <code>Literal['kubernetes']</code> <code>kubernetes</code> <code>kubeconfig</code> <code>KubeconfigConfig</code> required <code>networking</code> <code>Optional[KubernetesNetworkingConfig]</code> <code>None</code>"},{"location":"docs/reference/server/config.yml/#kubeconfigconfig","title":"KubeconfigConfig","text":"Property Description Type Default value <code>filename</code> <code>str</code> required <code>data</code> <code>str</code> required"},{"location":"docs/reference/server/config.yml/#kubernetesnetworkingconfig","title":"KubernetesNetworkingConfig","text":"Property Description Type Default value <code>ssh_host</code> <code>Optional[str]</code> <code>None</code> <code>ssh_port</code> <code>Optional[int]</code> <code>None</code>"},{"location":"examples/infinity/","title":"Infinity","text":"<p>This example demonstrates how to use Infinity with <code>dstack</code>' s services to deploy any SentenceTransformers based embedding models.</p>"},{"location":"examples/infinity/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy a <code>SentenceTransformers</code> based embedding models using <code>Infinity</code>, you need to define the following configuration file at minimum:</p> <pre><code>type: service\n\nimage: michaelf34/infinity:latest\nenv:\n  - MODEL_ID=BAAI/bge-small-en-v1.5\ncommands:\n  - infinity_emb --model-name-or-path $MODEL_ID --port 80\nport: 80\n\nresources:\n  gpu: 16GB\n</code></pre>"},{"location":"examples/infinity/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f infinity/serve.dstack.yml\n</code></pre>"},{"location":"examples/infinity/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you can query it at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (using the domain set up for the gateway):</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p>"},{"location":"examples/infinity/#openai-interface","title":"OpenAI interface","text":"<p>Any embedding models served by Infinity automatically comes with OpenAI's Embeddings APIs compatible APIs,  so we can directly use <code>openai</code> package to interact with the deployed Infinity.</p> <pre><code>from openai import OpenAI\nfrom functools import partial\n\nclient = OpenAI(base_url=\"https://&lt;run name&gt;.&lt;gateway domain&gt;\", api_key=\"&lt;dstack token&gt;\")\n\nclient.embeddings.create = partial(\n  client.embeddings.create, model=\"bge-small-en-v1.5\"\n)\n\nprint(client.embeddings.create(input=[\"A sentence to encode.\"]))\n</code></pre>"},{"location":"examples/infinity/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/infinity/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Embeddings Inference, TGI, and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/llama-index/","title":"Llama Index","text":"<p>RAG, or retrieval-augmented generation, empowers LLMs by providing them with access to your data.</p> <p>Here's an example of how to apply this technique using the Llama Index framework  and Weaviate vector database.</p>"},{"location":"examples/llama-index/#how-does-it-work","title":"How does it work?","text":"<ol> <li>Llama Index loads data from local files, structures it into chunks, and ingests it into Weaviate (an open-source vector database).    We set up Llama Index to use local embeddings through the SentenceTransformers library.</li> <li><code>dstack</code> allows us to deploy LLMs to any cloud provider, e.g. via Services using TGI or vLLM.</li> <li>Llama Index allows us to prompt the LLM automatically incorporating the context from Weaviate. </li> </ol>"},{"location":"examples/llama-index/#requirements","title":"Requirements","text":"<p>Here's the list of Python libraries that we'll use:</p> <pre><code>weaviate-client\nllama-index\nsentence-transformers\ntext_generation\n</code></pre>"},{"location":"examples/llama-index/#load-data-to-weaviate","title":"Load data to Weaviate","text":"<p>The first thing we do is load the data from local files and ingest it into Weaviate.</p> <p>NOTE:</p> <p>To use Weaviate, you need to either install  it on-premises or sign up for their managed service.</p> <p>Since we're going to load data into or from Weaviate, we'll need a <code>weaviate.Client</code>:</p> <pre><code>import os\n\nimport weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_TOKEN\"))\n\nclient = weaviate.Client(url=os.getenv(\"WEAVIATE_URL\"), auth_client_secret=auth_config)\n\nclient.schema.delete_class(\"DstackExample\")\n</code></pre> <p>Next, prepare the Llama Index classes: <code>llama_index.ServiceContext</code> (for indexing and querying) and <code>llama_index.StorageContext</code> (for loading and storing). </p> <p>Embeddings</p> <p>Note that we're using <code>langchain.embeddings.huggingface.HuggingFaceEmbeddings</code> for local embeddings instead of OpenAI.</p> <pre><code>from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index import (\n    LangchainEmbedding,\n    ServiceContext,\n    StorageContext,\n)\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n</code></pre> <p>Once the utility classes are configured, we can load the data from local files and pass it to <code>llama_index.VectorStoreIndex</code>. Using its <code>from_documents</code> method will then store the data in the vector database.</p> <pre><code>from pathlib import Path\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\n\ndocuments = SimpleDirectoryReader(Path(__file__).parent / \"data\").load_data()\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    service_context=service_context,\n    storage_context=storage_context,\n)\n</code></pre> <p>The data is in the vector database! Now we can proceed with the part where we invoke an LLM using this data as context.</p>"},{"location":"examples/llama-index/#deploy-an-llm","title":"Deploy an LLM","text":"<p>This example assumes we're using an LLM deployed using TGI.</p> <p>Once you deployed the model, make sure to set the <code>TGI_ENDPOINT_URL</code> environment variable  to its URL, e.g. <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (or <code>http://localhost:&lt;port&gt;</code> if it's deployed  as a task). We'll use this environment variable below.</p> <pre><code>$ curl -X POST --location $TGI_ENDPOINT_URL/generate \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n          \"inputs\": \"What is Deep Learning?\",\n          \"parameters\": {\n            \"max_new_tokens\": 20\n          }\n        }'\n</code></pre>"},{"location":"examples/llama-index/#generate-response","title":"Generate response","text":"<p>Once the LLM endpoint is up, we can prompt it through Llama Index to automatically incorporate context from Weaviate.</p> <p>Since we'll invoke the actual LLM, when configuring <code>llama_index.ServiceContext</code>, we must include the LLM configuration.</p> <pre><code>import os\n\nfrom llama_index import (\n    LangchainEmbedding,\n    PromptHelper,\n    ServiceContext,\n    VectorStoreIndex,\n)\n\nfrom langchain import HuggingFaceTextGenInference\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nfrom llama_index.llm_predictor import LLMPredictor\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\nllm_predictor = LLMPredictor(\n    llm=HuggingFaceTextGenInference(\n        inference_server_url=os.getenv(\"TGI_ENDPOINT_URL\"),\n        max_new_tokens=512,\n        streaming=True,\n    ),\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model,\n    llm_predictor=llm_predictor,\n    prompt_helper=PromptHelper(context_window=1024),\n)\n\nvector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"DstackExample\")\n\nindex = VectorStoreIndex.from_vector_store(\n    vector_store, service_context=service_context\n)\n</code></pre> <p>Once <code>llama_index.VectorStoreIndex</code> is ready, we can proceed with querying it.</p> <p>Prompt format</p> <p>If we're deploying Llama 2, we have to ensure that the prompt format is correct.</p> <pre><code>from llama_index import (QuestionAnswerPrompt, RefinePrompt)\n\ntext_qa_template = QuestionAnswerPrompt(\n        \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nWe have provided context information below. \n\n{context_str}\n\nGiven this information, please answer the question.\n&lt;&lt;/SYS&gt;&gt;\n\n{query_str} [/INST]\"\"\"\n    )\n\nrefine_template = RefinePrompt(\n    \"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nThe original query is as follows: \n\n{query_str}\n\nWe have provided an existing answer:\n\n{existing_answer}\n\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\n\n{context_msg}\n&lt;&lt;/SYS&gt;&gt;\n\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. [/INST]\"\"\"\n)\n\nquery_engine = index.as_query_engine(\n    text_qa_template=text_qa_template,\n    refine_template=refine_template,\n    streaming=True,\n)\n\nresponse = query_engine.query(\"Make a bullet-point timeline of the authors biography?\")\nresponse.print_response_stream()\n</code></pre> <p>That's it! This basic example shows how straightforward it is to use Llama Index and Weaviate with the LLMs deployed using <code>dstack</code>. For more in-depth information, we encourage you to explore the documentation for each tool.</p>"},{"location":"examples/llama-index/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/llama-index/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/mixtral/","title":"Mixtral 8x7B","text":"<p>This example demonstrates how to deploy Mixtral with <code>dstack</code>'s services.</p>"},{"location":"examples/mixtral/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy Mixtral as a service, you have to define the corresponding configuration file. Below are multiple variants: via vLLM (<code>fp16</code>), TGI (<code>fp16</code>), or TGI (<code>int4</code>).</p> TGI <code>fp16</code>TGI <code>int4</code>vLLM <code>fp16</code> <p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mixtral-8x7B-Instruct-v0.1\ncommands:\n  - text-generation-launcher \n    --port 80\n    --trust-remote-code\n    --num-shard 2 # Should match the number of GPUs \nport: 80\n\nresources:\n  gpu: 80GB:2\n  disk: 200GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n  format: tgi\n</code></pre> <p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest \nenv:\n  - MODEL_ID=TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ \ncommands:\n  - text-generation-launcher\n    --port 80\n    --trust-remote-code\n    --quantize gptq\nport: 80\n\nresources:\n    gpu: 25GB..50GB \n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n  format: tgi\n</code></pre> <p> <pre><code>type: service\n\npython: \"3.11\"\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server\n    --model mistralai/Mixtral-8X7B-Instruct-v0.1\n    --host 0.0.0.0\n    --tensor-parallel-size 2 # Should match the number of GPUs\nport: 8000\n\nresources:\n  gpu: 80GB:2\n  disk: 200GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n  format: openai\n</code></pre> <p>NOTE:</p> <p>Support for quantized Mixtral in vLLM is not yet stable.</p>"},{"location":"examples/mixtral/#run-the-configuration","title":"Run the configuration","text":"<p>Prerequisites</p> <p>Before running a service, make sure to set up a gateway. However, it's not required when using dstack Sky, as it's set up automatically.</p> <pre><code>$ dstack run . -f llms/mixtral/tgi.dstack.yml\n</code></pre>"},{"location":"examples/mixtral/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you'll be able to access it at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p>"},{"location":"examples/mixtral/#openai-interface","title":"OpenAI interface","text":"<p>In case the service has the model mapping configured, you will also be able  to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"https://gateway.&lt;gateway domain&gt;\", api_key=\"&lt;dstack token&gt;\")\n\ncompletion = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n        }\n    ],\n    stream=True,\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content, end=\"\")\nprint()\n</code></pre> Hugging Face Hub token <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p>"},{"location":"examples/mixtral/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/mixtral/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/ollama/","title":"Ollama","text":"<p>This example demonstrates how to use Ollama with <code>dstack</code>'s services to deploy LLMs.</p>"},{"location":"examples/ollama/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using vLLM, you have to define the following configuration file:</p> <pre><code>type: service\n\nimage: ollama/ollama\ncommands:\n  - ollama serve &amp;\n  - sleep 3\n  - ollama pull mixtral\n  - fg\nport: 11434\n\nresources:\n  gpu: 48GB..80GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  type: chat\n  name: mixtral\n  format: openai\n</code></pre>"},{"location":"examples/ollama/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f deployment/ollama/serve.dstack.yml\n</code></pre>"},{"location":"examples/ollama/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you can query it at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (using the domain set up for the gateway):</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p>"},{"location":"examples/ollama/#openai-interface","title":"OpenAI interface","text":"<p>Because we've configured the model mapping, it will also be possible  to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://gateway.&lt;gateway domain&gt;\", \n    api_key=\"&lt;dstack token&gt;\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"mixtral\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n        }\n    ],\n    stream=True,\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content, end=\"\")\nprint()\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p>"},{"location":"examples/ollama/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p> <p>What's next?</p> <ol> <li>Check the vLLM and Text Generation Inference examples</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/qlora/","title":"QLoRA","text":"<p>This example demonstrates how to fine-tune <code>llama-2-7b-chat-hf</code>, with QLoRA and your own script, using Tasks.</p>"},{"location":"examples/qlora/#prepare-a-dataset","title":"Prepare a dataset","text":"<p>When selecting a dataset, make sure that it is pre-processed to match the prompt format of Llama 2:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\n</code></pre> <p>In our example, we'll use the <code>mlabonne/guanaco-llama2-1k</code> dataset. It is a 1K sample from the <code>timdettmers/openassistant-guanaco</code> dataset converted to Llama 2's format.</p>"},{"location":"examples/qlora/#define-the-training-script","title":"Define the training script","text":""},{"location":"examples/qlora/#requirements","title":"Requirements","text":"<p>The most notable libraries that we'll use are <code>peft</code> (required for using the QLoRA technique), <code>bitsandbytes</code> (required for using the quantization technique), and <code>trl</code> (required for supervised fine-tuning).</p> <pre><code>accelerate==0.21.0\npeft==0.4.0\nbitsandbytes==0.40.2\ntransformers==4.31.0\ntrl==0.4.7\nscipy\ntensorboard\nsentencepiece\nhf-transfer\n</code></pre>"},{"location":"examples/qlora/#load-the-base-model","title":"Load the base model","text":"<p>In the first part of our script, we prepare the <code>bitsandbytes</code> config and load the base model along with its tokenizer, based on the script arguments.</p> <pre><code>from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\ndef create_and_prepare_model(args):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=args.use_4bit,\n        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n        bnb_4bit_use_double_quant=args.use_nested_quant,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n\n    model.config.use_cache = False\n    model.config.pretraining_tp\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    return model, tokenizer\n</code></pre>"},{"location":"examples/qlora/#create-a-trainer-instance","title":"Create a trainer instance","text":"<p>In the second part of our script, we prepare the <code>peft</code> config and create the trainer based on the script arguments.</p> <pre><code>from peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ndef create_and_prepare_trainer(model, tokenizer, dataset, args):\n    training_arguments = TrainingArguments(\n        output_dir=args.output_dir,\n        num_train_epochs=args.num_train_epochs,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        optim=args.optim,\n        save_steps=args.save_steps,\n        logging_steps=args.logging_steps,\n        learning_rate=args.learning_rate,\n        weight_decay=args.weight_decay,\n        fp16=args.fp16,\n        bf16=args.bf16,\n        max_grad_norm=args.max_grad_norm,\n        max_steps=args.max_steps,\n        warmup_ratio=args.warmup_ratio,\n        group_by_length=args.group_by_length,\n        lr_scheduler_type=args.lr_scheduler_type,\n        report_to=\"tensorboard\",\n    )\n\n    peft_config = LoraConfig(\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        r=args.lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=args.max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=args.packing,\n    )\n\n    return trainer\n</code></pre>"},{"location":"examples/qlora/#publish-the-fine-tuned-model","title":"Publish the fine-tuned model","text":"<p>In the third part of the script, we merge the base model with the fine-tuned model and push it to the Hugging Face Hub.</p> <pre><code>from peft import PeftModel\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer\n)\n\ndef merge_and_push(args):\n    # Reload model in FP16 and merge it with LoRA weights\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, args.new_model_name)\n    model = model.merge_and_unload()\n\n    # Reload the new tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name, trust_remote_code=True\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    # Publish the new model to Hugging Face Hub\n    model.push_to_hub(args.new_model_name, use_temp_dir=False)\n    tokenizer.push_to_hub(args.new_model_name, use_temp_dir=False)\n</code></pre>"},{"location":"examples/qlora/#put-it-all-together","title":"Put it all together","text":"<p>Finally, in the main part of the script, we put it all together.</p> <pre><code>from dataclasses import dataclass\nfrom datasets import load_dataset\nfrom transformers import HfArgumentParser\n\n@dataclass\nclass ScriptArguments:\n    # ...\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n\n    dataset = load_dataset(args.dataset_name, split=\"train\")\n\n    model, tokenizer = create_and_prepare_model(args)\n\n    trainer = create_and_prepare_trainer(model, tokenizer, dataset, args)\n\n    trainer.train()\n    trainer.model.save_pretrained(args.new_model_name)\n\n    if args.merge_and_push:\n        merge_and_push(args)\n</code></pre>"},{"location":"examples/qlora/#define-the-configuration","title":"Define the configuration","text":"<p>Here's the configuration that runs the training task via <code>dstack</code>:</p> <pre><code>type: task\n\npython: \"3.11\"\nenv: \n  - HF_HUB_ENABLE_HF_TRANSFER=1\n  # (Required) Specify your Hugging Face token to publish the fine-tuned model\n  - HUGGING_FACE_HUB_TOKEN=\ncommands:\n  - pip install -r llama-2/requirements.txt\n  - tensorboard --logdir results/runs &amp;\n  - python llama-2/train.py --merge_and_push ${{ run.args }}\nports:\n  - 6006\n\nresources:\n  gpu: 16GB..24GB\n</code></pre>"},{"location":"examples/qlora/#run-the-configuration","title":"Run the configuration","text":"<p>Here's how you run it with <code>dstack</code>:</p> <pre><code>$ dstack run . -f finetuning/qlora/train.dstack.yml --num_train_epochs 10\n\nInstalling requirements...\nTensorBoard 2.14.0 at http://127.0.0.1:6006/ (Press CTRL+C to quit)\n{'loss': 1.3491, 'learning_rate': 0.0002, 'epoch': 0.1}\n{'loss': 1.6299, 'learning_rate': 0.0002, 'epoch': 0.2}\n{'loss': 1.2071, 'learning_rate': 0.0002, 'epoch': 0.3}\n</code></pre> <p><code>dstack</code> will provision the cloud instance corresponding to the configured project and profile, run the training, and tear down the cloud instance once the training is complete.</p> Tensorboard <p>Since we've executed <code>tensorboard</code> within our task and configured its port using <code>ports</code>, you can access it using the URL provided in the output. <code>dstack</code> automatically forwards the configured port to your local machine.</p> <p></p>"},{"location":"examples/qlora/#source-code","title":"Source code","text":"<p>The complete and ready-to-run code for the example is available in our GitHub repo.</p>"},{"location":"examples/qlora/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about tasks</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/sdxl/","title":"SDXL","text":"<p>Stable Diffusion XL (SDXL) 1.0 is the latest version of the open-source model that is capable  of generating high-quality images from text.</p> <p>The example below demonstrates how to use <code>dstack</code> to serve SDXL as a REST endpoint in a cloud of your choice for image generation and refinement.</p>"},{"location":"examples/sdxl/#define-endpoints","title":"Define endpoints","text":""},{"location":"examples/sdxl/#requirements","title":"Requirements","text":"<p>Here's the list of libraries that our example will require:</p> <pre><code>transformers\naccelerate\nsafetensors\ndiffusers\ninvisible-watermark&gt;=0.2.0\nopencv-python-headless\nfastapi\nuvicorn\n</code></pre> <p>Let's walk through the code of the example.</p>"},{"location":"examples/sdxl/#load-the-model","title":"Load the model","text":"<p>First of all, let's load the base SDXL model using the <code>diffusers</code> library.</p> <pre><code>from diffusers import StableDiffusionXLPipeline\nimport torch\n\n\nbase = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n)\nbase.to(\"cuda\")\n</code></pre>"},{"location":"examples/sdxl/#define-the-generate-endpoint","title":"Define the generate endpoint","text":"<p>Now that the model is loaded, let's define the FastAPI app and the <code>/generate</code> REST endpoint that will accept a prompt and generate an image.</p> <pre><code>import uuid\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = None\n    width: Optional[int] = None\n    height: Optional[str] = None\n\n\nclass ImageResponse(BaseModel):\n    id: str\n\n\nimages_dir = Path(\"images\")\nimages_dir.mkdir(exist_ok=True)\n\n\n@app.post(\"/generate\")\nasync def generate(request: GenerateRequest):\n    image = base(\n        prompt=request.prompt,\n        negative_prompt=request.negative_prompt,\n        width=request.width,\n        height=request.height,\n    ).images[0]\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre>"},{"location":"examples/sdxl/#define-the-download-endpoint","title":"Define the download endpoint","text":"<p>Notice that the endpoint only returns the ID of the image. To download images by ID, we'll define another endpoint:</p> <pre><code>from fastapi.responses import FileResponse\n\n\n@app.get(\"/download/{id}\")\ndef download(id: str):\n    filename = f\"{id}.png\"\n    return FileResponse(\n        images_dir / filename, media_type=\"image/png\", filename=filename\n    )\n</code></pre> <p>That's it. Once we run the application, we can already utilize the <code>/generate</code> and <code>/download</code> endpoints.</p>"},{"location":"examples/sdxl/#define-the-refine-endpoint","title":"Define the refine endpoint","text":"<p>Since SDXL allows refining images, let's define the refine endpoint to accept the image ID and the refinement prompt.</p> <pre><code>import asyncio\n\nimport PIL\n\n\nclass RefineRequest(BaseModel):\n    id: str\n    prompt: str\n\n\nrefiner = None\nrefiner_lock = asyncio.Lock()\n\n\n@app.post(\"/refine\")\nasync def refine(request: RefineRequest):\n    await refiner_lock.acquire()\n    global refiner\n    if refiner is None:\n        refiner = DiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n            text_encoder_2=base.text_encoder_2,\n            vae=base.vae,\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n            variant=\"fp16\",\n        )\n        refiner.to(\"cuda\")\n    refiner_lock.release()\n\n    image = refiner(\n        prompt=request.prompt,\n        image=PIL.Image.open(images_dir / f\"{request.id}.png\"),\n    ).images[0]\n\n    id = str(uuid.uuid4())\n    image.save(images_dir / f\"{id}.png\")\n    return ImageResponse(id=id)\n</code></pre> <p>The code for the endpoints is ready. Now, let's explore how to use <code>dstack</code> to serve it on a cloud account of your choice.</p>"},{"location":"examples/sdxl/#define-the-configuration","title":"Define the configuration","text":"Tasks <p>If you want to serve an application for development purposes only, you can use  tasks.  In this scenario, while the application runs in the cloud,  it is accessible from your local machine only.</p> <p>For production purposes, the optimal approach to serve an application is by using  services. In this case, the application can be accessed through a public endpoint.</p> <p>Here's the configuration that uses services:</p> <pre><code>type: service\n\npython: \"3.11\"\ncommands: \n  - apt-get update \n  - apt-get install libgl1 -y\n  - pip install -r stable-diffusion-xl/requirements.txt\n  - uvicorn stable-diffusion-xl.main:app --port 8000\nport: 8000\n\nresources:\n  gpu: 16GB\n</code></pre>"},{"location":"examples/sdxl/#run-the-configuration","title":"Run the configuration","text":"<p>NOTE:</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <p>After the gateway is configured, go ahead run the service.</p> <pre><code>$ dstack run . -f deployment/sdxl/serve.dstack.yml\n</code></pre>"},{"location":"examples/sdxl/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you can query it at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (using the domain set up for the gateway):</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p> <pre><code>$ curl -X POST --location https://yellow-cat-1.mydomain.com/generate \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authentication: \"Bearer &amp;lt;dstack token&amp;gt;\"' \\\n    -d '{ \"prompt\": \"A cat in a hat\" }'\n</code></pre>"},{"location":"examples/sdxl/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/sdxl/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/tei/","title":"Text Embeddings Inference","text":"<p>This example demonstrates how to use TEI with <code>dstack</code>'s services to deploy embeddings.</p>"},{"location":"examples/tei/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy a text embeddings model as a service using TEI, define the following configuration file:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-embeddings-inference:latest\nenv:\n  - MODEL_ID=thenlper/gte-base\ncommands: \n  - text-embeddings-router --port 80\nport: 80\n\nresources:\n  gpu: 16GB\n</code></pre>"},{"location":"examples/tei/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f deployment/tae/serve.dstack.yml\n</code></pre>"},{"location":"examples/tei/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you can query it at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (using the domain set up for the gateway):</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p> <pre><code>$ curl https://yellow-cat-1.example.com \\\n    -X POST \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authentication: \"Bearer &amp;lt;dstack token&amp;gt;\"' \\\n    -d '{\"inputs\":\"What is Deep Learning?\"}'\n\n[[0.010704354,-0.033910684,0.004793657,-0.0042832214,0.07551489,0.028702762,0.03985837,0.021956133,...]]\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p>"},{"location":"examples/tei/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/tei/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Generation Inference and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/tgi/","title":"Text Generation Inference","text":"<p>This example demonstrates how to use TGI with <code>dstack</code>'s services to deploy LLMs.</p>"},{"location":"examples/tgi/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using TGI, you have to define the following configuration file:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code\n\nresources:\n  gpu: 24GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  format: tgi\n  type: chat\n  name: mistralai/Mistral-7B-Instruct-v0.1\n</code></pre> <p>Model mapping</p> <p>Note the <code>model</code> property is optional and is only required if you're running a chat model and want to access it via an OpenAI-compatible endpoint. For more details on how to use this feature, check the documentation on services.</p>"},{"location":"examples/tgi/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f deployment/tgi/serve.dstack.yml\n</code></pre>"},{"location":"examples/tgi/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you'll be able to  access it at <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code>.</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p> <pre><code>$ curl https://yellow-cat-1.example.com/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"&amp;lt;s&amp;gt;[INST] What is your favourite condiment?[/INST]\"}' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authentication: \"Bearer &amp;lt;dstack token&amp;gt;\"'\n</code></pre>"},{"location":"examples/tgi/#openai-interface","title":"OpenAI interface","text":"<p>Because we've configured the model mapping, it will also be possible  to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n  base_url=\"https://gateway.&lt;gateway domain&gt;\",\n  api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p> <p><pre><code>$ dstack run . -f text-generation-inference/serve.dstack.yml \\\n    --env HUGGING_FACE_HUB_TOKEN=&amp;lt;token&amp;gt; \\\n    --gpu 24GB\n</code></pre> </p>"},{"location":"examples/tgi/#quantization","title":"Quantization","text":"<p>Here's an example of using TGI with quantization:</p> <pre><code>type: service\n\nimage: ghcr.io/huggingface/text-generation-inference:latest\nenv:\n  - MODEL_ID=TheBloke/Llama-2-13B-chat-GPTQ\nport: 80\ncommands:\n  - text-generation-launcher --port 80 --trust-remote-code --quantize gptq\n\nresources:\n  gpu: 24GB\n\nmodel:\n  type: chat\n  name: TheBloke/Llama-2-13B-chat-GPTQ\n  format: tgi\n  chat_template: \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\\n' + system_message + '\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '&lt;s&gt;[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' &lt;/s&gt;' }}{% endif %}{% endfor %}\"\n  eos_token: \"&lt;/s&gt;\"\n</code></pre>"},{"location":"examples/tgi/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p>"},{"location":"examples/tgi/#whats-next","title":"What's next?","text":"<ol> <li>Check the Text Embeddings Inference and vLLM examples</li> <li>Read about services</li> <li>Browse all examples</li> <li>Join the Discord server</li> </ol>"},{"location":"examples/vllm/","title":"vLLM","text":"<p>This example demonstrates how to use vLLM with <code>dstack</code>'s services to deploy LLMs.</p>"},{"location":"examples/vllm/#define-the-configuration","title":"Define the configuration","text":"<p>To deploy an LLM as a service using vLLM, you have to define the following configuration file:</p> <pre><code>type: service\n\npython: \"3.11\"\nenv:\n  - MODEL=NousResearch/Llama-2-7b-chat-hf\ncommands:\n  - pip install vllm\n  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000\nport: 8000\n\nresources:\n  gpu: 24GB\n\n# (Optional) Enable the OpenAI-compatible endpoint\nmodel:\n  format: openai\n  type: chat\n  name: NousResearch/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"examples/vllm/#run-the-configuration","title":"Run the configuration","text":"<p>Gateway</p> <p>Before running a service, ensure that you have configured a gateway. If you're using dstack Sky, the default gateway is configured automatically for you.</p> <pre><code>$ dstack run . -f deployment/vllm/serve.dstack.yml\n</code></pre>"},{"location":"examples/vllm/#access-the-endpoint","title":"Access the endpoint","text":"<p>Once the service is up, you can query it at  <code>https://&lt;run name&gt;.&lt;gateway domain&gt;</code> (using the domain set up for the gateway):</p> <p>Authentication</p> <p>By default, the service endpoint requires the <code>Authentication</code> header with <code>\"Bearer &lt;dstack token&gt;\"</code>.</p>"},{"location":"examples/vllm/#openai-interface","title":"OpenAI interface","text":"<p>Because we've configured the model mapping, it will also be possible  to access the model at <code>https://gateway.&lt;gateway domain&gt;</code> via the OpenAI-compatible interface.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://gateway.&lt;gateway domain&gt;\", \n    api_key=\"&lt;dstack token&gt;\"\n)\n\ncompletion = client.chat.completions.create(\n    model=\"mixtral\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n        }\n    ],\n    stream=True,\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content, end=\"\")\nprint()\n</code></pre> <p>Hugging Face Hub token</p> <p>To use a model with gated access, ensure configuring the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable  (with <code>--env</code> in <code>dstack run</code> or  using <code>env</code> in the configuration file).</p>"},{"location":"examples/vllm/#source-code","title":"Source code","text":"<p>The complete, ready-to-run code is available in <code>dstackai/dstack-examples</code>.</p> <p>What's next?</p> <ol> <li>Check the Text Generation Inference example</li> <li>Read about services</li> <li>Browse examples</li> <li>Join the Discord server</li> </ol>"},{"location":"changelog/archive/2024/","title":"2024","text":""},{"location":"changelog/archive/2023/","title":"2023","text":""},{"location":"changelog/page/2/","title":"Changelog","text":""},{"location":"blog/archive/2024/","title":"2024","text":""}]}